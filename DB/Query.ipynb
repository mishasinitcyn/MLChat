{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from mixedbread_ai.client import MixedbreadAI\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "MIXEDBREAD_API_KEY = os.getenv('MIXEDBREAD_API_KEY')\n",
    "DATA_DIRECTORY = os.getenv('DATA_DIRECTORY')\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "INDEX_NAME = os.getenv('INDEX_NAME')\n",
    "CLOUD = os.getenv('CLOUD') or 'aws'\n",
    "REGION = os.getenv('REGION') or 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "spec = ServerlessSpec(cloud=CLOUD, region=REGION)\n",
    "index = pc.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixedbread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxbai = MixedbreadAI(api_key=MIXEDBREAD_API_KEY)\n",
    "\n",
    "def get_embeddings(queries):\n",
    "    res = mxbai.embeddings(\n",
    "        model='mixedbread-ai/mxbai-embed-large-v1',\n",
    "        input=queries,\n",
    "        normalized=True,\n",
    "        encoding_format='float',\n",
    "        truncation_strategy='start'\n",
    "    )\n",
    "\n",
    "    embeddings = np.array([res.data[i].embedding for i in range(len(res.data))])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are my Machine Learning tutor. Please help me answer this question: \"{query}\".\n",
    "    \n",
    "    Below, I have provided an excerpt from a textbook. If the included textbook content is relevant, consider including a \n",
    "    quote from it as part of your explanation. If you use a quote, you MUST cite the source as `{textbook} Chapter {chapter}`. Feel free to elaborate on the topic and provide additional context. \n",
    "    Else if the included textbook content does not contain the answer, but covers similar material indicating that the chapter may contain relevant information, you MUST use this format: \"<Your answer>. Read more about this in {textbook} Chapter {chapter}.\"\n",
    "    Else, if the included textbook content is completely irrelevant, you MUST use this format: \n",
    "    `Unfortunately, I can't find an answer for this question in my knowledge base. I will make my best attempt to answer per my pre-training knowledge. <Your answer>`\n",
    "    \n",
    "    Here is the textbook excerpt: \n",
    "    ```\n",
    "    {textbook_content}\n",
    "    ```\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pinecone(query, top_k=5):\n",
    "    embedded_query = get_embeddings([query]).tolist()\n",
    "    return index.query(vector=embedded_query, top_k=top_k, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(query):\n",
    "    response = query_pinecone(query)\n",
    "    print(response)\n",
    "    first_match = response['matches'][0]\n",
    "    \n",
    "    fields = {\n",
    "        \"query\": query,\n",
    "        \"textbook\": first_match['metadata']['textbook'],\n",
    "        \"chapter\": first_match['metadata']['chapter'],\n",
    "        \"textbook_content\": first_match['metadata']['content']\n",
    "    }\n",
    "    \n",
    "    prompt = prompt_template.format(**fields)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):    \n",
    "    generated_text = model.generate_content(prompt)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_prompt(\"Please explain Gradient Descent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(prompt)\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
