{
    "Deep Learning": {
        "authors": [
            "Ian Goodfellow",
            "Yoshua Bengio",
            "Aaron Courville"
        ],
        "year": 2016,
        "chapters": [
            {
                "5": [
                    "Chapter 5\nMachine Learning Basics\nDeep learning is a specific kind of machine learning. In order to understand\ndeep learning well, one must have a solid understanding of the basic principles of\nmachine learning. This chapter provides a brief course in the most important general\nprinciples that will be applied throughout the rest of the book. Novice readers or\nthose who want a wider perspective are encouraged to consider machine learning\ntextbooks with a more comprehensive coverage of the fundamentals, such as Murphy\n(2012) or Bishop (2006). If you are already familiar with machine learning basics,\nfeel free to skip ahead to section 5.11. That section covers some perspectives\non traditional machine learning techniques that have strongly influenced the\ndevelopment of deep learning algorithms. We begin with a definition of what a learning algorithm is, and present an\nexample: the linear regression algorithm. We then proceed to describe how the\nchallenge of fitting the training data differs from the challenge of finding patterns\nthat generalize to new data. Most machine learning algorithms have settings\ncalled hyperparameters that must be determined external to the learning algorithm\nitself; we discuss how to set these using additional data. Machine learning is\nessentially a form of applied statistics with increased emphasis on the use of\ncomputers to statistically estimate complicated functions and a decreased emphasis\non proving confidence intervals around these functions; we therefore present the\ntwo central approaches to statistics: frequentist estimators and Bayesian inference.\nMost machine learning algorithms can be divided into the categories of supervised\nlearning and unsupervised learning; we describe these categories and give some\nexamples of simple learning algorithms from each category. Most deep learning\nalgorithms are based on an optimization algorithm called stochastic gradient\ndescent. We describe how to combine various algorithm components such as\nan optimization algorithm, a cost function, a model, and a dataset to build a\nmachine learning algorithm. Finally, in section 5.11, we describe some of the\nfactors that have limited the ability of traditional machine learning to generalize.\nThese challenges have motivated the development of deep learning algorithms that\novercome these obstacles.\n5.1 Learning Algorithms\nA machine learning algorithm is an algorithm that is able to learn from data. But\nwhat do we mean by learning? Mitchell (1997) provides the definition {\\textquotedblleft}A computer\nprogram is said to learn from experience E with respect to some class of tasks T\nand performance measure P, if its performance at tasks in T, as",
                    " measured by P,\nimproves with experience E.{\\textquotedblright} One can imagine a very wide variety of experiences\nE, tasks T, and performance measures P, and we do not make any attempt in this\nbook to provide a formal definition of what may be used for each of these entities. Instead, the following sections provide intuitive descriptions and examples of the\ndifferent kinds of tasks, performance measures and experiences that can be used\nto construct machine learning algorithms.\n5.1.1 The Task, T\nMachine learning allows us to tackle tasks that are too difficult to solve with\nfixed programs written and designed by human beings. From a scientific and\nphilosophical point of view, machine learning is interesting because developing our\nunderstanding of machine learning entails developing our understanding of the\nprinciples that underlie intelligence.\nIn this relatively formal definition of the word {\\textquotedblleft}task,{\\textquotedblright} the process of learning\nitself is not the task. Learning is our means of attaining the ability to perform the\ntask. For example, if we want a robot to be able to walk, then walking is the task. We could program the robot to learn to walk, or we could attempt to directly write\na program that specifies how to walk manually. Machine learning tasks are usually described in terms of how the machine\nlearning system should process an example. An example is a collection of features\nthat have been quantitatively measured from some object or event that we want\nthe machine learning system to process. We typically represent an example as a\nvector x {\\in} R\nn where each entry xi of the vector is another feature. For example,\nthe features of an image are usually the values of the pixels in the image.\nMany kinds of tasks can be solved with machine learning. Some of the most\ncommon machine learning tasks include the following:\n Classification: In this type of task, the computer program is asked to specify\nwhich of k categories some input belongs to. To solve this task, the learning\nalgorithm is usually asked to produce a function f : R\nn {\\textrightarrow} {\\{}1, . . . , k{\\}}. When\ny = f(x), the model assigns an input described by vector x to a category\nidentified by numeric code y. There are other variants of the classification\ntask, for example, where f outputs a probability distribution over classes. An example of a classification task is object recognition, where",
                    " the input\nis an image (usually described as a set of pixel brightness values), and the\noutput is a numeric code identifying the object in the image. For example,\nthe Willow Garage PR2 robot is able to act as a waiter that can recognize\ndifferent kinds of drinks and deliver them to people on command (Goodfellow et al., 2010). Modern object recognition is best accomplished with\ndeep learning (Krizhevsky et al., 2012; Ioffe and Szegedy, 2015). Object\nrecognition is the same basic technology that allows computers to recognize\nfaces (Taigman et al., 2014), which can be used to automatically tag people\nin photo collections and allow computers to interact more naturally with\ntheir users.\n Classification with missing inputs: Classification becomes more challenging if the computer program is not guaranteed that every measurement\nin its input vector will always be provided. In order to solve the classification\ntask, the learning algorithm only has to define a single function mapping\nfrom a vector input to a categorical output. When some of the inputs may\nbe missing, rather than providing a single classification function, the learning\nalgorithm must learn a set of functions. Each function corresponds to classifying x with a different subset of its inputs missing. This kind of situation\narises frequently in medical diagnosis, because many kinds of medical tests\nare expensive or invasive. One way to efficiently define such a large set\nof functions is to learn a probability distribution over all of the relevant\nvariables, then solve the classification task by marginalizing out the missing\nvariables. With n input variables, we can now obtain all 2\nn different classifi- cation functions needed for each possible set of missing inputs, but we only\nneed to learn a single function describing the joint probability distribution.\nSee Goodfellow et al. (2013b) for an example of a deep probabilistic model\napplied to such a task in this way. Many of the other tasks described in this\nsection can also be generalized to work with missing inputs; classification\nwith missing inputs is just one example of what machine learning can do.\n Regression: In this type of task, the computer program is asked to predict a\nnumerical value given some input. To solve this task, the learning algorithm\nis asked to output a function f : R\nn {\\textrightarrow} R. This type of task is similar to\nclassification, except that the format of output is different. An example of\na regression task",
                    " is the prediction of the expected claim amount that an\ninsured person will make (used to set insurance premiums), or the prediction\nof future prices of securities. These kinds of predictions are also used for\nalgorithmic trading.\n Transcription: In this type of task, the machine learning system is asked\nto observe a relatively unstructured representation of some kind of data and\ntranscribe it into discrete, textual form. For example, in optical character\nrecognition, the computer program is shown a photograph containing an\nimage of text and is asked to return this text in the form of a sequence\nof characters (e.g., in ASCII or Unicode format). Google Street View uses\ndeep learning to process address numbers in this way (Goodfellow et al., 2014d). Another example is speech recognition, where the computer program\nis provided an audio waveform and emits a sequence of characters or word\nID codes describing the words that were spoken in the audio recording. Deep\nlearning is a crucial component of modern speech recognition systems used\nat major companies including Microsoft, IBM and Google (Hinton et al., 2012b).\n Machine translation: In a machine translation task, the input already\nconsists of a sequence of symbols in some language, and the computer program\nmust convert this into a sequence of symbols in another language. This is\ncommonly applied to natural languages, such as translating from English to\nFrench. Deep learning has recently begun to have an important impact on\nthis kind of task (Sutskever et al., 2014; Bahdanau et al., 2015).\n Structured output: Structured output tasks involve any task where the\noutput is a vector (or other data structure containing multiple values) with\nimportant relationships between the different elements. This is a broad\ncategory, and subsumes the transcription and translation tasks described\nabove, but also many other tasks. One example is parsing{\\textemdash}mapping a\nnatural language sentence into a tree that describes its grammatical structure\nand tagging nodes of the trees as being verbs, nouns, or adverbs, and so on.\nSee Collobert (2011) for an example of deep learning applied to a parsing\ntask. Another example is pixel-wise segmentation of images, where the\ncomputer program assigns every pixel in an image to a specific category. For\nexample, deep learning can be used to annotate the locations of roads in\naerial photographs (Mnih and Hinton, 2010). The output need not have its",
                    "\nform mirror the structure of the input as closely as in these annotation-style\ntasks. For example, in image captioning, the computer program observes an\nimage and outputs a natural language sentence describing the image (Kiros\net al., 2014a,b; Mao et al., 2015; Vinyals et al., 2015b; Donahue et al., 2014;\nKarpathy and Li, 2015; Fang et al., 2015; Xu et al., 2015). These tasks are\ncalled structured output tasks because the program must output several\nvalues that are all tightly inter-related. For example, the words produced by\nan image captioning program must form a valid sentence.\n Anomaly detection: In this type of task, the computer program sifts\nthrough a set of events or objects, and flags some of them as being unusual\nor atypical. An example of an anomaly detection task is credit card fraud\ndetection. By modeling your purchasing habits, a credit card company can\ndetect misuse of your cards. If a thief steals your credit card or credit card\ninformation, the thief`s purchases will often come from a different probability\ndistribution over purchase types than your own. The credit card company\ncan prevent fraud by placing a hold on an account as soon as that card has\nbeen used for an uncharacteristic purchase. See Chandola et al. (2009) for a\nsurvey of anomaly detection methods.  Synthesis and sampling: In this type of task, the machine learning algorithm is asked to generate new examples that are similar to those in the\ntraining data. Synthesis and sampling via machine learning can be useful\nfor media applications where it can be expensive or boring for an artist to\ngenerate large volumes of content by hand. For example, video games can\nautomatically generate textures for large objects or landscapes, rather than\nrequiring an artist to manually label each pixel (Luo et al., 2013). In some\ncases, we want the sampling or synthesis procedure to generate some specific\nkind of output given the input. For example, in a speech synthesis task, we\nprovide a written sentence and ask the program to emit an audio waveform\ncontaining a spoken version of that sentence. This is a kind of structured\noutput task, but with the added qualification that there is no single correct\noutput for each input, and we explicitly desire a large amount of variation in\nthe output, in order for the output to seem more natural and realistic.\n",
                    " Imputation of missing values: In this type of task, the machine learning\nalgorithm is given a new example x {\\in} R\nn\n, but with some entries xi of x\nmissing. The algorithm must provide a prediction of the values of the missing\nentries.\n Denoising: In this type of task, the machine learning algorithm is given in\ninput a corrupted example x{\\textasciitilde} {\\in} Rn obtained by an unknown corruption process\nfrom a clean example x {\\in} Rn\n. The learner must predict the clean example\nx from its corrupted version x{\\textasciitilde}, or more generally predict the conditional\nprobability distribution p(x | x{\\textasciitilde}).\n Density estimation or probability mass function estimation: In\nthe density estimation problem, the machine learning algorithm is asked\nto learn a function pmodel : R n {\\textrightarrow} R, where pmodel(x) can be interpreted\nas a probability density function (if x is continuous) or a probability mass\nfunction (if x is discrete) on the space that the examples were drawn from.\nTo do such a task well (we will specify exactly what that means when we\ndiscuss performance measures P), the algorithm needs to learn the structure\nof the data it has seen. It must know where examples cluster tightly and\nwhere they are unlikely to occur. Most of the tasks described above require\nthe learning algorithm to at least implicitly capture the structure of the\nprobability distribution. Density estimation allows us to explicitly capture\nthat distribution. In principle, we can then perform computations on that\ndistribution in order to solve the other tasks as well. For example, if we\nhave performed density estimation to obtain a probability distribution p(x), we can use that distribution to solve the missing value imputation task. If\na value xi is missing and all of the other values, denoted x{-}i, are given,\nthen we know the distribution over it is given by p(xi | x{-}i). In practice,\ndensity estimation does not always allow us to solve all of these related tasks, because in many cases the required operations on p(x) are computationally\nintractable.\nOf course, many other tasks and types of tasks are possible. The types of tasks\nwe list here are intended only to provide examples of what machine learning can\ndo, not to define a rigid",
                    " taxonomy of tasks.\n5.1.2 The Performance Measure, P\nIn order to evaluate the abilities of a machine learning algorithm, we must design\na quantitative measure of its performance. Usually this performance measure P is\nspecific to the task T being carried out by the system.\nFor tasks such as classification, classification with missing inputs, and transcription, we often measure the accuracy of the model. Accuracy is just the\nproportion of examples for which the model produces the correct output. We can\nalso obtain equivalent information by measuring the error rate, the proportion\nof examples for which the model produces an incorrect output. We often refer to\nthe error rate as the expected 0-1 loss. The 0-1 loss on a particular example is 0\nif it is correctly classified and 1 if it is not. For tasks such as density estimation,\nit does not make sense to measure accuracy, error rate, or any other kind of 0-1\nloss. Instead, we must use a different performance metric that gives the model\na continuous-valued score for each example. The most common approach is to\nreport the average log-probability the model assigns to some examples. Usually we are interested in how well the machine learning algorithm performs\non data that it has not seen before, since this determines how well it will work when\ndeployed in the real world. We therefore evaluate these performance measures using\na test set of data that is separate from the data used for training the machine\nlearning system.\nThe choice of performance measure may seem straightforward and objective, but it is often difficult to choose a performance measure that corresponds well to\nthe desired behavior of the system.\nIn some cases, this is because it is difficult to decide what should be measured. For example, when performing a transcription task, should we measure the accuracy\nof the system at transcribing entire sequences, or should we use a more fine-grained\nperformance measure that gives partial credit for getting some elements of the\nsequence correct? When performing a regression task, should we penalize the\nsystem more if it frequently makes medium-sized mistakes or if it rarely makes\nvery large mistakes? These kinds of design choices depend on the application.\nIn other cases, we know what quantity we would ideally like to measure, but\nmeasuring it is impractical. For example, this arises frequently in the context of\ndensity estimation. Many of the best probabilistic models represent probability\ndistributions only implicitly. Computing the actual probability value assigned to\na",
                    " specific point in space in many such models is intractable. In these cases, one\nmust design an alternative criterion that still corresponds to the design objectives, or design a good approximation to the desired criterion.\n5.1.3 The Experience, E\nMachine learning algorithms can be broadly categorized as unsupervised or\nsupervised by what kind of experience they are allowed to have during the\nlearning process. Most of the learning algorithms in this book can be understood as being allowed\nto experience an entire dataset. A dataset is a collection of many examples, as\ndefined in section 5.1.1. Sometimes we will also call examples data points. One of the oldest datasets studied by statisticians and machine learning re- searchers is the Iris dataset (Fisher, 1936). It is a collection of measurements of\ndifferent parts of 150 iris plants. Each individual plant corresponds to one example.\nThe features within each example are the measurements of each of the parts of the\nplant: the sepal length, sepal width, petal length and petal width. The dataset\nalso records which species each plant belonged to. Three different species are\nrepresented in the dataset. Unsupervised learning algorithms experience a dataset containing many\nfeatures, then learn useful properties of the structure of this dataset. In the context\nof deep learning, we usually want to learn the entire probability distribution that\ngenerated a dataset, whether explicitly as in density estimation or implicitly for\ntasks like synthesis or denoising. Some other unsupervised learning algorithms\nperform other roles, like clustering, which consists of dividing the dataset into\nclusters of similar examples. Supervised learning algorithms experience a dataset containing features, but each example is also associated with a label or target. For example, the Iris\ndataset is annotated with the species of each iris plant. A supervised learning\nalgorithm can study the Iris dataset and learn to classify iris plants into three\ndifferent species based on their measurements. Roughly speaking, unsupervised learning involves observing several examples\nof a random vector x, and attempting to implicitly or explicitly learn the probability distribution p(x), or some interesting properties of that distribution, while\nsupervised learning involves observing several examples of a random vector x and\nan associated value or vector y, and learning to predict y from x, usually by\nestimating p(y | x). The term supervised learning originates from the view of\nthe target y being provided by an instructor or teacher who shows the machine\nlearning system",
                    " what to do. In unsupervised learning, there is no instructor or\nteacher, and the algorithm must learn to make sense of the data without this guide.\nUnsupervised learning and supervised learning are not formally defined terms. The lines between them are often blurred. Many machine learning technologies can\nbe used to perform both tasks. For example, the chain rule of probability states\nthat for a vector x {\\in} Rn\n, the joint distribution can be decomposed as\np(x) =n\ni=1\np(xi | x1, . . . , xi{-}1). (5.1)\nThis decomposition means that we can solve the ostensibly unsupervised problem of\nmodeling p(x) by splitting it into n supervised learning problems. Alternatively, we\ncan solve the supervised learning problem of learning p(y | x) by using traditional\nunsupervised learning technologies to learn the joint distribution p(x, y) and\ninferring\np(y | x) =\np(x, y)\n\ny\n p(x, y\n)\n. (5.2)\nThough unsupervised learning and supervised learning are not completely formal or\ndistinct concepts, they do help to roughly categorize some of the things we do with\nmachine learning algorithms. Traditionally, people refer to regression, classification\nand structured output problems as supervised learning. Density estimation in\nsupport of other tasks is usually considered unsupervised learning.\nOther variants of the learning paradigm are possible. For example, in semisupervised learning, some examples include a supervision target but others do\nnot. In multi-instance learning, an entire collection of examples is labeled as\ncontaining or not containing an example of a class, but the individual members\nof the collection are not labeled. For a recent example of multi-instance learning\nwith deep models, see Kotzias et al. (2015). Some machine learning algorithms do not just experience a fixed dataset. For\nexample, reinforcement learning algorithms interact with an environment, so\nthere is a feedback loop between the learning system and its experiences. Such\nalgorithms are beyond the scope of this book. Please see Sutton and Barto (1998)\nor Bertsekas and Tsitsiklis (1996) for information about reinforcement learning,\nand Mnih et al. (2013) for the deep learning approach to reinforcement learning.\nMost machine learning algorithms simply experience a dataset. A dataset can\n",
                    "be described in many ways. In all cases, a dataset is a collection of examples, which are in turn collections of features. One common way of describing a dataset is with a design matrix. A design\nmatrix is a matrix containing a different example in each row. Each column of the\nmatrix corresponds to a different feature. For instance, the Iris dataset contains\n150 examples with four features for each example. This means we can represent\nthe dataset with a design matrix X {\\in} R150{\\texttimes}4\n, where Xi,1 is the sepal length of\nplant i, Xi,2 is the sepal width of plant i, etc. We will describe most of the learning\nalgorithms in this book in terms of how they operate on design matrix datasets. Of course, to describe a dataset as a design matrix, it must be possible to\ndescribe each example as a vector, and each of these vectors must be the same size.\nThis is not always possible. For example, if you have a collection of photographs\nwith different widths and heights, then different photographs will contain different\nnumbers of pixels, so not all of the photographs may be described with the same\nlength of vector. Section 9.7 and chapter 10 describe how to handle different\ntypes of such heterogeneous data. In cases like these, rather than describing the\ndataset as a matrix with m rows, we will describe it as a set containing m elements:\n{\\{}x\n(1)\n, x\n(2)\n, . . . , x\n(m){\\}}. This notation does not imply that any two example vectors\nx\n(i) and x\n(j) have the same size.\nIn the case of supervised learning, the example contains a label or target as\nwell as a collection of features. For example, if we want to use a learning algorithm\nto perform object recognition from photographs, we need to specify which object\nappears in each of the photos. We might do this with a numeric code, with 0\nsignifying a person, 1 signifying a car, 2 signifying a cat, etc. Often when working\nwith a dataset containing a design matrix of feature observations X, we also\nprovide a vector of labels y, with yi providing the label for example i. Of course, sometimes the label may be more than just a single number. For\nexample, if we want to train a speech recognition system to transcribe entire\n",
                    "sentences, then the label for each example sentence is a sequence of words. Just as there is no formal definition of supervised and unsupervised learning,\nthere is no rigid taxonomy of datasets or experiences. The structures described here\ncover most cases, but it is always possible to design new ones for new applications.\n5.1.4 Example: Linear Regression\nOur definition of a machine learning algorithm as an algorithm that is capable\nof improving a computer program`s performance at some task via experience is\nsomewhat abstract. To make this more concrete, we present an example of a\nsimple machine learning algorithm: linear regression. We will return to this\nexample repeatedly as we introduce more machine learning concepts that help to\nunderstand its behavior.\nAs the name implies, linear regression solves a regression problem. In other\nwords, the goal is to build a system that can take a vector x {\\in} Rn as input and\npredict the value of a scalar y {\\in} R as its output. In the case of linear regression,\nthe output is a linear function of the input. Let y{\\textasciicircum} be the value that our model\npredicts y should take on. We define the output to be\ny{\\textasciicircum} = wx (5.3)\nwhere w {\\in} R\nn\nis a vector of parameters. Parameters are values that control the behavior of the system. In this case, wi is\nthe coefficient that we multiply by feature xi before summing up the contributions\nfrom all the features. We can think of w as a set of weights that determine how\neach feature affects the prediction. If a feature xi receives a positive weight wi,\nthen increasing the value of that feature increases the value of our prediction y{\\textasciicircum}. If a feature receives a negative weight, then increasing the value of that feature\ndecreases the value of our prediction. If a feature`s weight is large in magnitude,\nthen it has a large effect on the prediction. If a feature`s weight is zero, it has no\neffect on the prediction.\nWe thus have a definition of our task T : to predict y from x by outputting\ny{\\textasciicircum} = wx. Next we need a definition of our performance measure, P. Suppose that we have a design matrix of m example inputs that we will not\nuse for training, only for evaluating",
                    " how well the model performs. We also have\na vector of regression targets providing the correct value of y for each of these\nexamples. Because this dataset will only be used for evaluation, we call it the test\nset. We refer to the design matrix of inputs as X(test) and the vector of regression\ntargets as y\n(test)\n. One way of measuring the performance of the model is to compute the mean\nsquared error of the model on the test set. If y{\\textasciicircum}\n(test) gives the predictions of the\nmodel on the test set, then the mean squared error is given by\nMSEtest =\n1\nm \ni\n(y{\\textasciicircum}\n(test) {-} y\n(test))\n2\ni . (5.4)\nIntuitively, one can see that this error measure decreases to 0 when y{\\textasciicircum}\n(test) = y\n(test)\n. We can also see that\nMSEtest =\n1\nm\n||{\\textasciicircum}y\n(test) {-} y\n(test)\n||2\n2\n, (5.5)\nso the error increases whenever the Euclidean distance between the predictions\nand the targets increases. To make a machine learning algorithm, we need to design an algorithm that\nwill improve the weights w in a way that reduces MSEtest when the algorithm\nis allowed to gain experience by observing a training set (X(train)\n, y\n(train)). One\nintuitive way of doing this (which we will justify later, in section 5.5.1) is just to\nminimize the mean squared error on the training set, MSEtrain. To minimize MSEtrain, we can simply solve for where its gradient is 0:\n{\\nabla}wMSEtrain = 0 (5.6)\n{\\Rightarrow} {\\nabla}w\n1\nm\n||y{\\textasciicircum}\n(train) {-} y\n(train)\n||2\n2 = 0 (5.7)\n{\\Rightarrow}\n1\nm\n{\\nabla}w||X(train)w {-} y\n(train)\n||2\n2 = 0 (5.8)\n{-}1.0 {-}0.",
                    "5 0.0 0.5 1.0 x1\n{-}3\n{-}2\n{-}1\n0\n1\n2\n3\ny\nLinear regression example\n0.5 1.0 1.5 w1\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55 MSE(train)\nOptimization of w\nFigure 5.1: A linear regression problem, with a training set consisting of ten data points, each containing one feature. Because there is only one feature, the weight vector w\ncontains only a single parameter to learn, w1. (Left)Observe that linear regression learns\nto set w1 such that the line y = w1x comes as close as possible to passing through all the\ntraining points. (Right)The plotted point indicates the value of w1 found by the normal\nequations, which we can see minimizes the mean squared error on the training set. {\\Rightarrow} {\\nabla}w\nX(train)w {-} y\n(train)  X(train)w {-} y\n(train) = 0 (5.9)\n{\\Rightarrow} {\\nabla}w\nwX(train)X(train)w {-} 2wX(train)y\n(train) + y\n(train)y\n(train) = 0\n(5.10)\n{\\Rightarrow} 2X(train)X(train)w {-} 2X(train) y\n(train) = 0 (5.11)\n{\\Rightarrow} w =\nX(train)X(train){-}1\nX(train)\ny\n(train) (5.12)\nThe system of equations whose solution is given by equation 5.12 is known as\nthe normal equations. Evaluating equation 5.12 constitutes a simple learning\nalgorithm. For an example of the linear regression learning algorithm in action,\nsee figure 5.1.\nIt is worth noting that the term linear regression is often used to refer to\na slightly more sophisticated model with one additional parameter{\\textemdash}an intercept\nterm b. In this model\ny{\\textasciicircum} = wx + b (5.13)\nso the mapping from parameters to predictions is still a linear function but the\nm",
                    "apping from features to predictions is now an affine function. This extension to\naffine functions means that the plot of the model`s predictions still looks like a\nline, but it need not pass through the origin. Instead of adding the bias parameter\nb, one can continue to use the model with only weights but augment x with an\nextra entry that is always set to 1. The weight corresponding to the extra 1 entry\nplays the role of the bias parameter. We will frequently use the term {\\textquotedblleft}linear{\\textquotedblright} when\nreferring to affine functions throughout this book.\nThe intercept term b is often called the bias parameter of the affine transformation. This terminology derives from the point of view that the output of the\ntransformation is biased toward being b in the absence of any input. This term\nis different from the idea of a statistical bias, in which a statistical estimation\nalgorithm`s expected estimate of a quantity is not equal to the true quantity. Linear regression is of course an extremely simple and limited learning algorithm,\nbut it provides an example of how a learning algorithm can work. In the subsequent\nsections we will describe some of the basic principles underlying learning algorithm\ndesign and demonstrate how these principles can be used to build more complicated\nlearning algorithms.\n5.2 Capacity, Overfitting and Underfitting\nThe central challenge in machine learning is that we must perform well on new, previously unseen inputs{\\textemdash}not just those on which our model was trained. The\nability to perform well on previously unobserved inputs is called generalization. Typically, when training a machine learning model, we have access to a training\nset, we can compute some error measure on the training set called the training\nerror, and we reduce this training error. So far, what we have described is simply\nan optimization problem. What separates machine learning from optimization is\nthat we want the generalization error, also called the test error, to be low as\nwell. The generalization error is defined as the expected value of the error on a\nnew input. Here the expectation is taken across different possible inputs, drawn\nfrom the distribution of inputs we expect the system to encounter in practice.\nWe typically estimate the generalization error of a machine learning model by\nmeasuring its performance on a test set of examples that were collected separately\nfrom the training set.\nIn our linear regression example, we trained the model by minimizing the\ntraining error,\n1\nm(train",
                    ")\n||X(train)w {-} y\n(train)\n||2\n2\n, (5.14)\nbut we actually care about the test error, 1 m(test) ||X(test)w {-} y\n(test)\n||2\n2\n. How can we affect performance on the test set when we get to observe only the\ntraining set? The field of statistical learning theory provides some answers. If\nthe training and the test set are collected arbitrarily, there is indeed little we can\ndo. If we are allowed to make some assumptions about how the training and test\nset are collected, then we can make some progress. The train and test data are generated by a probability distribution over datasets\ncalled the data generating process. We typically make a set of assumptions\nknown collectively as the i.i.d. assumptions. These assumptions are that the\nexamples in each dataset are independent from each other, and that the train\nset and test set are identically distributed, drawn from the same probability\ndistribution as each other. This assumption allows us to describe the data generating process with a probability distribution over a single example. The same\ndistribution is then used to generate every train example and every test example.\nWe call that shared underlying distribution the data generating distribution, denoted pdata. This probabilistic framework and the i.i.d. assumptions allow us to\nmathematically study the relationship between training error and test error.\nOne immediate connection we can observe between the training and test error\nis that the expected training error of a randomly selected model is equal to the\nexpected test error of that model. Suppose we have a probability distribution\np(x, y) and we sample from it repeatedly to generate the train set and the test\nset. For some fixed value w, the expected training set error is exactly the same as\nthe expected test set error, because both expectations are formed using the same\ndataset sampling process. The only difference between the two conditions is the\nname we assign to the dataset we sample.\nOf course, when we use a machine learning algorithm, we do not fix the\nparameters ahead of time, then sample both datasets. We sample the training set, then use it to choose the parameters to reduce training set error, then sample the\ntest set. Under this process, the expected test error is greater than or equal to\nthe expected value of training error. The factors determining how well a machine\nlearning algorithm will perform",
                    " are its ability to:\n1. Make the training error small.\n2. Make the gap between training and test error small.\nThese two factors correspond to the two central challenges in machine learning:\nunderfitting and overfitting. Underfitting occurs when the model is not able to\nobtain a sufficiently low error value on the training set. Overfitting occurs when\nthe gap between the training error and test error is too large.\nWe can control whether a model is more likely to overfit or underfit by altering\nits capacity. Informally, a model`s capacity is its ability to fit a wide variety of\nfunctions. Models with low capacity may struggle to fit the training set. Models\nwith high capacity can overfit by memorizing properties of the training set that do\nnot serve them well on the test set. One way to control the capacity of a learning algorithm is by choosing its\nhypothesis space, the set of functions that the learning algorithm is allowed to\nselect as being the solution. For example, the linear regression algorithm has the\nset of all linear functions of its input as its hypothesis space. We can generalize\nlinear regression to include polynomials, rather than just linear functions, in its\nhypothesis space. Doing so increases the model`s capacity. A polynomial of degree one gives us the linear regression model with which we\nare already familiar, with prediction\ny{\\textasciicircum} = b + wx. (5.15)\nBy introducing x\n2 as another feature provided to the linear regression model, we\ncan learn a model that is quadratic as a function of x:\ny{\\textasciicircum} = b + w1x + w2x\n2\n. (5.16)\nThough this model implements a quadratic function of its input, the output is\nstill a linear function of the parameters, so we can still use the normal equations\nto train the model in closed form. We can continue to add more powers of x as\nadditional features, for example to obtain a polynomial of degree 9:\ny{\\textasciicircum} = b +\n9\ni=1\nwix\ni. (5.17)\nMachine learning algorithms will generally perform best when their capacity\nis appropriate for the true complexity of the task they need to perform and the\namount of training data they are provided with. Models with insufficient capacity\nare unable to",
                    " solve complex tasks. Models with high capacity can solve complex\ntasks, but when their capacity is higher than needed to solve the present task they\nmay overfit. Figure 5.2 shows this principle in action. We compare a linear, quadratic\nand degree-9 predictor attempting to fit a problem where the true underlying\nfunction is quadratic. The linear function is unable to capture the curvature in\nthe true underlying problem, so it underfits. The degree-9 predictor is capable of\nrepresenting the correct function, but it is also capable of representing infinitely\nmany other functions that pass exactly through the training points, because we\nhave more parameters than training examples. We have little chance of choosing\na solution that generalizes well when so many wildly different solutions exist. In\nthis example, the quadratic model is perfectly matched to the true structure of\nthe task so it generalizes well to new data.\n\n\n\n\n\n \n\n\n\nFigure 5.2: We fit three models to this example training set. The training data was\ngenerated synthetically, by randomly sampling x values and choosing y deterministically\nby evaluating a quadratic function. (Left)A linear function fit to the data suffers from\nunderfitting{\\textemdash}it cannot capture the curvature that is present in the data. (Center)A\nquadratic function fit to the data generalizes well to unseen points. It does not suffer from\na significant amount of overfitting or underfitting. (Right)A polynomial of degree 9 fit to\nthe data suffers from overfitting. Here we used the Moore-Penrose pseudoinverse to solve\nthe underdetermined normal equations. The solution passes through all of the training\npoints exactly, but we have not been lucky enough for it to extract the correct structure.\nIt now has a deep valley in between two training points that does not appear in the true\nunderlying function. It also increases sharply on the left side of the data, while the true\nfunction decreases in this area.\nSo far we have described only one way of changing a model`s capacity: by\nchanging the number of input features it has, and simultaneously adding new\nparameters associated with those features. There are in fact many ways of changing\na model`s capacity. Capacity is not determined only by the choice of model. The\nmodel specifies which family of functions the learning algorithm can choose from\nwhen varying the parameters in order to reduce a training objective. This is",
                    " called\nthe representational capacity of the model. In many cases, finding the best\nfunction within this family is a very difficult optimization problem. In practice,\nthe learning algorithm does not actually find the best function, but merely one\nthat significantly reduces the training error. These additional limitations, such as\nthe imperfection of the optimization algorithm, mean that the learning algorithm`s\neffective capacity may be less than the representational capacity of the model\nfamily. Our modern ideas about improving the generalization of machine learning\nmodels are refinements of thought dating back to philosophers at least as early\nas Ptolemy. Many early scholars invoke a principle of parsimony that is now\nmost widely known as Occam`s razor (c. 1287-1347). This principle states that\namong competing hypotheses that explain known observations equally well, one\nshould choose the {\\textquotedblleft}simplest{\\textquotedblright} one. This idea was formalized and made more precise\nin the 20th century by the founders of statistical learning theory (Vapnik and\nChervonenkis, 1971; Vapnik, 1982; Blumer et al., 1989; Vapnik, 1995). Statistical learning theory provides various means of quantifying model capacity. Among these, the most well-known is the Vapnik-Chervonenkis dimension, or\nVC dimension. The VC dimension measures the capacity of a binary classifier. The\nVC dimension is defined as being the largest possible value of m for which there\nexists a training set of m different x points that the classifier can label arbitrarily. Quantifying the capacity of the model allows statistical learning theory to\nmake quantitative predictions. The most important results in statistical learning\ntheory show that the discrepancy between training error and generalization error\nis bounded from above by a quantity that grows as the model capacity grows but\nshrinks as the number of training examples increases (Vapnik and Chervonenkis, 1971; Vapnik, 1982; Blumer et al., 1989; Vapnik, 1995). These bounds provide\nintellectual justification that machine learning algorithms can work, but they are\nrarely used in practice when working with deep learning algorithms. This is in\npart because the bounds are often quite loose and in part because it can be quite\ndifficult to determine the capacity of deep learning algorithms. The problem of\ndetermining the capacity of a deep learning model is especially difficult because the\neffective capacity is",
                    " limited by the capabilities of the optimization algorithm, and\nwe have little theoretical understanding of the very general non-convex optimization\nproblems involved in deep learning.\nWe must remember that while simpler functions are more likely to generalize\n(to have a small gap between training and test error) we must still choose a\nsufficiently complex hypothesis to achieve low training error. Typically, training\nerror decreases until it asymptotes to the minimum possible error value as model\ncapacity increases (assuming the error measure has a minimum value). Typically, generalization error has a U-shaped curve as a function of model capacity. This is\nillustrated in figure 5.3. To reach the most extreme case of arbitrarily high capacity, we introduce\n0 Optimal Capacity\nCapacity\nError Underfitting zone Overfitting zone\nGeneralization gap\nTraining error\nGeneralization error\nFigure 5.3: Typical relationship between capacity and error. Training and test error\nbehave differently. At the left end of the graph, training error and generalization error\nare both high. This is the underfitting regime. As we increase capacity, training error\ndecreases, but the gap between training and generalization error increases. Eventually, the size of this gap outweighs the decrease in training error, and we enter the overfitting\nregime, where capacity is too large, above the optimal capacity.\nthe concept of non-parametric models. So far, we have seen only parametric\nmodels, such as linear regression. Parametric models learn a function described\nby a parameter vector whose size is finite and fixed before any data is observed.\nNon-parametric models have no such limitation.\nSometimes, non-parametric models are just theoretical abstractions (such as\nan algorithm that searches over all possible probability distributions) that cannot\nbe implemented in practice. However, we can also design practical non-parametric\nmodels by making their complexity a function of the training set size. One example\nof such an algorithm is nearest neighbor regression. Unlike linear regression,\nwhich has a fixed-length vector of weights, the nearest neighbor regression model\nsimply stores the X and y from the training set. When asked to classify a test\npoint x, the model looks up the nearest entry in the training set and returns the\nassociated regression target. In other words, y{\\textasciicircum} = yi where i = arg min ||Xi,: {-} x||2\n2\n. The algorithm can",
                    " also be generalized to distance metrics other than the L\n2 norm,\nsuch as learned distance metrics (Goldberger et al., 2005). If the algorithm is\nallowed to break ties by averaging the yi values for all Xi,: that are tied for nearest, then this algorithm is able to achieve the minimum possible training error (which\nmight be greater than zero, if two identical inputs are associated with different\noutputs) on any regression dataset. Finally, we can also create a non-parametric learning algorithm by wrapping a\nparametric learning algorithm inside another algorithm that increases the number\nof parameters as needed. For example, we could imagine an outer loop of learning\nthat changes the degree of the polynomial learned by linear regression on top of a\npolynomial expansion of the input.\nThe ideal model is an oracle that simply knows the true probability distribution\nthat generates the data. Even such a model will still incur some error on many\nproblems, because there may still be some noise in the distribution. In the case\nof supervised learning, the mapping from x to y may be inherently stochastic,\nor y may be a deterministic function that involves other variables besides those\nincluded in x. The error incurred by an oracle making predictions from the true\ndistribution p(x, y) is called the Bayes error. Training and generalization error vary as the size of the training set varies. Expected generalization error can never increase as the number of training examples\nincreases. For non-parametric models, more data yields better generalization until\nthe best possible error is achieved. Any fixed parametric model with less than\noptimal capacity will asymptote to an error value that exceeds the Bayes error. See\nfigure 5.4 for an illustration. Note that it is possible for the model to have optimal\ncapacity and yet still have a large gap between training and generalization error.\nIn this situation, we may be able to reduce this gap by gathering more training\nexamples.\n5.2.1 The No Free Lunch Theorem\nLearning theory claims that a machine learning algorithm can generalize well from\na finite training set of examples. This seems to contradict some basic principles of\nlogic. Inductive reasoning, or inferring general rules from a limited set of examples,\nis not logically valid. To logically infer a rule describing every member of a set, one must have information about every member of that set.\nIn part, machine learning avoids this",
                    " problem by offering only probabilistic rules, rather than the entirely certain rules used in purely logical reasoning. Machine\nlearning promises to find rules that are probably correct about most members of\nthe set they concern.\nUnfortunately, even this does not resolve the entire problem. The no free\nlunch theorem for machine learning (Wolpert, 1996) states that, averaged over\nall possible data generating distributions, every classification algorithm has the\nsame error rate when classifying previously unobserved points. In other words,\nin some sense, no machine learning algorithm is universally any better than any\nother. The most sophisticated algorithm we can conceive of has the same average\n\n \n\nFigure 5.4: The effect of the training dataset size on the train and test error, as well as\non the optimal model capacity. We constructed a synthetic regression problem based on\nadding a moderate amount of noise to a degree-5 polynomial, generated a single test set, and then generated several different sizes of training set. For each size, we generated 40\ndifferent training sets in order to plot error bars showing 95 percent confidence intervals. (Top)The MSE on the training and test set for two different models: a quadratic model,\nand a model with degree chosen to minimize the test error. Both are fit in closed form. For\nthe quadratic model, the training error increases as the size of the training set increases. This is because larger datasets are harder to fit. Simultaneously, the test error decreases, because fewer incorrect hypotheses are consistent with the training data. The quadratic\nmodel does not have enough capacity to solve the task, so its test error asymptotes to\na high value. The test error at optimal capacity asymptotes to the Bayes error. The\ntraining error can fall below the Bayes error, due to the ability of the training algorithm\nto memorize specific instances of the training set. As the training size increases to infinity, the training error of any fixed-capacity model (here, the quadratic model) must rise to at\nleast the Bayes error. (Bottom)As the training set size increases, the optimal capacity\n(shown here as the degree of the optimal polynomial regressor) increases. The optimal\ncapacity plateaus after reaching sufficient complexity to solve the task.\n\nperformance (over all possible tasks) as merely predicting that every point belongs\nto the same class. Fortunately, these results hold only when we average over all possible data\n",
                    "generating distributions. If we make assumptions about the kinds of probability\ndistributions we encounter in real-world applications, then we can design learning\nalgorithms that perform well on these distributions.\nThis means that the goal of machine learning research is not to seek a universal\nlearning algorithm or the absolute best learning algorithm. Instead, our goal is to\nunderstand what kinds of distributions are relevant to the {\\textquotedblleft}real world{\\textquotedblright} that an AI\nagent experiences, and what kinds of machine learning algorithms perform well on\ndata drawn from the kinds of data generating distributions we care about.\n5.2.2 Regularization\nThe no free lunch theorem implies that we must design our machine learning\nalgorithms to perform well on a specific task. We do so by building a set of\npreferences into the learning algorithm. When these preferences are aligned with\nthe learning problems we ask the algorithm to solve, it performs better. So far, the only method of modifying a learning algorithm that we have discussed\nconcretely is to increase or decrease the model`s representational capacity by adding\nor removing functions from the hypothesis space of solutions the learning algorithm\nis able to choose. We gave the specific example of increasing or decreasing the\ndegree of a polynomial for a regression problem. The view we have described so\nfar is oversimplified.\nThe behavior of our algorithm is strongly affected not just by how large we\nmake the set of functions allowed in its hypothesis space, but by the specific identity\nof those functions. The learning algorithm we have studied so far, linear regression,\nhas a hypothesis space consisting of the set of linear functions of its input. These\nlinear functions can be very useful for problems where the relationship between\ninputs and outputs truly is close to linear. They are less useful for problems\nthat behave in a very nonlinear fashion. For example, linear regression would\nnot perform very well if we tried to use it to predict sin(x) from x. We can thus\ncontrol the performance of our algorithms by choosing what kind of functions we\nallow them to draw solutions from, as well as by controlling the amount of these\nfunctions. We can also give a learning algorithm a preference for one solution in its\nhypothesis space to another. This means that both functions are eligible, but one\nis preferred. The unpreferred solution will be chosen only if it fits the training\ndata significantly better than the preferred solution.\nFor example",
                    ", we can modify the training criterion for linear regression to include\nweight decay. To perform linear regression with weight decay, we minimize a sum\ncomprising both the mean squared error on the training and a criterion J (w) that\nexpresses a preference for the weights to have smaller squared L\n2 norm. Specifically,\nJ(w) = MSEtrain + {\\lambda}ww, (5.18)\nwhere {\\lambda} is a value chosen ahead of time that controls the strength of our preference\nfor smaller weights. When {\\lambda} = 0, we impose no preference, and larger {\\lambda} forces the\nweights to become smaller. Minimizing J(w) results in a choice of weights that\nmake a tradeoff between fitting the training data and being small. This gives us\nsolutions that have a smaller slope, or put weight on fewer of the features. As an\nexample of how we can control a model`s tendency to overfit or underfit via weight\ndecay, we can train a high-degree polynomial regression model with different values\nof {\\lambda}. See figure 5.5 for the results.\n\n\n\n \n\n\n  \n \n\n\n\n\nFigure 5.5: We fit a high-degree polynomial regression model to our example training set\nfrom figure 5.2. The true function is quadratic, but here we use only models with degree 9.\nWe vary the amount of weight decay to prevent these high-degree models from overfitting.\n(Left)With very large {\\lambda}, we can force the model to learn a function with no slope at\nall. This underfits because it can only represent a constant function. (Center)With a\nmedium value of {\\lambda}, the learning algorithm recovers a curve with the right general shape.\nEven though the model is capable of representing functions with much more complicated\nshape, weight decay has encouraged it to use a simpler function described by smaller\ncoefficients. (Right)With weight decay approaching zero (i.e., using the Moore-Penrose\npseudoinverse to solve the underdetermined problem with minimal regularization), the\ndegree-9 polynomial overfits significantly, as we saw in figure 5.2.\nMore generally, we can regularize a model that learns a function f(x; {\\theta}) by\nadding a penalty called a regularizer to the cost function. In the case of weight\ndecay, the regularizer is {\\Omega",
                    "}(w) = ww. In chapter 7, we will see that many other\nregularizers are possible.\nExpressing preferences for one function over another is a more general way\nof controlling a model`s capacity than including or excluding members from the\nhypothesis space. We can think of excluding a function from a hypothesis space as\nexpressing an infinitely strong preference against that function.\nIn our weight decay example, we expressed our preference for linear functions\ndefined with smaller weights explicitly, via an extra term in the criterion we\nminimize. There are many other ways of expressing preferences for different\nsolutions, both implicitly and explicitly. Together, these different approaches\nare known as regularization. Regularization is any modification we make to a\nlearning algorithm that is intended to reduce its generalization error but not its\ntraining error. Regularization is one of the central concerns of the field of machine\nlearning, rivaled in its importance only by optimization.\nThe no free lunch theorem has made it clear that there is no best machine\nlearning algorithm, and, in particular, no best form of regularization. Instead\nwe must choose a form of regularization that is well-suited to the particular task\nwe want to solve. The philosophy of deep learning in general and this book in\nparticular is that a very wide range of tasks (such as all of the intellectual tasks\nthat people can do) may all be solved effectively using very general-purpose forms\nof regularization.\n5.3 Hyperparameters and Validation Sets\nMost machine learning algorithms have several settings that we can use to control\nthe behavior of the learning algorithm. These settings are called hyperparame- ters. The values of hyperparameters are not adapted by the learning algorithm\nitself (though we can design a nested learning procedure where one learning\nalgorithm learns the best hyperparameters for another learning algorithm).\nIn the polynomial regression example we saw in figure 5.2, there is a single\nhyperparameter: the degree of the polynomial, which acts as a capacity hyper- parameter. The {\\lambda} value used to control the strength of weight decay is another\nexample of a hyperparameter.\nSometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because it is difficult to optimize. More frequently, the\nsetting must be a hyperparameter because it is not appropriate to learn that\nhyperparameter on the training set. This applies to all hyperparam",
                    "eters that\ncontrol model capacity. If learned on the training set, such hyperparameters would\nalways choose the maximum possible model capacity, resulting in overfitting (refer\nto figure 5.3). For example, we can always fit the training set better with a higher\ndegree polynomial and a weight decay setting of {\\lambda} = 0 than we could with a lower\ndegree polynomial and a positive weight decay setting.\nTo solve this problem, we need a validation set of examples that the training\nalgorithm does not observe. Earlier we discussed how a held-out test set, composed of examples coming from\nthe same distribution as the training set, can be used to estimate the generalization\nerror of a learner, after the learning process has completed. It is important that the\ntest examples are not used in any way to make choices about the model, including\nits hyperparameters. For this reason, no example from the test set can be used\nin the validation set. Therefore, we always construct the validation set from the\ntraining data. Specifically, we split the training data into two disjoint subsets. One\nof these subsets is used to learn the parameters. The other subset is our validation\nset, used to estimate the generalization error during or after training, allowing\nfor the hyperparameters to be updated accordingly. The subset of data used to\nlearn the parameters is still typically called the training set, even though this\nmay be confused with the larger pool of data used for the entire training process. The subset of data used to guide the selection of hyperparameters is called the\nvalidation set. Typically, one uses about 80{\\%} of the training data for training and\n20{\\%} for validation. Since the validation set is used to {\\textquotedblleft}train{\\textquotedblright} the hyperparameters, the validation set error will underestimate the generalization error, though typically\nby a smaller amount than the training error. After all hyperparameter optimization\nis complete, the generalization error may be estimated using the test set.\nIn practice, when the same test set has been used repeatedly to evaluate\nperformance of different algorithms over many years, and especially if we consider\nall the attempts from the scientific community at beating the reported state-ofthe-art performance on that test set, we end up having optimistic evaluations with\nthe test set as well. Benchmarks can thus become stale and then do not reflect the\ntrue field performance of",
                    " a trained system. Thankfully, the community tends to\nmove on to new (and usually more ambitious and larger) benchmark datasets.\n\n\n5.3.1 Cross-Validation\nDividing the dataset into a fixed training set and a fixed test set can be problematic\nif it results in the test set being small. A small test set implies statistical uncertainty\naround the estimated average test error, making it difficult to claim that algorithm\nA works better than algorithm B on the given task. When the dataset has hundreds of thousands of examples or more, this is not a\nserious issue. When the dataset is too small, are alternative procedures enable one\nto use all of the examples in the estimation of the mean test error, at the price of\nincreased computational cost. These procedures are based on the idea of repeating\nthe training and testing computation on different randomly chosen subsets or splits\nof the original dataset. The most common of these is the k-fold cross-validation\nprocedure, shown in algorithm 5.1, in which a partition of the dataset is formed by\nsplitting it into k non-overlapping subsets. The test error may then be estimated\nby taking the average test error across k trials. On trial i, the i-th subset of the\ndata is used as the test set and the rest of the data is used as the training set. One\nproblem is that there exist no unbiased estimators of the variance of such average\nerror estimators (Bengio and Grandvalet, 2004), but approximations are typically\nused.\n5.4 Estimators, Bias and Variance\nThe field of statistics gives us many tools that can be used to achieve the machine\nlearning goal of solving a task not only on the training set but also to generalize.\nFoundational concepts such as parameter estimation, bias and variance are useful\nto formally characterize notions of generalization, underfitting and overfitting.\n5.4.1 Point Estimation\nPoint estimation is the attempt to provide the single {\\textquotedblleft}best{\\textquotedblright} prediction of some\nquantity of interest. In general the quantity of interest can be a single parameter\nor a vector of parameters in some parametric model, such as the weights in our\nlinear regression example in section 5.1.4, but it can also be a whole function.\nIn order to distinguish estimates of parameters from their true value, our\nconvention will be to denote",
                    " a point estimate of a parameter {\\theta} by {\\theta}{\\textasciicircum}. Let {\\{}x\n(1)\n, . . . , x\n(m){\\}} be a set of m independent and identically distributed\n\n\nAlgorithm 5.1 The k-fold cross-validation algorithm. It can be used to estimate\ngeneralization error of a learning algorithm A when the given dataset D is too\nsmall for a simple train/test or train/valid split to yield accurate estimation of\ngeneralization error, because the mean of a loss L on a small test set may have too\nhigh variance. The dataset D contains as elements the abstract examples z\n(i) (for\nthe i-th example), which could stand for an (input,target) pair z\n(i) = (x\n(i)\n, y\n(i))\nin the case of supervised learning, or for just an input z\n(i) = x\n(i)\nin the case\nof unsupervised learning. The algorithm returns the vector of errors e for each\nexample in D, whose mean is the estimated generalization error. The errors on\nindividual examples can be used to compute a confidence interval around the mean\n(equation 5.47). While these confidence intervals are not well-justified after the\nuse of cross-validation, it is still common practice to use them to declare that\nalgorithm A is better than algorithm B only if the confidence interval of the error\nof algorithm A lies below and does not intersect the confidence interval of algorithm\nB. Define KFoldXV(D, A,L, k):\nRequire: D, the given dataset, with elements z\n(i)\nRequire: A, the learning algorithm, seen as a function that takes a dataset as\ninput and outputs a learned function\nRequire: L, the loss function, seen as a function from a learned function f and\nan example z\n(i) {\\in} D to a scalar {\\in} R\nRequire: k, the number of folds\nSplit D into k mutually exclusive subsets Di, whose union is D. for i from 1 to k do\nfi = A(D{\\textbackslash}Di)\nfor z\n(j)\nin Di do\nej = L(fi, z\n(j))\nend for\nend for\nReturn e\n(i",
                    ".i.d.) data points. A point estimator or statistic is any function of the data:\n{\\theta}{\\textasciicircum}m = g(x\n(1)\n, . . . , x\n(m) ). (5.19)\nThe definition does not require that g return a value that is close to the true\n{\\theta} or even that the range of g is the same as the set of allowable values of {\\theta}. This definition of a point estimator is very general and allows the designer of an\nestimator great flexibility. While almost any function thus qualifies as an estimator, a good estimator is a function whose output is close to the true underlying {\\theta} that\ngenerated the training data.\nFor now, we take the frequentist perspective on statistics. That is, we assume\nthat the true parameter value {\\theta} is fixed but unknown, while the point estimate\n{\\theta}{\\textasciicircum} is a function of the data. Since the data is drawn from a random process, any\nfunction of the data is random. Therefore {\\theta}{\\textasciicircum} is a random variable.\nPoint estimation can also refer to the estimation of the relationship between\ninput and target variables. We refer to these types of point estimates as function\nestimators.\nFunction Estimation As we mentioned above, sometimes we are interested in\nperforming function estimation (or function approximation). Here we are trying to\npredict a variable y given an input vector x. We assume that there is a function\nf (x) that describes the approximate relationship between y and x. For example,\nwe may assume that y = f(x) + , where  stands for the part of y that is not\npredictable from x. In function estimation, we are interested in approximating\nf with a model or estimate {\\textasciicircum}f. Function estimation is really just the same as\nestimating a parameter {\\theta}; the function estimator {\\textasciicircum}f is simply a point estimator in\nfunction space. The linear regression example (discussed above in section 5.1.4) and\nthe polynomial regression example (discussed in section 5.2) are both examples of\nscenarios that may be interpreted either as estimating a parameter w or estimating\na function {\\textasciicircum}f mapping",
                    " from x to y. We now review the most commonly studied properties of point estimators and\ndiscuss what they tell us about these estimators.\n5.4.2 Bias\nThe bias of an estimator is defined as:\nbias({\\theta}{\\textasciicircum}m) = E({\\theta}{\\textasciicircum}m) {-} {\\theta} (5.20)\nwhere the expectation is over the data (seen as samples from a random variable)\nand {\\theta} is the true underlying value of {\\theta} used to define the data generating distribution. An estimator {\\theta}{\\textasciicircum}m is said to be unbiased if bias({\\theta}{\\textasciicircum}m) = 0, which implies\nthat E({\\theta}{\\textasciicircum}m) = {\\theta}. An estimator {\\theta}{\\textasciicircum}m is said to be asymptotically unbiased if\nlimm{\\textrightarrow}{\\infty} bias({\\theta}{\\textasciicircum}m) = 0, which implies that limm{\\textrightarrow}{\\infty} E({\\theta}{\\textasciicircum}m) = {\\theta}.\nExample: Bernoulli Distribution Consider a set of samples {\\{}x\n(1)\n, . . . , x\n(m){\\}}\nthat are independently and identically distributed according to a Bernoulli distribution with mean {\\theta}:\nP(x\n(i)\n; {\\theta}) = {\\theta}\nx\n(i) (1 {-} {\\theta})\n(1{-}x\n(i))\n. (5.21)\nA common estimator for the {\\theta} parameter of this distribution is the mean of the\ntraining samples: {\\textasciicircum}{\\theta}m =\n1\nm m\ni=1\nx\n(i)\n. (5.22)\nTo determine whether this estimator is biased, we can substitute equation 5.22\ninto equation 5.20:\nbias({\\textasciicircum}{\\theta}m) = E[{\\textasciicircum}{\\theta}m]",
                    " {-} {\\theta} (5.23) = E\n\n1\nm m\ni=1\nx\n(i){-} {\\theta} (5.24)\n=\n1\nmm\ni=1\nE\nx\n(i) {-} {\\theta} (5.25)\n=\n1\nmm\ni=1\n\n1\nx(i)=0\n\nx\n(i){\\theta}\nx\n(i)(1 {-} {\\theta})\n(1{-}x\n(i) ) {-} {\\theta} (5.26)\n=\n1\nmm\ni=1\n({\\theta}) {-} {\\theta} (5.27) = {\\theta} {-} {\\theta} = 0 (5.28)\nSince bias({\\textasciicircum}{\\theta}) = 0, we say that our estimator {\\textasciicircum}{\\theta} is unbiased.\nExample: Gaussian Distribution Estimator of the Mean Now, consider\na set of samples {\\{}x\n(1)\n, . . . , x\n(m){\\}} that are independently and identically distributed\naccording to a Gaussian distribution p(x\n(i)) = N (x\n(i)\n; {\\textmu}, {\\sigma}\n2 ), where i {\\in} {\\{}1, . . . , m{\\}}.\n\nRecall that the Gaussian probability density function is given by\np(x\n(i)\n; {\\textmu}, {\\sigma}\n2) =\n1\n{\\sqrt{}}\n2{\\pi}{\\sigma}2\nexp \n{-}\n1\n2\n(x\n(i) {-} {\\textmu})\n2\n{\\sigma}2\n\n. (5.29)\nA common estimator of the Gaussian mean parameter is known as the sample\nmean:\n{\\textmu}{\\textasciicircum}m =\n1\nm m\ni=1\nx\n(i) (5.30)\nTo determine the bias of the sample mean, we are again interested in calculating\nits expectation:\nbias({\\textmu}{\\textasciicircum}m) = E[{\\textmu}{",
                    "\\textasciicircum}m] {-} {\\textmu} (5.31) = E\n\n1\nm m\ni=1\nx\n(i) {-} {\\textmu} (5.32)\n= \n1\nm m\ni=1\nE\nx\n(i)\n{-} {\\textmu} (5.33)\n= \n1\nm m\ni=1\n{\\textmu}\n{-} {\\textmu} (5.34) = {\\textmu} {-} {\\textmu} = 0 (5.35)\nThus we find that the sample mean is an unbiased estimator of Gaussian mean\nparameter.\nExample: Estimators of the Variance of a Gaussian Distribution As an\nexample, we compare two different estimators of the variance parameter {\\sigma}\n2 of a\nGaussian distribution. We are interested in knowing if either estimator is biased.\nThe first estimator of {\\sigma}\n2 we consider is known as the sample variance:\n{\\sigma}{\\textasciicircum}\n2m =\n1\nmm\ni=1\n\nx\n(i) {-} {\\textmu}{\\textasciicircum} m\n2\n, (5.36)\nwhere {\\textmu}{\\textasciicircum}m is the sample mean, defined above. More formally, we are interested in\ncomputing\nbias({\\sigma}{\\textasciicircum}\n2m) = E[{\\sigma}{\\textasciicircum}\n2m] {-} {\\sigma}\n2\n(5.37)\nWe begin by evaluating the term E[{\\sigma}{\\textasciicircum}\n2m ]:\nE[{\\sigma}{\\textasciicircum}\n2m] =E\n\n1\nmm\ni=1\n\nx\n(i) {-} {\\textmu}{\\textasciicircum}m\n2\n(5.38)\n=m {-} 1\nm\n{\\sigma}\n2\n(5.39)\nReturning to equation 5.37, we conclude that the bias of {\\sigma}{\\textasciicircum}\n2m is {-}{\\sigma",
                    "}\n2/m. Therefore, the sample variance is a biased estimator. The unbiased sample variance estimator\n{\\sigma}{\\textasciitilde}\n2m =\n1\nm {-} 1 m\ni=1\n\nx\n(i) {-} {\\textmu}{\\textasciicircum}m\n2\n(5.40)\nprovides an alternative approach. As the name suggests this estimator is unbiased.\nThat is, we find that E[{\\sigma}{\\textasciitilde}\n2m] = {\\sigma}\n2\n:\nE[{\\sigma}{\\textasciitilde}\n2m] = E\n\n1\nm {-} 1 m\ni=1\n\nx\n(i) {-} {\\textmu}{\\textasciicircum}m\n2\n(5.41) = m\nm {-} 1\nE[{\\sigma}{\\textasciicircum}\n2m ] (5.42) = m\nm {-} 1\nm {-} 1\nm\n{\\sigma}\n2\n(5.43) = {\\sigma}\n2\n. (5.44)\nWe have two estimators: one is biased and the other is not. While unbiased\nestimators are clearly desirable, they are not always the {\\textquotedblleft}best{\\textquotedblright} estimators. As we\nwill see we often use biased estimators that possess other important properties.\n5.4.3 Variance and Standard Error\nAnother property of the estimator that we might want to consider is how much\nwe expect it to vary as a function of the data sample. Just as we computed the\nexpectation of the estimator to determine its bias, we can compute its variance.\nThe variance of an estimator is simply the variance\nVar( {\\textasciicircum}{\\theta}) (5.45)\nwhere the random variable is the training set. Alternately, the square root of the\nvariance is called the standard error, denoted SE({\\textasciicircum}{\\theta}).\nThe variance or the standard error of an estimator provides a measure of how\nwe would expect the estimate we compute from data to vary as we independently\nresample the dataset from the underlying data generating process",
                    ". Just as we\nmight like an estimator to exhibit low bias we would also like it to have relatively\nlow variance.\nWhen we compute any statistic using a finite number of samples, our estimate\nof the true underlying parameter is uncertain, in the sense that we could have\nobtained other samples from the same distribution and their statistics would have\nbeen different. The expected degree of variation in any estimator is a source of\nerror that we want to quantify. The standard error of the mean is given by\nSE({\\textmu}{\\textasciicircum}m) = \nVar\n1\nmm\ni=1\nx\n(i) =\n{\\sigma}\n{\\sqrt{}}m\n, (5.46)\nwhere {\\sigma}\n2\nis the true variance of the samples x\ni. The standard error is often\nestimated by using an estimate of {\\sigma}. Unfortunately, neither the square root of\nthe sample variance nor the square root of the unbiased estimator of the variance\nprovide an unbiased estimate of the standard deviation. Both approaches tend\nto underestimate the true standard deviation, but are still used in practice. The\nsquare root of the unbiased estimator of the variance is less of an underestimate.\nFor large m, the approximation is quite reasonable. The standard error of the mean is very useful in machine learning experiments. We often estimate the generalization error by computing the sample mean of the\nerror on the test set. The number of examples in the test set determines the\naccuracy of this estimate. Taking advantage of the central limit theorem, which\ntells us that the mean will be approximately distributed with a normal distribution,\nwe can use the standard error to compute the probability that the true expectation\nfalls in any chosen interval. For example, the 95{\\%} confidence interval centered on\nthe mean {\\textmu}{\\textasciicircum}m is\n({\\textmu}{\\textasciicircum}m {-} 1.96SE({\\textmu}{\\textasciicircum} m), {\\textmu}{\\textasciicircum}m + 1.96SE({\\textmu}{\\textasciicircum}m)), (5.47)\nunder the normal distribution with mean {\\textmu}{\\textasciicircum}m and variance SE({\\textmu}{\\textasciicircum}m)\n2",
                    "\n. In machine\nlearning experiments, it is common to say that algorithm A is better than algorithm\nB if the upper bound of the 95{\\%} confidence interval for the error of algorithm A is\nless than the lower bound of the 95{\\%} confidence interval for the error of algorithm\nB.\nExample: Bernoulli Distribution We once again consider a set of samples\n{\\{}x\n(1)\n, . . . , x\n(m){\\}} drawn independently and identically from a Bernoulli distribution\n(recall P(x\n(i)\n; {\\theta}) = {\\theta}\nx\n(i)(1 {-} {\\theta})\n(1{-}x\n(i) )). This time we are interested in computing\nthe variance of the estimator {\\textasciicircum}{\\theta}m =\n1mm\ni=1 x\n(i)\n.\nVar\n{\\textasciicircum}{\\theta}m\n = Var \n1\nm m\ni=1\nx\n(i)\n(5.48)\n=\n1\nm2 m\ni=1\nVar \nx\n(i)\n(5.49)\n=\n1\nm2 m\ni=1\n{\\theta}(1 {-} {\\theta}) (5.50)\n=\n1\nm2 m{\\theta}(1 {-} {\\theta}) (5.51) =\n1\nm\n{\\theta}(1 {-} {\\theta}) (5.52)\nThe variance of the estimator decreases as a function of m, the number of examples\nin the dataset. This is a common property of popular estimators that we will\nreturn to when we discuss consistency (see section 5.4.5).\n5.4.4 Trading off Bias and Variance to Minimize Mean Squared\nError\nBias and variance measure two different sources of error in an estimator. Bias\nmeasures the expected deviation from the true value of the function or parameter.\nVariance on the other hand, provides a measure of the deviation from the expected\nestimator value that any particular sampling of the data is likely to cause. What happens when we are given a choice between two estimators, one with\nmore bias and one with more variance? How do we choose between them?",
                    " For\nexample, imagine that we are interested in approximating the function shown in\nfigure 5.2 and we are only offered the choice between a model with large bias and\none that suffers from large variance. How do we choose between them?\nThe most common way to negotiate this trade-off is to use cross-validation.\nEmpirically, cross-validation is highly successful on many real-world tasks. Alternatively, we can also compare the mean squared error (MSE) of the estimates:\nMSE = E[({\\textasciicircum}{\\theta}m {-} {\\theta})\n2\n] (5.53) = Bias({\\textasciicircum}{\\theta}m)\n2 + Var({\\textasciicircum}{\\theta}m) (5.54)\nThe MSE measures the overall expected deviation{\\textemdash}in a squared error sense{\\textemdash}\nbetween the estimator and the true value of the parameter {\\theta}. As is clear from\nequation 5.54, evaluating the MSE incorporates both the bias and the variance.\nDesirable estimators are those with small MSE and these are estimators that\nmanage to keep both their bias and variance somewhat in check.\nCapacity\nBias Generalization\nerror Variance\nOptimal capacity\nUnderfitting zone Overfitting zone\nFigure 5.6: As capacity increases (x-axis), bias (dotted) tends to decrease and variance\n(dashed) tends to increase, yielding another U-shaped curve for generalization error (bold\ncurve). If we vary capacity along one axis, there is an optimal capacity, with underfitting\nwhen the capacity is below this optimum and overfitting when it is above. This relationship\nis similar to the relationship between capacity, underfitting, and overfitting, discussed in\nsection 5.2 and figure 5.3. The relationship between bias and variance is tightly linked to the machine\nlearning concepts of capacity, underfitting and overfitting. In the case where generalization error is measured by the MSE (where bias and variance are meaningful\ncomponents of generalization error), increasing capacity tends to increase variance\nand decrease bias. This is illustrated in figure 5.6, where we see again the U-shaped\ncurve of generalization error as a function of capacity.\n5.4.5 Consistency\nSo far we",
                    " have discussed the properties of various estimators for a training set of\nfixed size. Usually, we are also concerned with the behavior of an estimator as the\namount of training data grows. In particular, we usually wish that, as the number\nof data points m in our dataset increases, our point estimates converge to the true\nvalue of the corresponding parameters. More formally, we would like that\nplimm{\\textrightarrow}{\\infty}\n{\\textasciicircum}{\\theta}m = {\\theta}. (5.55)\nThe symbol plim indicates convergence in probability, meaning that for any  {>} 0, P (|{\\textasciicircum}{\\theta}m {-} {\\theta}| {>} ) {\\textrightarrow} 0 as m {\\textrightarrow} {\\infty}. The condition described by equation 5.55 is\nknown as consistency. It is sometimes referred to as weak consistency, with\nstrong consistency referring to the almost sure convergence of {\\textasciicircum}{\\theta} to {\\theta}. Almost\nsure convergence of a sequence of random variables x\n(1)\n, x\n(2)\n, . . . to a value x\noccurs when p(limm{\\textrightarrow}{\\infty} x\n(m) = x) = 1. Consistency ensures that the bias induced by the estimator diminishes as the\nnumber of data examples grows. However, the reverse is not true{\\textemdash}asymptotic\nunbiasedness does not imply consistency. For example, consider estimating the\nmean parameter {\\textmu} of a normal distribution N (x; {\\textmu}, {\\sigma}\n2), with a dataset consisting\nof m samples: {\\{}x\n(1)\n, . . . , x\n(m){\\}}. We could use the first sample x\n(1) of the dataset\nas an unbiased estimator: {\\textasciicircum}{\\theta} = x\n(1)\n. In that case, E({\\textasciicircum}{\\theta}m) = {\\theta} so the estimator\nis unbiased no matter how many data points are seen. This, of course, implies\nthat the estimate is asymptotically unbiased. However, this is not a consistent\nestimator as it is not the case that",
                    " {\\textasciicircum}{\\theta}m {\\textrightarrow} {\\theta} as m {\\textrightarrow} {\\infty}.\n5.5 Maximum Likelihood Estimation\nPreviously, we have seen some definitions of common estimators and analyzed\ntheir properties. But where did these estimators come from? Rather than guessing\nthat some function might make a good estimator and then analyzing its bias and\nvariance, we would like to have some principle from which we can derive specific\nfunctions that are good estimators for different models. The most common such principle is the maximum likelihood principle.\nConsider a set of m examples X = {\\{}x\n(1)\n, . . . , x\n(m){\\}} drawn independently from\nthe true but unknown data generating distribution pdata(x). Let pmodel(x;{\\theta}) be a parametric family of probability distributions over the\nsame space indexed by {\\theta}. In other words, pmodel(x;{\\theta} ) maps any configuration x\nto a real number estimating the true probability pdata(x). The maximum likelihood estimator for {\\theta} is then defined as\n{\\theta}ML = arg max\n{\\theta}\npmodel(X; {\\theta}) (5.56)\n= arg max\n{\\theta} m\ni=1\npmodel(x\n(i)\n; {\\theta}) (5.57)\nThis product over many probabilities can be inconvenient for a variety of reasons. For example, it is prone to numerical underflow. To obtain a more convenient\nbut equivalent optimization problem, we observe that taking the logarithm of the\nlikelihood does not change its arg max but does conveniently transform a product\ninto a sum:\n{\\theta}ML = arg max\n{\\theta} m\ni=1\nlog pmodel(x\n(i)\n; {\\theta}). (5.58)\nBecause the arg max does not change when we rescale the cost function, we can\ndivide by m to obtain a version of the criterion that is expressed as an expectation\nwith respect to the empirical distribution p{\\textasciicircum}data defined by the training data:\n{\\theta}ML = arg max\n{\\theta}\nEx{\\sim}p{\\textasciicircum}data\nlog pmodel(x; {\\theta",
                    "}). (5.59)\nOne way to interpret maximum likelihood estimation is to view it as minimizing\nthe dissimilarity between the empirical distribution p{\\textasciicircum}data defined by the training\nset and the model distribution, with the degree of dissimilarity between the two\nmeasured by the KL divergence. The KL divergence is given by\nDKL (p{\\textasciicircum}data pmodel) = Ex{\\sim}p{\\textasciicircum}data\n[log p{\\textasciicircum}data (x) {-} log pmodel(x)] . (5.60)\nThe term on the left is a function only of the data generating process, not the\nmodel. This means when we train the model to minimize the KL divergence, we\nneed only minimize {-} Ex{\\sim}p{\\textasciicircum}data\n[log pmodel(x)] (5.61)\nwhich is of course the same as the maximization in equation 5.59. Minimizing this KL divergence corresponds exactly to minimizing the cross- entropy between the distributions. Many authors use the term {\\textquotedblleft}cross-entropy{\\textquotedblright} to\nidentify specifically the negative log-likelihood of a Bernoulli or softmax distribution,\nbut that is a misnomer. Any loss consisting of a negative log-likelihood is a cross- entropy between the empirical distribution defined by the training set and the\nprobability distribution defined by model. For example, mean squared error is the\ncross-entropy between the empirical distribution and a Gaussian model.\nWe can thus see maximum likelihood as an attempt to make the model dis- tribution match the empirical distribution p{\\textasciicircum}data. Ideally, we would like to match\nthe true data generating distribution pdata, but we have no direct access to this\ndistribution.\nWhile the optimal {\\theta} is the same regardless of whether we are maximizing the\nlikelihood or minimizing the KL divergence, the values of the objective functions\nare different. In software, we often phrase both as minimizing a cost function.\nMaximum likelihood thus becomes minimization of the negative log-likelihood\n(NLL), or equivalently, minimization of the cross entropy. The perspective of\nmaximum likelihood as minimum KL divergence becomes helpful in this case\nbecause the KL divergence has a known minimum value of zero",
                    ". The negative\nlog-likelihood can actually become negative when x is real-valued.\n5.5.1 Conditional Log-Likelihood and Mean Squared Error\nThe maximum likelihood estimator can readily be generalized to the case where\nour goal is to estimate a conditional probability P(y | x;{\\theta}) in order to predict y\ngiven x. This is actually the most common situation because it forms the basis for\nmost supervised learning. If X represents all our inputs and Y all our observed\ntargets, then the conditional maximum likelihood estimator is\n{\\theta}ML = arg max\n{\\theta}\nP(Y | X; {\\theta}). (5.62)\nIf the examples are assumed to be i.i.d., then this can be decomposed into\n{\\theta}ML = arg max\n{\\theta} m\ni=1\nlog P(y\n(i)\n| x\n(i)\n; {\\theta}). (5.63)\nExample: Linear Regression as Maximum Likelihood Linear regression,\nintroduced earlier in section 5.1.4, may be justified as a maximum likelihood\nprocedure. Previously, we motivated linear regression as an algorithm that learns\nto take an input x and produce an output value y{\\textasciicircum}. The mapping from x to y{\\textasciicircum} is\nchosen to minimize mean squared error, a criterion that we introduced more or less\narbitrarily. We now revisit linear regression from the point of view of maximum\nlikelihood estimation. Instead of producing a single prediction y{\\textasciicircum}, we now think\nof the model as producing a conditional distribution p(y | x). We can imagine\nthat with an infinitely large training set, we might see several training examples\nwith the same input value x but different values of y. The goal of the learning\nalgorithm is now to fit the distribution p(y | x) to all of those different y values\nthat are all compatible with x. To derive the same linear regression algorithm\nwe obtained before, we define p(y | x) = N (y;y{\\textasciicircum}(x; w), {\\sigma}\n2). The function y{\\textasciicircum}(x; w)\ngives the prediction of the mean of the Gaussian. In this example, we assume",
                    " that\nthe variance is fixed to some constant {\\sigma}\n2 chosen by the user. We will see that this\nchoice of the functional form of p(y | x) causes the maximum likelihood estimation\nprocedure to yield the same learning algorithm as we developed before. Since the\nexamples are assumed to be i.i.d., the conditional log-likelihood (equation 5.63) is\ngiven by\nm\ni=1\nlog p(y\n(i)\n| x\n(i)\n; {\\theta}) (5.64)\n= {-} mlog {\\sigma} {-} m\n2\nlog(2{\\pi}) {-}m\ni=1\n\ny{\\textasciicircum}\n(i) {-} y\n(i)2\n2{\\sigma}2\n, (5.65)\nwhere y{\\textasciicircum}\n(i)\nis the output of the linear regression on the i-th input x\n(i) and m is the\nnumber of the training examples. Comparing the log-likelihood with the mean\nsquared error,\nMSEtrain =\n1\nm m\ni=1\n||y{\\textasciicircum}\n(i) {-} y\n(i)\n||2\n, (5.66)\nwe immediately see that maximizing the log-likelihood with respect to w yields\nthe same estimate of the parameters w as does minimizing the mean squared error.\nThe two criteria have different values but the same location of the optimum. This\njustifies the use of the MSE as a maximum likelihood estimation procedure. As we\nwill see, the maximum likelihood estimator has several desirable properties.\n5.5.2 Properties of Maximum Likelihood\nThe main appeal of the maximum likelihood estimator is that it can be shown to\nbe the best estimator asymptotically, as the number of examples m {\\textrightarrow} {\\infty}, in terms\nof its rate of convergence as m increases. Under appropriate conditions, the maximum likelihood estimator has the\nproperty of consistency (see section 5.4.5 above), meaning that as the number\nof training examples approaches infinity, the maximum likelihood estimate of a\nparameter converges to the true value of the parameter. These conditions are:\n The true distribution pdata must lie within the model family pmodel",
                    "({\\textperiodcentered}; {\\theta}). Otherwise, no estimator can recover pdata .\n The true distribution pdata must correspond to exactly one value of {\\theta}. Otherwise, maximum likelihood can recover the correct pdata , but will not be able\nto determine which value of {\\theta} was used by the data generating processing.\nThere are other inductive principles besides the maximum likelihood estimator, many of which share the property of being consistent estimators. However,\nconsistent estimators can differ in their statistic efficiency, meaning that one\nconsistent estimator may obtain lower generalization error for a fixed number of\nsamples m, or equivalently, may require fewer examples to obtain a fixed level of\ngeneralization error.\nStatistical efficiency is typically studied in the parametric case (like in linear\nregression) where our goal is to estimate the value of a parameter (and assuming\nit is possible to identify the true parameter), not the value of a function. A way to\nmeasure how close we are to the true parameter is by the expected mean squared\nerror, computing the squared difference between the estimated and true parameter\nvalues, where the expectation is over m training samples from the data generating\ndistribution. That parametric mean squared error decreases as m increases, and\nfor m large, the Cram{\\'e}r-Rao lower bound (Rao, 1945; Cram{\\'e}r, 1946) shows that no\nconsistent estimator has a lower mean squared error than the maximum likelihood\nestimator. For these reasons (consistency and efficiency), maximum likelihood is often\nconsidered the preferred estimator to use for machine learning. When the number\nof examples is small enough to yield overfitting behavior, regularization strategies\nsuch as weight decay may be used to obtain a biased version of maximum likelihood\nthat has less variance when training data is limited.\n5.6 Bayesian Statistics\nSo far we have discussed frequentist statistics and approaches based on estimating a single value of {\\theta}, then making all predictions thereafter based on that one\nestimate. Another approach is to consider all possible values of {\\theta} when making a\nprediction. The latter is the domain of Bayesian statistics. As discussed in section 5.4.1, the frequentist perspective is that the true\nparameter value {\\theta} is fixed but unknown, while the point estimate {\\theta}{\\textasciicircum} is a random",
                    "\nvariable on account of it being a function of the dataset (which is seen as random).\nThe Bayesian perspective on statistics is quite different. The Bayesian uses\nprobability to reflect degrees of certainty of states of knowledge. The dataset is\ndirectly observed and so is not random. On the other hand, the true parameter {\\theta}\nis unknown or uncertain and thus is represented as a random variable.\nBefore observing the data, we represent our knowledge of {\\theta} using the prior\nprobability distribution, p({\\theta}) (sometimes referred to as simply {\\textquotedblleft}the prior{\\textquotedblright}).\nGenerally, the machine learning practitioner selects a prior distribution that is\nquite broad (i.e. with high entropy) to reflect a high degree of uncertainty in the\nvalue of {\\theta} before observing any data. For example, one might assume a priori that\n{\\theta} lies in some finite range or volume, with a uniform distribution. Many priors\ninstead reflect a preference for {\\textquotedblleft}simpler{\\textquotedblright} solutions (such as smaller magnitude\ncoefficients, or a function that is closer to being constant). Now consider that we have a set of data samples {\\{}x\n(1)\n, . . . , x\n(m) {\\}}. We can\nrecover the effect of data on our belief about {\\theta} by combining the data likelihood\np(x\n(1)\n, . . . , x\n(m)\n| {\\theta}) with the prior via Bayes` rule:\np({\\theta} | x\n(1)\n, . . . , x\n(m)) =\np(x\n(1)\n, . . . , x\n(m)\n| {\\theta})p({\\theta})\np(x(1)\n, . . . , x(m) )\n(5.67)\nIn the scenarios where Bayesian estimation is typically used, the prior begins as a\nrelatively uniform or Gaussian distribution with high entropy, and the observation\nof the data usually causes the posterior to lose entropy and concentrate around a\nfew highly likely values of the parameters. Relative to maximum likelihood estimation, Bayesian estimation offers two\nimportant differences. First, unlike the maximum likelihood approach that makes\npredictions using a point estimate of {\\theta}, the Bay",
                    "esian approach is to make predictions\nusing a full distribution over {\\theta}. For example, after observing m examples, the\npredicted distribution over the next data sample, x\n(m+1)\n, is given by\np(x\n(m+1)\n| x\n(1)\n, . . . , x\n(m) ) =\n\np(x\n(m+1)\n| {\\theta})p({\\theta} | x\n(1)\n, . . . , x\n(m) ) d{\\theta}. (5.68)\nHere each value of {\\theta} with positive probability density contributes to the prediction\nof the next example, with the contribution weighted by the posterior density itself.\nAfter having observed {\\{}x\n(1)\n, . . . , x\n(m){\\}}, if we are still quite uncertain about the\nvalue of {\\theta}, then this uncertainty is incorporated directly into any predictions we\nmight make.\nIn section 5.4, we discussed how the frequentist approach addresses the uncertainty in a given point estimate of {\\theta} by evaluating its variance. The variance of\nthe estimator is an assessment of how the estimate might change with alternative\nsamplings of the observed data. The Bayesian answer to the question of how to deal\nwith the uncertainty in the estimator is to simply integrate over it, which tends to\nprotect well against overfitting. This integral is of course just an application of\nthe laws of probability, making the Bayesian approach simple to justify, while the\nfrequentist machinery for constructing an estimator is based on the rather ad hoc\ndecision to summarize all knowledge contained in the dataset with a single point\nestimate.\nThe second important difference between the Bayesian approach to estimation\nand the maximum likelihood approach is due to the contribution of the Bayesian\nprior distribution. The prior has an influence by shifting probability mass density\ntowards regions of the parameter space that are preferred a priori. In practice,\nthe prior often expresses a preference for models that are simpler or more smooth.\nCritics of the Bayesian approach identify the prior as a source of subjective human\njudgment impacting the predictions.\nBayesian methods typically generalize much better when limited training data\nis available, but typically suffer from high computational cost when the number of\ntraining examples is large.\nExample: Bayesian Linear Regression Here we consider the Bayesian estimation",
                    " approach to learning the linear regression parameters. In linear regression,\nwe learn a linear mapping from an input vector x {\\in} Rn to predict the value of a\nscalar y {\\in} R. The prediction is parametrized by the vector w {\\in} Rn\n:\ny{\\textasciicircum} = wx. (5.69)\nGiven a set of m training samples (X(train)\n, y\n(train) ), we can express the prediction\nof y over the entire training set as:\ny{\\textasciicircum}\n(train) = X(train)w. (5.70)\nExpressed as a Gaussian conditional distribution on y\n(train)\n, we have\np(y\n(train)\n| X(train)\n, w) = N (y\n(train)\n; X(train)w, I) (5.71)\n{\\propto} exp {-}\n1\n2\n(y\n(train) {-} X(train)w)\n(y\n(train) {-} X(train)w)\n,\n(5.72)\nwhere we follow the standard MSE formulation in assuming that the Gaussian\nvariance on y is one. In what follows, to reduce the notational burden, we refer to\n(X(train)\n, y\n(train)) as simply (X, y). To determine the posterior distribution over the model parameter vector w, we\nfirst need to specify a prior distribution. The prior should reflect our naive belief\nabout the value of these parameters. While it is sometimes difficult or unnatural\nto express our prior beliefs in terms of the parameters of the model, in practice we\ntypically assume a fairly broad distribution expressing a high degree of uncertainty\nabout {\\theta}. For real-valued parameters it is common to use a Gaussian as a prior\ndistribution:\np(w) = N (w; {\\textmu}0 , {\\Lambda}0) {\\propto} exp {-}\n1\n2\n(w {-} {\\textmu}0){\\Lambda}\n{-}1\n0 (w {-} {\\textmu}0)\n, (5.73)\nwhere {\\textmu}0 and {\\Lambda}0 are the prior distribution mean vector and covariance matrix\nrespectively.",
                    "1\nWith the prior thus specified, we can now proceed in determining the posterior\ndistribution over the model parameters. p(w | X, y) {\\propto} p(y | X, w)p(w) (5.74)\n{\\propto} exp {-}\n1\n2\n(y {-} Xw) (y {-} Xw)\nexp{-}\n1\n2\n(w {-} {\\textmu}0){\\Lambda}\n{-}1\n0 (w {-} {\\textmu}0)\n(5.75)\n{\\propto} exp {-}\n1\n2\n{-}2yXw + w XXw + w{\\Lambda}\n{-}1\n0 w {-} 2{\\textmu}\n\n0 {\\Lambda}\n{-}1\n0 w\n\n.\n(5.76)\nWe now define {\\Lambda}m =\nXX + {\\Lambda}\n{-}1\n0 {-}1 and {\\textmu}m = {\\Lambda}m\nX y + {\\Lambda}\n{-}1\n0 {\\textmu}0 \n. Using\nthese new variables, we find that the posterior may be rewritten as a Gaussian\ndistribution:\np(w | X, y) {\\propto} exp {-}\n1\n2\n(w {-} {\\textmu}m) {\\Lambda}\n{-}1 m (w {-} {\\textmu} m) +\n1\n2{\\textmu}\nm{\\Lambda}\n{-}1 m {\\textmu}m\n(5.77)\n{\\propto} exp {-}\n1\n2\n(w {-} {\\textmu}m) {\\Lambda}\n{-}1 m (w {-} {\\textmu} m)\n. (5.78)\nAll terms that do not include the parameter vector w have been omitted; they\nare implied by the fact that the distribution must be normalized to integrate to 1. Equation 3.23 shows how to normalize a multivariate Gaussian distribution.\nExamining this posterior distribution allows us to gain some intuition for the\neffect of Bayesian inference. In most situations, we set {\\",
                    "textmu}0 to 0. If we set {\\Lambda}0 = 1{\\alpha}I, then {\\textmu}m gives the same estimate of w as does frequentist linear regression with a\nweight decay penalty of {\\alpha}ww. One difference is that the Bayesian estimate is\nundefined if {\\alpha} is set to zero{\\textemdash}-we are not allowed to begin the Bayesian learning\nprocess with an infinitely wide prior on w. The more important difference is that\nthe Bayesian estimate provides a covariance matrix, showing how likely all the\ndifferent values of w are, rather than providing only the estimate {\\textmu}m.\n5.6.1 Maximum A Posteriori (MAP) Estimation\nWhile the most principled approach is to make predictions using the full Bayesian\nposterior distribution over the parameter {\\theta} , it is still often desirable to have a\n1 Unless there is a reason to assume a particular covariance structure, we typically assume a\ndiagonal covariance matrix {\\Lambda}0 = diag({\\lambda}0).\nsingle point estimate. One common reason for desiring a point estimate is that\nmost operations involving the Bayesian posterior for most interesting models are\nintractable, and a point estimate offers a tractable approximation. Rather than\nsimply returning to the maximum likelihood estimate, we can still gain some of\nthe benefit of the Bayesian approach by allowing the prior to influence the choice\nof the point estimate. One rational way to do this is to choose the maximum\na posteriori (MAP) point estimate. The MAP estimate chooses the point of\nmaximal posterior probability (or maximal probability density in the more common\ncase of continuous {\\theta}):\n{\\theta}MAP = arg max\n{\\theta}\np({\\theta} | x) = arg max\n{\\theta}\nlog p(x | {\\theta}) + log p({\\theta}). (5.79)\nWe recognize, above on the right hand side, log p(x | {\\theta}), i.e. the standard loglikelihood term, and log p({\\theta}), corresponding to the prior distribution.\nAs an example, consider a linear regression model with a Gaussian prior on\nthe weights w. If this prior is given by N(w;0, 1\n{\\lambda}I\n2), then the log-prior term in\n",
                    "equation 5.79 is proportional to the familiar {\\lambda}ww weight decay penalty, plus a\nterm that does not depend on w and does not affect the learning process. MAP\nBayesian inference with a Gaussian prior on the weights thus corresponds to weight\ndecay. As with full Bayesian inference, MAP Bayesian inference has the advantage of\nleveraging information that is brought by the prior and cannot be found in the\ntraining data. This additional information helps to reduce the variance in the\nMAP point estimate (in comparison to the ML estimate). However, it does so at\nthe price of increased bias. Many regularized estimation strategies, such as maximum likelihood learning\nregularized with weight decay, can be interpreted as making the MAP approximation to Bayesian inference. This view applies when the regularization consists of\nadding an extra term to the objective function that corresponds to log p({\\theta} ). Not\nall regularization penalties correspond to MAP Bayesian inference. For example,\nsome regularizer terms may not be the logarithm of a probability distribution.\nOther regularization terms depend on the data, which of course a prior probability\ndistribution is not allowed to do.\nMAP Bayesian inference provides a straightforward way to design complicated\nyet interpretable regularization terms. For example, a more complicated penalty\nterm can be derived by using a mixture of Gaussians, rather than a single Gaussian\ndistribution, as the prior (Nowlan and Hinton, 1992).\n\n\n\n\n\n\n\n5.7 Supervised Learning Algorithms\nRecall from section 5.1.3 that supervised learning algorithms are, roughly speaking,\nlearning algorithms that learn to associate some input with some output, given a\ntraining set of examples of inputs x and outputs y. In many cases the outputs\ny may be difficult to collect automatically and must be provided by a human\n{\\textquotedblleft}supervisor,{\\textquotedblright} but the term still applies even when the training set targets were\ncollected automatically.\n5.7.1 Probabilistic Supervised Learning\nMost supervised learning algorithms in this book are based on estimating a\nprobability distribution p(y | x). We can do this simply by using maximum\nlikelihood estimation to find the best parameter vector {\\theta} for a parametric family\nof distributions p(y | x; {\\theta}). We have already seen that linear regression corresponds to the family\np(y | x; {\\",
                    "theta}) = N (y; {\\theta}x, I). (5.80)\nWe can generalize linear regression to the classification scenario by defining a\ndifferent family of probability distributions. If we have two classes, class 0 and\nclass 1, then we need only specify the probability of one of these classes. The\nprobability of class 1 determines the probability of class 0, because these two values\nmust add up to 1.\nThe normal distribution over real-valued numbers that we used for linear\nregression is parametrized in terms of a mean. Any value we supply for this mean\nis valid. A distribution over a binary variable is slightly more complicated, because\nits mean must always be between 0 and 1. One way to solve this problem is to use\nthe logistic sigmoid function to squash the output of the linear function into the\ninterval (0, 1) and interpret that value as a probability:\np(y = 1 | x; {\\theta}) = {\\sigma}({\\theta}x). (5.81)\nThis approach is known as logistic regression (a somewhat strange name since\nwe use the model for classification rather than regression).\nIn the case of linear regression, we were able to find the optimal weights by\nsolving the normal equations. Logistic regression is somewhat more difficult. There\nis no closed-form solution for its optimal weights. Instead, we must search for\nthem by maximizing the log-likelihood. We can do this by minimizing the negative\nlog-likelihood (NLL) using gradient descent.\nThis same strategy can be applied to essentially any supervised learning problem,\nby writing down a parametric family of conditional probability distributions over\nthe right kind of input and output variables.\n5.7.2 Support Vector Machines\nOne of the most influential approaches to supervised learning is the support vector\nmachine (Boser et al., 1992; Cortes and Vapnik, 1995). This model is similar to\nlogistic regression in that it is driven by a linear function wx + b. Unlike logistic\nregression, the support vector machine does not provide probabilities, but only\noutputs a class identity. The SVM predicts that the positive class is present when\nw x + b is positive. Likewise, it predicts that the negative class is present when\nw x + b is negative.\nOne key innovation associated with support vector machines is the kernel\ntrick. The kernel trick",
                    " consists of observing that many machine learning algorithms\ncan be written exclusively in terms of dot products between examples. For example,\nit can be shown that the linear function used by the support vector machine can\nbe re-written as\nw\nx + b = b +m\ni=1\n{\\alpha}ix\nx\n(i) (5.82)\nwhere x\n(i)\nis a training example and {\\alpha} is a vector of coefficients. Rewriting the\nlearning algorithm this way allows us to replace x by the output of a given feature\nfunction {\\varphi}(x) and the dot product with a function k(x, x\n(i)) = {\\varphi}(x){\\textperiodcentered}{\\varphi}(x\n(i) ) called\na kernel. The {\\textperiodcentered} operator represents an inner product analogous to {\\varphi}(x){\\varphi}(x\n(i)). For some feature spaces, we may not use literally the vector inner product. In\nsome infinite dimensional spaces, we need to use other kinds of inner products, for\nexample, inner products based on integration rather than summation. A complete\ndevelopment of these kinds of inner products is beyond the scope of this book.\nAfter replacing dot products with kernel evaluations, we can make predictions\nusing the function\nf(x) = b + \ni\n{\\alpha}ik(x, x\n(i) ). (5.83)\nThis function is nonlinear with respect to x, but the relationship between {\\varphi}(x)\nand f (x) is linear. Also, the relationship between {\\alpha} and f(x) is linear. The\nkernel-based function is exactly equivalent to preprocessing the data by applying\n{\\varphi}(x) to all inputs, then learning a linear model in the new transformed space.\nThe kernel trick is powerful for two reasons. First, it allows us to learn models\nthat are nonlinear as a function of x using convex optimization techniques that are\nguaranteed to converge efficiently. This is possible because we consider {\\varphi} fixed and\noptimize only {\\alpha}, i.e., the optimization algorithm can view the decision function\nas being linear in a different space. Second, the kernel function k often admits\nan implementation that is significantly more computational efficient than naively\nconstructing two {\\varphi}(x) vectors and explicitly taking their dot product",
                    ".\nIn some cases, {\\varphi}(x) can even be infinite dimensional, which would result in\nan infinite computational cost for the naive, explicit approach. In many cases, k(x, x\n) is a nonlinear, tractable function of x even when {\\varphi}(x) is intractable. As\nan example of an infinite-dimensional feature space with a tractable kernel, we\nconstruct a feature mapping {\\varphi}(x) over the non-negative integers x. Suppose that\nthis mapping returns a vector containing x ones followed by infinitely many zeros. We can write a kernel function k(x, x\n(i)) = min(x, x\n(i)) that is exactly equivalent\nto the corresponding infinite-dimensional dot product.\nThe most commonly used kernel is the Gaussian kernel\nk(u, v) = N (u {-} v; 0, {\\sigma}\n2I) (5.84)\nwhere N(x; {\\textmu}, {\\Sigma}) is the standard normal density. This kernel is also known as\nthe radial basis function (RBF) kernel, because its value decreases along lines\nin v space radiating outward from u. The Gaussian kernel corresponds to a dot\nproduct in an infinite-dimensional space, but the derivation of this space is less\nstraightforward than in our example of the min kernel over the integers. We can think of the Gaussian kernel as performing a kind of template matching. A training example x associated with training label y becomes a template\nfor class y. When a test point x\n is near x according to Euclidean distance, the\nGaussian kernel has a large response, indicating that x\n is very similar to the x\ntemplate. The model then puts a large weight on the associated training label y. Overall, the prediction will combine many such training labels weighted by the\nsimilarity of the corresponding training examples. Support vector machines are not the only algorithm that can be enhanced\nusing the kernel trick. Many other linear models can be enhanced in this way. The\ncategory of algorithms that employ the kernel trick is known as kernel machines\nor kernel methods (Williams and Rasmussen, 1996; Sch{\\\"o}lkopf et al., 1999). A major drawback to kernel machines is that the cost of evaluating the decision\nfunction is linear in the number of training examples, because the i-th example\ncontributes a term {\\alpha}ik(x,",
                    " x\n(i)) to the decision function. Support vector machines\nare able to mitigate this by learning an {\\alpha} vector that contains mostly zeros. Classifying a new example then requires evaluating the kernel function only for\nthe training examples that have non-zero {\\alpha}i. These training examples are known\nas support vectors. Kernel machines also suffer from a high computational cost of training when\nthe dataset is large. We will revisit this idea in section 5.9. Kernel machines with\ngeneric kernels struggle to generalize well. We will explain why in section 5.11. The\nmodern incarnation of deep learning was designed to overcome these limitations of\nkernel machines. The current deep learning renaissance began when Hinton et al. (2006) demonstrated that a neural network could outperform the RBF kernel SVM\non the MNIST benchmark"
                ]
            },
            {
                "6": [
                    "Chapter 6\nDeep Feedforward Networks\nDeep feedforward networks, also often called feedforward neural networks, or multilayer perceptrons (MLPs), are the quintessential deep learning models. The goal of a feedforward network is to approximate some function f\n{*}\n. For example,\nfor a classifier, y = f\n{*}(x) maps an input x to a category y. A feedforward network\ndefines a mapping y = f(x; {\\theta}) and learns the value of the parameters {\\theta} that result\nin the best function approximation.\nThese models are called feedforward because information flows through the\nfunction being evaluated from x, through the intermediate computations used to\ndefine f, and finally to the output y. There are no feedback connections in which\noutputs of the model are fed back into itself. When feedforward neural networks\nare extended to include feedback connections, they are called recurrent neural\nnetworks, presented in chapter 10. Feedforward networks are of extreme importance to machine learning practitioners. They form the basis of many important commercial applications. For\nexample, the convolutional networks used for object recognition from photos are a\nspecialized kind of feedforward network. Feedforward networks are a conceptual\nstepping stone on the path to recurrent networks, which power many natural\nlanguage applications. Feedforward neural networks are called networks because they are typically\nrepresented by composing together many different functions. The model is associated with a directed acyclic graph describing how the functions are composed\ntogether. For example, we might have three functions f\n(1)\n, f\n(2)\n, and f\n(3) connected\nin a chain, to form f(x) = f\n(3)(f\n(2)(f\n(1)(x))). These chain structures are the most\ncommonly used structures of neural networks. In this case, f\n(1)\nis called the first\nlayer of the network, f\n(2)\nis called the second layer, and so on. The overall\n168\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nlength of the chain gives the depth of the model. It is from this terminology that\nthe name {\\textquotedblleft}deep learning{\\textquotedblright} arises. The final layer of a feedforward network is called\nthe output layer. During neural network training, we drive f(x) to match f\n{*}(",
                    "x). The training data provides us with noisy, approximate examples of f\n{*}(x) evaluated\nat different training points. Each example x is accompanied by a label y {\\approx} f\n{*}(x). The training examples specify directly what the output layer must do at each point\nx; it must produce a value that is close to y. The behavior of the other layers is\nnot directly specified by the training data. The learning algorithm must decide\nhow to use those layers to produce the desired output, but the training data does\nnot say what each individual layer should do. Instead, the learning algorithm must\ndecide how to use these layers to best implement an approximation of f\n{*}\n. Because\nthe training data does not show the desired output for each of these layers, these\nlayers are called hidden layers. Finally, these networks are called neural because they are loosely inspired by\nneuroscience. Each hidden layer of the network is typically vector-valued. The\ndimensionality of these hidden layers determines the width of the model. Each\nelement of the vector may be interpreted as playing a role analogous to a neuron.\nRather than thinking of the layer as representing a single vector-to-vector function,\nwe can also think of the layer as consisting of many units that act in parallel,\neach representing a vector-to-scalar function. Each unit resembles a neuron in\nthe sense that it receives input from many other units and computes its own\nactivation value. The idea of using many layers of vector-valued representation\nis drawn from neuroscience. The choice of the functions f\n(i)(x) used to compute\nthese representations is also loosely guided by neuroscientific observations about\nthe functions that biological neurons compute. However, modern neural network\nresearch is guided by many mathematical and engineering disciplines, and the\ngoal of neural networks is not to perfectly model the brain. It is best to think of\nfeedforward networks as function approximation machines that are designed to\nachieve statistical generalization, occasionally drawing some insights from what we\nknow about the brain, rather than as models of brain function.\nOne way to understand feedforward networks is to begin with linear models\nand consider how to overcome their limitations. Linear models, such as logistic\nregression and linear regression, are appealing because they may be fit efficiently\nand reliably, either in closed form or with convex optimization. Linear models also\nhave the obvious defect that the model capacity is limited to linear functions",
                    ", so\nthe model cannot understand the interaction between any two input variables. To extend linear models to represent nonlinear functions of x, we can apply\nthe linear model not to x itself but to a transformed input {\\varphi}(x), where {\\varphi} is a\n169\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nnonlinear transformation. Equivalently, we can apply the kernel trick described in\nsection 5.7.2, to obtain a nonlinear learning algorithm based on implicitly applying\nthe {\\varphi} mapping. We can think of {\\varphi} as providing a set of features describing x, or\nas providing a new representation for x. The question is then how to choose the mapping {\\varphi}.\n1. One option is to use a very generic {\\varphi}, such as the infinite-dimensional {\\varphi} that\nis implicitly used by kernel machines based on the RBF kernel. If {\\varphi}(x) is\nof high enough dimension, we can always have enough capacity to fit the\ntraining set, but generalization to the test set often remains poor. Very\ngeneric feature mappings are usually based only on the principle of local\nsmoothness and do not encode enough prior information to solve advanced\nproblems.\n2. Another option is to manually engineer {\\varphi}. Until the advent of deep learning,\nthis was the dominant approach. This approach requires decades of human\neffort for each separate task, with practitioners specializing in different\ndomains such as speech recognition or computer vision, and with little\ntransfer between domains.\n3. The strategy of deep learning is to learn {\\varphi}. In this approach, we have a model\ny = f(x; {\\theta}, w) = {\\varphi}(x; {\\theta})w. We now have parameters {\\theta} that we use to learn\n{\\varphi} from a broad class of functions, and parameters w that map from {\\varphi}(x) to\nthe desired output. This is an example of a deep feedforward network, with\n{\\varphi} defining a hidden layer. This approach is the only one of the three that\ngives up on the convexity of the training problem, but the benefits outweigh\nthe harms. In this approach, we parametrize the representation as {\\varphi}(x; {\\theta})\nand use the optimization algorithm to find the {\\theta} that corresponds to a good",
                    "\nrepresentation. If we wish, this approach can capture the benefit of the first\napproach by being highly generic{\\textemdash}we do so by using a very broad family\n{\\varphi}(x; {\\theta}). This approach can also capture the benefit of the second approach.\nHuman practitioners can encode their knowledge to help generalization by\ndesigning families {\\varphi}(x; {\\theta}) that they expect will perform well. The advantage\nis that the human designer only needs to find the right general function\nfamily rather than finding precisely the right function.\nThis general principle of improving models by learning features extends beyond\nthe feedforward networks described in this chapter. It is a recurring theme of deep\nlearning that applies to all of the kinds of models described throughout this book.\nFeedforward networks are the application of this principle to learning deterministic\n170\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nmappings from x to y that lack feedback connections. Other models presented\nlater will apply these principles to learning stochastic mappings, learning functions\nwith feedback, and learning probability distributions over a single vector.\nWe begin this chapter with a simple example of a feedforward network. Next, we address each of the design decisions needed to deploy a feedforward network.\nFirst, training a feedforward network requires making many of the same design\ndecisions as are necessary for a linear model: choosing the optimizer, the cost\nfunction, and the form of the output units. We review these basics of gradient-based\nlearning, then proceed to confront some of the design decisions that are unique\nto feedforward networks. Feedforward networks have introduced the concept of a\nhidden layer, and this requires us to choose the activation functions that will\nbe used to compute the hidden layer values. We must also design the architecture\nof the network, including how many layers the network should contain, how these\nlayers should be connected to each other, and how many units should be in\neach layer. Learning in deep neural networks requires computing the gradients\nof complicated functions. We present the back-propagation algorithm and its\nmodern generalizations, which can be used to efficiently compute these gradients. Finally, we close with some historical perspective.\n6.1 Example: Learning XOR\nTo make the idea of a feedforward network more concrete, we begin with an\nexample of a fully functioning feedforward network on a very simple task: learning\nthe XOR function.",
                    "\nThe XOR function ({\\textquotedblleft}exclusive or{\\textquotedblright}) is an operation on two binary values, x1\nand x2. When exactly one of these binary values is equal to 1, the XOR function\nreturns 1. Otherwise, it returns 0. The XOR function provides the target function\ny = f\n{*}(x) that we want to learn. Our model provides a function y = f(x;{\\theta}) and\nour learning algorithm will adapt the parameters {\\theta} to make f as similar as possible\nto f\n{*}\n.\nIn this simple example, we will not be concerned with statistical generalization.\nWe want our network to perform correctly on the four points X = {\\{}[0, 0], [0,1],\n[1, 0], and [1, 1]{\\}}. We will train the network on all four of these points. The\nonly challenge is to fit the training set. We can treat this problem as a regression problem and use a mean squared\nerror loss function. We choose this loss function to simplify the math for this\nexample as much as possible. In practical applications, MSE is usually not an\n171\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nappropriate cost function for modeling binary data. More appropriate approaches\nare described in section 6.2.2.2. Evaluated on our whole training set, the MSE loss function is\nJ({\\theta}) =\n1\n4 \nx{\\in}X\n(f\n{*}(x) {-} f(x; {\\theta}))\n2\n. (6.1)\nNow we must choose the form of our model, f (x;{\\theta}). Suppose that we choose\na linear model, with {\\theta} consisting of w and b. Our model is defined to be\nf(x; w, b) = x\nw + b. (6.2)\nWe can minimize J({\\theta}) in closed form with respect to w and b using the normal\nequations. After solving the normal equations, we obtain w = 0 and b = 1\n2\n. The linear\nmodel simply outputs 0.5 everywhere. Why does this happen? Figure 6.1 shows\nhow a linear model is not able to represent the XOR function. One way to solve\nthis problem is to",
                    " use a model that learns a different feature space in which a\nlinear model is able to represent the solution.\nSpecifically, we will introduce a very simple feedforward network with one\nhidden layer containing two hidden units. See figure 6.2 for an illustration of\nthis model. This feedforward network has a vector of hidden units h that are\ncomputed by a function f\n(1)(x; W, c). The values of these hidden units are then\nused as the input for a second layer. The second layer is the output layer of the\nnetwork. The output layer is still just a linear regression model, but now it is\napplied to h rather than to x. The network now contains two functions chained\ntogether: h = f\n(1)(x;W, c) and y = f\n(2)(h; w, b), with the complete model being\nf(x;W, c, w, b) = f\n(2) (f\n(1)(x)). What function should f\n(1) compute? Linear models have served us well so far,\nand it may be tempting to make f\n(1) be linear as well. Unfortunately, if f\n(1) were\nlinear, then the feedforward network as a whole would remain a linear function of\nits input. Ignoring the intercept terms for the moment, suppose f\n(1) (x) = Wx\nand f\n(2)(h) = h\nw. Then f (x) = wW x. We could represent this function as\nf(x) = x\nw where w = Ww. Clearly, we must use a nonlinear function to describe the features. Most neural\nnetworks do so using an affine transformation controlled by learned parameters,\nfollowed by a fixed, nonlinear function called an activation function. We use that\nstrategy here, by defining h = g(W x + c), where W provides the weights of a\nlinear transformation and c the biases. Previously, to describe a linear regression\n172\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n0 1 x1\n0\n1\nx2\nOriginal x space\n0 1 2\nh1\n0\n1\nh2\nLearned h space\nFigure 6.1: Solving the XOR problem by learning a representation. The bold numbers\nprinted on the plot indicate the value that the learned function must output at each point. (Left)",
                    "A linear model applied directly to the original input cannot implement the XOR\nfunction. When x1 = 0, the model`s output must increase as x2\nincreases. When x1 = 1, the model`s output must decrease as x2 increases. A linear model must apply a fixed\ncoefficient w2 to x2. The linear model therefore cannot use the value of x1 to change\nthe coefficient on x2 and cannot solve this problem. (Right)In the transformed space\nrepresented by the features extracted by a neural network, a linear model can now solve\nthe problem. In our example solution, the two points that must have output 1 have been\ncollapsed into a single point in feature space. In other words, the nonlinear features have\nmapped both x = [1, 0]  and x = [0, 1] to a single point in feature space, h = [1 ,0] . The linear model can now describe the function as increasing in h1 and decreasing in h2. In this example, the motivation for learning the feature space is only to make the model\ncapacity greater so that it can fit the training set. In more realistic applications, learned\nrepresentations can also help the model to generalize.\n173\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ny\nh\nx W\nw\ny\nh1\nx1\nh2\nx2\nFigure 6.2: An example of a feedforward network, drawn in two different styles. Specifically, this is the feedforward network we use to solve the XOR example. It has a single hidden\nlayer containing two units. (Left)In this style, we draw every unit as a node in the graph. This style is very explicit and unambiguous but for networks larger than this example\nit can consume too much space. (Right)In this style, we draw a node in the graph for\neach entire vector representing a layer`s activations. This style is much more compact. Sometimes we annotate the edges in this graph with the name of the parameters that\ndescribe the relationship between two layers. Here, we indicate that a matrix W describes\nthe mapping from x to h, and a vector w describes the mapping from h to y. We\ntypically omit the intercept parameters associated with each layer when labeling this kind\nof drawing.\nmodel, we used a vector of weights and a scalar bias parameter to describe an\naffine transformation from an input vector to",
                    " an output scalar. Now, we describe\nan affine transformation from a vector x to a vector h, so an entire vector of bias\nparameters is needed. The activation function g is typically chosen to be a function\nthat is applied element-wise, with hi = g(x\nW:,i + ci). In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU (Jarrett\net al., 2009; Nair and Hinton, 2010; Glorot et al., 2011a) defined by the activation\nfunction g(z) = max{\\{}0, z{\\}} depicted in figure 6.3. We can now specify our complete network as\nf(x;W, c, w, b) = w max{\\{}0,W x + c{\\}} + b. (6.3)\nWe can now specify a solution to the XOR problem. Let\nW =\n\n1 1\n1 1\n\n, (6.4)\nc =\n\n0{-}1\n\n, (6.5)\n174\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n0\nz\n0\ng z( ) =\nm\na\nx 0{\\{} , z{\\}}\nFigure 6.3: The rectified linear activation function. This activation function is the default\nactivation function recommended for use with most feedforward neural networks. Applying\nthis function to the output of a linear transformation yields a nonlinear transformation. However, the function remains very close to linear, in the sense that is a piecewise linear\nfunction with two linear pieces. Because rectified linear units are nearly linear, they\npreserve many of the properties that make linear models easy to optimize with gradient- based methods. They also preserve many of the properties that make linear models\ngeneralize well. A common principle throughout computer science is that we can build\ncomplicated systems from minimal components. Much as a Turing machine`s memory\nneeds only to be able to store 0 or 1 states, we can build a universal function approximator\nfrom rectified linear functions.\n175\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nw =\n\n1{-}2\n\n, (6.6)\nand b = 0. We can now walk through the way that the model processes a batch of inputs. Let X be the design matrix containing all four points in the binary input space,",
                    "\nwith one example per row:\nX =\n\n\n\n0 0\n0 1\n1 0\n1 1\n\n\n\n. (6.7)\nThe first step in the neural network is to multiply the input matrix by the first\nlayer`s weight matrix:\nXW =\n\n\n\n\n0 0\n1 1\n1 1\n2 2\n\n\n\n\n. (6.8)\nNext, we add the bias vector c, to obtain\n\n\n\n\n0 {-}1\n1 0\n1 0\n2 1\n\n\n\n\n. (6.9)\nIn this space, all of the examples lie along a line with slope 1. As we move along\nthis line, the output needs to begin at 0, then rise to 1, then drop back down to 0. A linear model cannot implement such a function. To finish computing the value\nof h for each example, we apply the rectified linear transformation:\n\n\n\n0 0\n1 0\n1 0\n2 1\n\n\n\n. (6.10)\nThis transformation has changed the relationship between the examples. They no\nlonger lie on a single line. As shown in figure 6.1, they now lie in a space where a\nlinear model can solve the problem.\nWe finish by multiplying by the weight vector w:\n\n\n\n\n0\n1\n1\n0\n\n\n\n\n. (6.11)\n176\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nThe neural network has obtained the correct answer for every example in the batch.\nIn this example, we simply specified the solution, then showed that it obtained\nzero error. In a real situation, there might be billions of model parameters and\nbillions of training examples, so one cannot simply guess the solution as we did\nhere. Instead, a gradient-based optimization algorithm can find parameters that\nproduce very little error. The solution we described to the XOR problem is at a\nglobal minimum of the loss function, so gradient descent could converge to this\npoint. There are other equivalent solutions to the XOR problem that gradient\ndescent could also find. The convergence point of gradient descent depends on the\ninitial values of the parameters. In practice, gradient descent would usually not\nfind clean, easily understood, integer-valued solutions like the one we presented\nhere.\n6.2 Gradient-Based Learning\nDesigning and training a neural network is not much different from training any\nother machine learning model with gradient descent",
                    ". In section 5.10, we described\nhow to build a machine learning algorithm by specifying an optimization procedure,\na cost function, and a model family. The largest difference between the linear models we have seen so far and neural\nnetworks is that the nonlinearity of a neural network causes most interesting loss\nfunctions to become non-convex. This means that neural networks are usually\ntrained by using iterative, gradient-based optimizers that merely drive the cost\nfunction to a very low value, rather than the linear equation solvers used to train\nlinear regression models or the convex optimization algorithms with global conver- gence guarantees used to train logistic regression or SVMs. Convex optimization\nconverges starting from any initial parameters (in theory{\\textemdash}in practice it is very\nrobust but can encounter numerical problems). Stochastic gradient descent applied\nto non-convex loss functions has no such convergence guarantee, and is sensitive\nto the values of the initial parameters. For feedforward neural networks, it is\nimportant to initialize all weights to small random values. The biases may be\ninitialized to zero or to small positive values. The iterative gradient-based optimization algorithms used to train feedforward networks and almost all other deep\nmodels will be described in detail in chapter 8, with parameter initialization in\nparticular discussed in section 8.4. For the moment, it suffices to understand that\nthe training algorithm is almost always based on using the gradient to descend the\ncost function in one way or another. The specific algorithms are improvements\nand refinements on the ideas of gradient descent, introduced in section 4.3, and,\n177\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nmore specifically, are most often improvements of the stochastic gradient descent\nalgorithm, introduced in section 5.9. We can of course, train models such as linear regression and support vector\nmachines with gradient descent too, and in fact this is common when the training\nset is extremely large. From this point of view, training a neural network is not\nmuch different from training any other model. Computing the gradient is slightly\nmore complicated for a neural network, but can still be done efficiently and exactly. Section 6.5 will describe how to obtain the gradient using the back-propagation\nalgorithm and modern generalizations of the back-propagation algorithm.\nAs with other machine learning models, to apply gradient-based learning we\nmust choose",
                    " a cost function, and we must choose how to represent the output of\nthe model. We now revisit these design considerations with special emphasis on\nthe neural networks scenario.\n6.2.1 Cost Functions\nAn important aspect of the design of a deep neural network is the choice of the\ncost function. Fortunately, the cost functions for neural networks are more or less\nthe same as those for other parametric models, such as linear models.\nIn most cases, our parametric model defines a distribution p(y | x;{\\theta} ) and\nwe simply use the principle of maximum likelihood. This means we use the\ncross-entropy between the training data and the model`s predictions as the cost\nfunction.\nSometimes, we take a simpler approach, where rather than predicting a complete\nprobability distribution over y, we merely predict some statistic of y conditioned\non x. Specialized loss functions allow us to train a predictor of these estimates. The total cost function used to train a neural network will often combine one\nof the primary cost functions described here with a regularization term. We have\nalready seen some simple examples of regularization applied to linear models in\nsection 5.2.2. The weight decay approach used for linear models is also directly\napplicable to deep neural networks and is among the most popular regularization\nstrategies. More advanced regularization strategies for neural networks will be\ndescribed in chapter 7.\n6.2.1.1 Learning Conditional Distributions with Maximum Likelihood\nMost modern neural networks are trained using maximum likelihood. This means\nthat the cost function is simply the negative log-likelihood, equivalently described\n178\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nas the cross-entropy between the training data and the model distribution. This\ncost function is given by\nJ({\\theta}) = {-}Ex,y{\\sim}p{\\textasciicircum}data\nlog pmodel(y | x). (6.12)\nThe specific form of the cost function changes from model to model, depending\non the specific form of log pmodel. The expansion of the above equation typically\nyields some terms that do not depend on the model parameters and may be dis- carded. For example, as we saw in section 5.5.1, if pmodel(y | x) = N (y ; f(x; {\\theta}), I ), then we recover",
                    " the mean squared error cost,\nJ({\\theta}) =\n1\n2\nEx,y{\\sim}p{\\textasciicircum}data\n||y {-} f(x; {\\theta})||2 + const, (6.13)\nup to a scaling factor of 1\n2 and a term that does not depend on {\\theta}. The discarded\nconstant is based on the variance of the Gaussian distribution, which in this case\nwe chose not to parametrize. Previously, we saw that the equivalence between\nmaximum likelihood estimation with an output distribution and minimization of\nmean squared error holds for a linear model, but in fact, the equivalence holds\nregardless of the f(x; {\\theta}) used to predict the mean of the Gaussian.\nAn advantage of this approach of deriving the cost function from maximum\nlikelihood is that it removes the burden of designing cost functions for each model.\nSpecifying a model p(y | x) automatically determines a cost function log p(y | x). One recurring theme throughout neural network design is that the gradient of\nthe cost function must be large and predictable enough to serve as a good guide\nfor the learning algorithm. Functions that saturate (become very flat) undermine\nthis objective because they make the gradient become very small. In many cases\nthis happens because the activation functions used to produce the output of the\nhidden units or the output units saturate. The negative log-likelihood helps to\navoid this problem for many models. Many output units involve an exp function\nthat can saturate when its argument is very negative. The log function in the\nnegative log-likelihood cost function undoes the exp of some output units. We will\ndiscuss the interaction between the cost function and the choice of output unit in\nsection 6.2.2. One unusual property of the cross-entropy cost used to perform maximum\nlikelihood estimation is that it usually does not have a minimum value when applied\nto the models commonly used in practice. For discrete output variables, most\nmodels are parametrized in such a way that they cannot represent a probability\nof zero or one, but can come arbitrarily close to doing so. Logistic regression\nis an example of such a model. For real-valued output variables, if the model\n179\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ncan control the density of the output distribution (for example,",
                    " by learning the\nvariance parameter of a Gaussian output distribution) then it becomes possible\nto assign extremely high density to the correct training set outputs, resulting in\ncross-entropy approaching negative infinity. Regularization techniques described\nin chapter 7 provide several different ways of modifying the learning problem so\nthat the model cannot reap unlimited reward in this way.\n6.2.1.2 Learning Conditional Statistics\nInstead of learning a full probability distribution p(y | x; {\\theta}) we often want to learn\njust one conditional statistic of y given x. For example, we may have a predictor f(x; {\\theta}) that we wish to predict the mean\nof y.\nIf we use a sufficiently powerful neural network, we can think of the neural\nnetwork as being able to represent any function f from a wide class of functions, with this class being limited only by features such as continuity and boundedness\nrather than by having a specific parametric form. From this point of view, we\ncan view the cost function as being a functional rather than just a function. A\nfunctional is a mapping from functions to real numbers. We can thus think of\nlearning as choosing a function rather than merely choosing a set of parameters. We can design our cost functional to have its minimum occur at some specific\nfunction we desire. For example, we can design the cost functional to have its\nminimum lie on the function that maps x to the expected value of y given x. Solving an optimization problem with respect to a function requires a mathematical\ntool called calculus of variations, described in section 19.4.2. It is not necessary\nto understand calculus of variations to understand the content of this chapter. At\nthe moment, it is only necessary to understand that calculus of variations may be\nused to derive the following two results. Our first result derived using calculus of variations is that solving the optimization problem\nf\n{*} = arg min\nf\nEx,y{\\sim}pdata\n||y {-} f(x)||2\n(6.14)\nyields\nf\n{*} (x) = Ey{\\sim}pdata(y|x)\n[y], (6.15)\nso long as this function lies within the class we optimize over. In other words, if we\ncould train on infinitely many samples from the true data generating distribution,\nminimizing the mean squared error cost function gives a function that predicts the\nmean of y for each",
                    " value of x.\n180\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nDifferent cost functions give different statistics. A second result derived using\ncalculus of variations is that\nf\n{*} = arg min\nf\nEx,y{\\sim}pdata\n||y {-} f(x)||1 (6.16)\nyields a function that predicts the median value of y for each x, so long as such a\nfunction may be described by the family of functions we optimize over. This cost\nfunction is commonly called mean absolute error. Unfortunately, mean squared error and mean absolute error often lead to poor\nresults when used with gradient-based optimization. Some output units that\nsaturate produce very small gradients when combined with these cost functions. This is one reason that the cross-entropy cost function is more popular than mean\nsquared error or mean absolute error, even when it is not necessary to estimate an\nentire distribution p(y | x).\n6.2.2 Output Units\nThe choice of cost function is tightly coupled with the choice of output unit. Most\nof the time, we simply use the cross-entropy between the data distribution and the\nmodel distribution. The choice of how to represent the output then determines\nthe form of the cross-entropy function.\nAny kind of neural network unit that may be used as an output can also be\nused as a hidden unit. Here, we focus on the use of these units as outputs of the\nmodel, but in principle they can be used internally as well. We revisit these units\nwith additional detail about their use as hidden units in section 6.3. Throughout this section, we suppose that the feedforward network provides a\nset of hidden features defined by h = f (x;{\\theta}). The role of the output layer is then\nto provide some additional transformation from the features to complete the task\nthat the network must perform.\n6.2.2.1 Linear Units for Gaussian Output Distributions\nOne simple kind of output unit is an output unit based on an affine transformation\nwith no nonlinearity. These are often just called linear units. Given features h, a layer of linear output units produces a vector y{\\textasciicircum} = Wh+b. Linear output layers are often used to produce the mean of a conditional\nGaussian distribution:\np(y | x) = N (y; y{\\textasciic",
                    "ircum}, I). (6.17)\n181\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nMaximizing the log-likelihood is then equivalent to minimizing the mean squared\nerror.\nThe maximum likelihood framework makes it straightforward to learn the\ncovariance of the Gaussian too, or to make the covariance of the Gaussian be a\nfunction of the input. However, the covariance must be constrained to be a positive\ndefinite matrix for all inputs. It is difficult to satisfy such constraints with a linear\noutput layer, so typically other output units are used to parametrize the covariance.\nApproaches to modeling the covariance are described shortly, in section 6.2.2.4. Because linear units do not saturate, they pose little difficulty for gradientbased optimization algorithms and may be used with a wide variety of optimization\nalgorithms.\n6.2.2.2 Sigmoid Units for Bernoulli Output Distributions\nMany tasks require predicting the value of a binary variable y . Classification\nproblems with two classes can be cast in this form.\nThe maximum-likelihood approach is to define a Bernoulli distribution over y\nconditioned on x. A Bernoulli distribution is defined by just a single number. The neural net\nneeds to predict only P(y = 1 | x). For this number to be a valid probability, it\nmust lie in the interval [0, 1].\nSatisfying this constraint requires some careful design effort. Suppose we were\nto use a linear unit, and threshold its value to obtain a valid probability:\nP(y = 1 | x) = max \n0, min \n1, wh + b\n. (6.18)\nThis would indeed define a valid conditional distribution, but we would not be able\nto train it very effectively with gradient descent. Any time that wh +b strayed\noutside the unit interval, the gradient of the output of the model with respect to\nits parameters would be 0. A gradient of 0 is typically problematic because the\nlearning algorithm no longer has a guide for how to improve the corresponding\nparameters.\nInstead, it is better to use a different approach that ensures there is always a\nstrong gradient whenever the model has the wrong answer. This approach is based\non using sigmoid output units combined with maximum likelihood.\nA sigmoid output unit is defined by\ny{\\textasciicircum} =",
                    " {\\sigma}w\nh + b\n(6.19)\n182\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nwhere {\\sigma} is the logistic sigmoid function described in section 3.10. We can think of the sigmoid output unit as having two components. First, it\nuses a linear layer to compute z = wh + b. Next, it uses the sigmoid activation\nfunction to convert z into a probability. We omit the dependence on x for the moment to discuss how to define a\nprobability distribution over y using the value z. The sigmoid can be motivated\nby constructing an unnormalized probability distribution P{\\textasciitilde}(y), which does not\nsum to 1. We can then divide by an appropriate constant to obtain a valid\nprobability distribution. If we begin with the assumption that the unnormalized log\nprobabilities are linear in y and z, we can exponentiate to obtain the unnormalized\nprobabilities. We then normalize to see that this yields a Bernoulli distribution\ncontrolled by a sigmoidal transformation of z:\nlog P{\\textasciitilde}(y) = yz (6.20)\nP{\\textasciitilde}(y) = exp(yz) (6.21)\nP(y) =\nexp(yz)\n1\ny\n=0 exp(y\n z)\n(6.22)\nP(y) = {\\sigma} ((2y {-} 1)z). (6.23)\nProbability distributions based on exponentiation and normalization are common\nthroughout the statistical modeling literature. The z variable defining such a\ndistribution over binary variables is called a logit. This approach to predicting the probabilities in log-space is natural to use\nwith maximum likelihood learning. Because the cost function used with maximum\nlikelihood is {-} log P(y | x), the log in the cost function undoes the exp of the\nsigmoid. Without this effect, the saturation of the sigmoid could prevent gradient- based learning from making good progress. The loss function for maximum\nlikelihood learning of a Bernoulli parametrized by a sigmoid is\nJ({\\theta}) = {-} log P(y | x) (6.24) = {-} log {\\sigma} ((2y {-",
                    "} 1)z) (6.25) = {\\zeta} ((1 {-} 2y)z). (6.26)\nThis derivation makes use of some properties from section 3.10. By rewriting\nthe loss in terms of the softplus function, we can see that it saturates only when\n(1 {-} 2y)z is very negative. Saturation thus occurs only when the model already\nhas the right answer{\\textemdash}when y = 1 and z is very positive, or y = 0 and z is very\nnegative. When z has the wrong sign, the argument to the softplus function,\n183\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n(1{-}2y)z, may be simplified to |z|. As |z| becomes large while z has the wrong sign,\nthe softplus function asymptotes toward simply returning its argument |z|. The\nderivative with respect to z asymptotes to sign(z), so, in the limit of extremely\nincorrect z, the softplus function does not shrink the gradient at all. This property\nis very useful because it means that gradient-based learning can act to quickly\ncorrect a mistaken z. When we use other loss functions, such as mean squared error, the loss can\nsaturate anytime {\\sigma}(z) saturates. The sigmoid activation function saturates to 0\nwhen z becomes very negative and saturates to 1 when z becomes very positive. The gradient can shrink too small to be useful for learning whenever this happens, whether the model has the correct answer or the incorrect answer. For this reason,\nmaximum likelihood is almost always the preferred approach to training sigmoid\noutput units. Analytically, the logarithm of the sigmoid is always defined and finite, because\nthe sigmoid returns values restricted to the open interval (0, 1), rather than using\nthe entire closed interval of valid probabilities [0, 1]. In software implementations, to avoid numerical problems, it is best to write the negative log-likelihood as a\nfunction of z, rather than as a function of y{\\textasciicircum} = {\\sigma}(z). If the sigmoid function\nunderflows to zero, then taking the logarithm of y{\\textasciicircum} yields negative infinity.\n6.2.2.3 Softmax Units",
                    " for Multinoulli Output Distributions\nAny time we wish to represent a probability distribution over a discrete variable\nwith n possible values, we may use the softmax function. This can be seen as a\ngeneralization of the sigmoid function which was used to represent a probability\ndistribution over a binary variable.\nSoftmax functions are most often used as the output of a classifier, to represent\nthe probability distribution over n different classes. More rarely, softmax functions\ncan be used inside the model itself, if we wish the model to choose between one of\nn different options for some internal variable.\nIn the case of binary variables, we wished to produce a single number\ny{\\textasciicircum} = P(y = 1 | x). (6.27)\nBecause this number needed to lie between 0 and 1, and because we wanted the\nlogarithm of the number to be well-behaved for gradient-based optimization of\nthe log-likelihood, we chose to instead predict a number z = log P{\\textasciitilde}(y = 1 | x). Exponentiating and normalizing gave us a Bernoulli distribution controlled by the\nsigmoid function.\n184\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nTo generalize to the case of a discrete variable with n values, we now need\nto produce a vector y{\\textasciicircum}, with y{\\textasciicircum}i = P(y = i | x). We require not only that each\nelement of y{\\textasciicircum}i be between 0 and 1, but also that the entire vector sums to 1 so that\nit represents a valid probability distribution. The same approach that worked for\nthe Bernoulli distribution generalizes to the multinoulli distribution. First, a linear\nlayer predicts unnormalized log probabilities:\nz = W  h + b, (6.28)\nwhere zi = log P{\\textasciitilde}(y = i | x). The softmax function can then exponentiate and\nnormalize z to obtain the desired y{\\textasciicircum}. Formally, the softmax function is given by\nsoftmax(z)i =\nexp(zi)\n\nj exp(zj )\n. (6.29)\nAs with the logistic sigmoid, the use of the exp function works very well when",
                    "\ntraining the softmax to output a target value y using maximum log-likelihood. In\nthis case, we wish to maximize log P (y = i;z) = log softmax(z)i. Defining the\nsoftmax in terms of exp is natural because the log in the log-likelihood can undo\nthe exp of the softmax:\nlog softmax(z)i = zi {-} log\nj\nexp(zj ). (6.30)\nThe first term of equation 6.30 shows that the input zi always has a direct\ncontribution to the cost function. Because this term cannot saturate, we know\nthat learning can proceed, even if the contribution of zi to the second term of\nequation 6.30 becomes very small. When maximizing the log-likelihood, the first\nterm encourages zi to be pushed up, while the second term encourages all of z to be\npushed down. To gain some intuition for the second term, log\nj exp(zj ), observe\nthat this term can be roughly approximated by maxj zj\n. This approximation is\nbased on the idea that exp(zk) is insignificant for any zk that is noticeably less than\nmaxj zj. The intuition we can gain from this approximation is that the negative\nlog-likelihood cost function always strongly penalizes the most active incorrect\nprediction. If the correct answer already has the largest input to the softmax, then\nthe {-}zi term and the log\nj exp(zj) {\\approx} maxj zj = zi terms will roughly cancel.\nThis example will then contribute little to the overall training cost, which will be\ndominated by other examples that are not yet correctly classified.\nSo far we have discussed only a single example. Overall, unregularized maximum\nlikelihood will drive the model to learn parameters that drive the softmax to predict\n185\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nthe fraction of counts of each outcome observed in the training set:\nsoftmax(z(x; {\\theta}))i {\\approx}\nm\nj=1 1y\n(j)=i,x \n(j)=x m\nj=1 1x(j)=x\n. (6.31)\nBecause maximum likelihood is a consistent estimator, this is guaranteed to happen\nso long as the model family is capable of representing the",
                    " training distribution. In\npractice, limited model capacity and imperfect optimization will mean that the\nmodel is only able to approximate these fractions. Many objective functions other than the log-likelihood do not work as well\nwith the softmax function. Specifically, objective functions that do not use a log to\nundo the exp of the softmax fail to learn when the argument to the exp becomes\nvery negative, causing the gradient to vanish. In particular, squared error is a\npoor loss function for softmax units, and can fail to train the model to change its\noutput, even when the model makes highly confident incorrect predictions (Bridle, 1990). To understand why these other loss functions can fail, we need to examine\nthe softmax function itself.\nLike the sigmoid, the softmax activation can saturate. The sigmoid function has\na single output that saturates when its input is extremely negative or extremely\npositive. In the case of the softmax, there are multiple output values. These\noutput values can saturate when the differences between input values become\nextreme. When the softmax saturates, many cost functions based on the softmax\nalso saturate, unless they are able to invert the saturating activating function.\nTo see that the softmax function responds to the difference between its inputs, observe that the softmax output is invariant to adding the same scalar to all of its\ninputs:\nsoftmax(z) = softmax(z + c). (6.32)\nUsing this property, we can derive a numerically stable variant of the softmax:\nsoftmax(z) = softmax(z {-} max\ni\nzi). (6.33)\nThe reformulated version allows us to evaluate softmax with only small numerical\nerrors even when z contains extremely large or extremely negative numbers. Ex- amining the numerically stable variant, we see that the softmax function is driven\nby the amount that its arguments deviate from maxi zi. An output softmax(z)i saturates to 1 when the corresponding input is maximal\n(zi = maxi zi) and zi is much greater than all of the other inputs. The output\nsoftmax(z)i can also saturate to 0 when zi is not maximal and the maximum is\nmuch greater. This is a generalization of the way that sigmoid units saturate, and\n186\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n",
                    "can cause similar difficulties for learning if the loss function is not designed to\ncompensate for it.\nThe argument z to the softmax function can be produced in two different ways. The most common is simply to have an earlier layer of the neural network output\nevery element of z, as described above using the linear layer z = W h + b. While\nstraightforward, this approach actually overparametrizes the distribution. The\nconstraint that the n outputs must sum to 1 means that only n {-}1 parameters are\nnecessary; the probability of the n-th value may be obtained by subtracting the\nfirst n{-} 1 probabilities from 1. We can thus impose a requirement that one element\nof z be fixed. For example, we can require that zn = 0. Indeed, this is exactly\nwhat the sigmoid unit does. Defining P (y = 1 | x) = {\\sigma}(z) is equivalent to defining\nP (y = 1 | x) = softmax(z)1 with a two-dimensional z and z1 = 0. Both the n {-} 1\nargument and the n argument approaches to the softmax can describe the same\nset of probability distributions, but have different learning dynamics. In practice,\nthere is rarely much difference between using the overparametrized version or the\nrestricted version, and it is simpler to implement the overparametrized version.\nFrom a neuroscientific point of view, it is interesting to think of the softmax as\na way to create a form of competition between the units that participate in it: the\nsoftmax outputs always sum to 1 so an increase in the value of one unit necessarily\ncorresponds to a decrease in the value of others. This is analogous to the lateral\ninhibition that is believed to exist between nearby neurons in the cortex. At the\nextreme (when the difference between the maximal ai and the others is large in\nmagnitude) it becomes a form of winner-take-all (one of the outputs is nearly 1\nand the others are nearly 0).\nThe name {\\textquotedblleft}softmax{\\textquotedblright} can be somewhat confusing. The function is more closely\nrelated to the arg max function than the max function. The term {\\textquotedblleft}soft{\\textquotedblright} derives\nfrom the fact that the softmax function is continuous and differentiable. The\narg max",
                    " function, with its result represented as a one-hot vector, is not continuous\nor differentiable. The softmax function thus provides a {\\textquotedblleft}softened{\\textquotedblright} version of the\narg max. The corresponding soft version of the maximum function is softmax(z)z. It would perhaps be better to call the softmax function {\\textquotedblleft}softargmax,{\\textquotedblright} but the\ncurrent name is an entrenched convention.\n6.2.2.4 Other Output Types\nThe linear, sigmoid, and softmax output units described above are the most\ncommon. Neural networks can generalize to almost any kind of output layer that\nwe wish. The principle of maximum likelihood provides a guide for how to design\n187\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\na good cost function for nearly any kind of output layer.\nIn general, if we define a conditional distribution p(y | x; {\\theta}), the principle of\nmaximum likelihood suggests we use {-} log p(y | x; {\\theta}) as our cost function.\nIn general, we can think of the neural network as representing a function f(x;{\\theta}). The outputs of this function are not direct predictions of the value y. Instead,\nf (x;{\\theta}) = {\\omega} provides the parameters for a distribution over y. Our loss function\ncan then be interpreted as {-} log p(y; {\\omega}(x)). For example, we may wish to learn the variance of a conditional Gaussian for y, given x. In the simple case, where the variance {\\sigma}\n2\nis a constant, there is a closed\nform expression because the maximum likelihood estimator of variance is simply the\nempirical mean of the squared difference between observations y and their expected\nvalue. A computationally more expensive approach that does not require writing\nspecial-case code is to simply include the variance as one of the properties of the\ndistribution p(y | x) that is controlled by {\\omega} = f(x; {\\theta}). The negative log-likelihood {-} log p(y;{\\omega}(x)) will then provide a cost function with the appropriate terms\nnecessary to make our optimization procedure incrementally learn the variance. In\nthe simple case where the standard deviation does not depend on the input",
                    ", we\ncan make a new parameter in the network that is copied directly into {\\omega}. This new\nparameter might be {\\sigma} itself or could be a parameter v representing {\\sigma}\n2 or it could\nbe a parameter {\\beta} representing 1{\\sigma} 2, depending on how we choose to parametrize\nthe distribution. We may wish our model to predict a different amount of variance\nin y for different values of x. This is called a heteroscedastic model. In the\nheteroscedastic case, we simply make the specification of the variance be one of\nthe values output by f(x;{\\theta}). A typical way to do this is to formulate the Gaussian\ndistribution using precision, rather than variance, as described in equation 3.22. In the multivariate case it is most common to use a diagonal precision matrix\ndiag({\\beta}). (6.34)\nThis formulation works well with gradient descent because the formula for the\nlog-likelihood of the Gaussian distribution parametrized by {\\beta} involves only multiplication by {\\beta}i and addition of log {\\beta}i . The gradient of multiplication, addition,\nand logarithm operations is well-behaved. By comparison, if we parametrized the\noutput in terms of variance, we would need to use division. The division function\nbecomes arbitrarily steep near zero. While large gradients can help learning,\narbitrarily large gradients usually result in instability. If we parametrized the\noutput in terms of standard deviation, the log-likelihood would still involve division,\nand would also involve squaring. The gradient through the squaring operation\ncan vanish near zero, making it difficult to learn parameters that are squared.\n188\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nRegardless of whether we use standard deviation, variance, or precision, we must\nensure that the covariance matrix of the Gaussian is positive definite. Because\nthe eigenvalues of the precision matrix are the reciprocals of the eigenvalues of\nthe covariance matrix, this is equivalent to ensuring that the precision matrix is\npositive definite. If we use a diagonal matrix, or a scalar times the diagonal matrix,\nthen the only condition we need to enforce on the output of the model is positivity. If we suppose that a is the raw activation of the model used to determine the\ndiagonal precision,",
                    " we can use the softplus function to obtain a positive precision\nvector: {\\beta} = {\\zeta}(a). This same strategy applies equally if using variance or standard\ndeviation rather than precision or if using a scalar times identity rather than\ndiagonal matrix.\nIt is rare to learn a covariance or precision matrix with richer structure than\ndiagonal. If the covariance is full and conditional, then a parametrization must\nbe chosen that guarantees positive-definiteness of the predicted covariance matrix.\nThis can be achieved by writing {\\Sigma}(x) = B(x)B (x), where B is an unconstrained\nsquare matrix. One practical issue if the matrix is full rank is that computing the\nlikelihood is expensive, with a d {\\texttimes} d matrix requiring O(d\n3 ) computation for the\ndeterminant and inverse of {\\Sigma}(x) (or equivalently, and more commonly done, its\neigendecomposition or that of B(x)). We often want to perform multimodal regression, that is, to predict real values\nthat come from a conditional distribution p(y | x) that can have several different\npeaks in y space for the same value of x. In this case, a Gaussian mixture is\na natural representation for the output (Jacobs et al., 1991; Bishop, 1994). Neural networks with Gaussian mixtures as their output are often called mixture\ndensity networks. A Gaussian mixture output with n components is defined by\nthe conditional probability distribution\np(y | x) = n\ni=1\np(c = i | x)N (y; {\\textmu}\n(i)(x), {\\Sigma}\n(i) (x)). (6.35)\nThe neural network must have three outputs: a vector defining p(c = i | x), a\nmatrix providing {\\textmu}\n(i)(x) for all i, and a tensor providing {\\Sigma}(i)(x) for all i. These\noutputs must satisfy different constraints:\n1. Mixture components p(c = i | x): these form a multinoulli distribution\nover the n different components associated with latent variable1 c, and can\n1We consider c to be latent because we do not observe it in the data: given input x and target\ny, it is not possible to know with certainty which",
                    " Gaussian component was responsible for y, but\nwe can imagine that y was generated by picking one of them, and make that unobserved choice a\nrandom variable.\n189\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ntypically be obtained by a softmax over an n-dimensional vector, to guarantee\nthat these outputs are positive and sum to 1.\n2. Means {\\textmu}(i)(x): these indicate the center or mean associated with the i-th\nGaussian component, and are unconstrained (typically with no nonlinearity\nat all for these output units). If y is a d-vector, then the network must output\nan n {\\texttimes} d matrix containing all n of these d-dimensional vectors. Learning\nthese means with maximum likelihood is slightly more complicated than\nlearning the means of a distribution with only one output mode. We only\nwant to update the mean for the component that actually produced the\nobservation. In practice, we do not know which component produced each\nobservation. The expression for the negative log-likelihood naturally weights\neach example`s contribution to the loss for each component by the probability\nthat the component produced the example.\n3. Covariances {\\Sigma}(i)(x): these specify the covariance matrix for each component\ni. As when learning a single Gaussian component, we typically use a diagonal\nmatrix to avoid needing to compute determinants. As with learning the means\nof the mixture, maximum likelihood is complicated by needing to assign\npartial responsibility for each point to each mixture component. Gradient\ndescent will automatically follow the correct process if given the correct\nspecification of the negative log-likelihood under the mixture model.\nIt has been reported that gradient-based optimization of conditional Gaussian\nmixtures (on the output of neural networks) can be unreliable, in part because one\ngets divisions (by the variance) which can be numerically unstable (when some\nvariance gets to be small for a particular example, yielding very large gradients). One solution is to clip gradients (see section 10.11.1) while another is to scale\nthe gradients heuristically (Murray and Larochelle, 2014). Gaussian mixture outputs are particularly effective in generative models of\nspeech (Schuster, 1999) or movements of physical objects (Graves, 2013). The\nmixture density strategy gives a way for the network to represent multiple output\nmodes and to",
                    " control the variance of its output, which is crucial for obtaining\na high degree of quality in these real-valued domains. An example of a mixture\ndensity network is shown in figure 6.4.\nIn general, we may wish to continue to model larger vectors y containing more\nvariables, and to impose richer and richer structures on these output variables. For\nexample, we may wish for our neural network to output a sequence of characters\nthat forms a sentence. In these cases, we may continue to use the principle\nof maximum likelihood applied to our model p(y; {\\omega}(x)), but the model we use\n190\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nx\ny\nFigure 6.4: Samples drawn from a neural network with a mixture density output layer. The input x is sampled from a uniform distribution and the output y is sampled from\npmodel(y | x). The neural network is able to learn nonlinear mappings from the input to\nthe parameters of the output distribution. These parameters include the probabilities\ngoverning which of three mixture components will generate the output as well as the\nparameters for each mixture component. Each mixture component is Gaussian with\npredicted mean and variance. All of these aspects of the output distribution are able to\nvary with respect to the input x, and to do so in nonlinear ways.\nto describe y becomes complex enough to be beyond the scope of this chapter.\nChapter 10 describes how to use recurrent neural networks to define such models\nover sequences, and part III describes advanced techniques for modeling arbitrary\nprobability distributions.\n6.3 Hidden Units\nSo far we have focused our discussion on design choices for neural networks that\nare common to most parametric machine learning models trained with gradient- based optimization. Now we turn to an issue that is unique to feedforward neural\nnetworks: how to choose the type of hidden unit to use in the hidden layers of the\nmodel.\nThe design of hidden units is an extremely active area of research and does not\nyet have many definitive guiding theoretical principles. Rectified linear units are an excellent default choice of hidden unit. Many other\ntypes of hidden units are available. It can be difficult to determine when to use\nwhich kind (though rectified linear units are usually an acceptable choice). We\n191\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ndescribe here some of the basic intuitions motivating each type of hidden units. These intuitions can",
                    " help decide when to try out each of these units. It is usually\nimpossible to predict in advance which will work best. The design process consists\nof trial and error, intuiting that a kind of hidden unit may work well, and then\ntraining a network with that kind of hidden unit and evaluating its performance\non a validation set. Some of the hidden units included in this list are not actually differentiable at\nall input points. For example, the rectified linear function g (z) = max{\\{}0, z{\\}} is not\ndifferentiable at z = 0. This may seem like it invalidates g for use with a gradient- based learning algorithm. In practice, gradient descent still performs well enough\nfor these models to be used for machine learning tasks. This is in part because\nneural network training algorithms do not usually arrive at a local minimum of\nthe cost function, but instead merely reduce its value significantly, as shown in\nfigure 4.3. These ideas will be described further in chapter 8. Because we do not\nexpect training to actually reach a point where the gradient is 0, it is acceptable\nfor the minima of the cost function to correspond to points with undefined gradient. Hidden units that are not differentiable are usually non-differentiable at only a\nsmall number of points. In general, a function g(z) has a left derivative defined\nby the slope of the function immediately to the left of z and a right derivative\ndefined by the slope of the function immediately to the right of z. A function\nis differentiable at z only if both the left derivative and the right derivative are\ndefined and equal to each other. The functions used in the context of neural\nnetworks usually have defined left derivatives and defined right derivatives. In the\ncase of g(z) = max{\\{}0, z{\\}}, the left derivative at z = 0 is 0 and the right derivative\nis 1. Software implementations of neural network training usually return one of\nthe one-sided derivatives rather than reporting that the derivative is undefined or\nraising an error. This may be heuristically justified by observing that gradientbased optimization on a digital computer is subject to numerical error anyway. When a function is asked to evaluate g(0), it is very unlikely that the underlying\nvalue truly was 0. Instead, it was likely to be some small value  that was rounded\nto 0. In some contexts, more theoretically pleasing justifications are available, but\nthese usually do not apply to",
                    " neural network training. The important point is that\nin practice one can safely disregard the non-differentiability of the hidden unit\nactivation functions described below.\nUnless indicated otherwise, most hidden units can be described as accepting\na vector of inputs x, computing an affine transformation z = W x + b, and\nthen applying an element-wise nonlinear function g(z). Most hidden units are\ndistinguished from each other only by the choice of the form of the activation\nfunction g(z).\n192\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.3.1 Rectified Linear Units and Their Generalizations\nRectified linear units use the activation function g(z) = max{\\{}0, z{\\}}. Rectified linear units are easy to optimize because they are so similar to linear\nunits. The only difference between a linear unit and a rectified linear unit is\nthat a rectified linear unit outputs zero across half its domain. This makes the\nderivatives through a rectified linear unit remain large whenever the unit is active.\nThe gradients are not only large but also consistent. The second derivative of the\nrectifying operation is 0 almost everywhere, and the derivative of the rectifying\noperation is 1 everywhere that the unit is active. This means that the gradient\ndirection is far more useful for learning than it would be with activation functions\nthat introduce second-order effects. Rectified linear units are typically used on top of an affine transformation:\nh = g(Wx + b). (6.36)\nWhen initializing the parameters of the affine transformation, it can be a good\npractice to set all elements of b to a small, positive value, such as 0.1. This makes\nit very likely that the rectified linear units will be initially active for most inputs\nin the training set and allow the derivatives to pass through.\nSeveral generalizations of rectified linear units exist. Most of these generalizations perform comparably to rectified linear units and occasionally perform\nbetter. One drawback to rectified linear units is that they cannot learn via gradient- based methods on examples for which their activation is zero. A variety of\ngeneralizations of rectified linear units guarantee that they receive gradient every- where.\nThree generalizations of rectified linear units are based on using a non-zero\nslope {\\alpha}i when zi {<} 0: hi = g(z, {\\alpha})i = max(0, zi)",
                    " + {\\alpha}i min(0, zi). Absolute value\nrectification fixes {\\alpha}i = {-}1 to obtain g( z) = |z|. It is used for object recognition\nfrom images (Jarrett et al., 2009), where it makes sense to seek features that are\ninvariant under a polarity reversal of the input illumination. Other generalizations\nof rectified linear units are more broadly applicable. A leaky ReLU (Maas et al., 2013) fixes {\\alpha}i to a small value like 0.01 while a parametric ReLU or PReLU\ntreats {\\alpha}i as a learnable parameter (He et al., 2015). Maxout units (Goodfellow et al., 2013a) generalize rectified linear units\nfurther. Instead of applying an element-wise function g(z ), maxout units divide z\ninto groups of k values. Each maxout unit then outputs the maximum element of\n193\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\none of these groups:\ng(z)i = max\nj{\\in}G(i)\nzj (6.37)\nwhere G(i)\nis the set of indices into the inputs for group i, {\\{}(i {-} 1)k + 1, . . . ,ik{\\}}. This provides a way of learning a piecewise linear function that responds to multiple\ndirections in the input x space.\nA maxout unit can learn a piecewise linear, convex function with up to k pieces. Maxout units can thus be seen as learning the activation function itself rather\nthan just the relationship between units. With large enough k, a maxout unit can\nlearn to approximate any convex function with arbitrary fidelity. In particular,\na maxout layer with two pieces can learn to implement the same function of the\ninput x as a traditional layer using the rectified linear activation function, absolute\nvalue rectification function, or the leaky or parametric ReLU, or can learn to\nimplement a totally different function altogether. The maxout layer will of course\nbe parametrized differently from any of these other layer types, so the learning\ndynamics will be different even in the cases where maxout learns to implement the\nsame function of x as one of the other layer types. Each maxout unit is now parametrized by k weight vectors instead of just one,\n",
                    "so maxout units typically need more regularization than rectified linear units. They\ncan work well without regularization if the training set is large and the number of\npieces per unit is kept low (Cai et al., 2013). Maxout units have a few other benefits. In some cases, one can gain some statistical and computational advantages by requiring fewer parameters. Specifically,\nif the features captured by n different linear filters can be summarized without\nlosing information by taking the max over each group of k features, then the next\nlayer can get by with k times fewer weights. Because each unit is driven by multiple filters, maxout units have some redundancy that helps them to resist a phenomenon called catastrophic forgetting\nin which neural networks forget how to perform tasks that they were trained on in\nthe past (Goodfellow et al., 2014a). Rectified linear units and all of these generalizations of them are based on the\nprinciple that models are easier to optimize if their behavior is closer to linear.\nThis same general principle of using linear behavior to obtain easier optimization\nalso applies in other contexts besides deep linear networks. Recurrent networks can\nlearn from sequences and produce a sequence of states and outputs. When training\nthem, one needs to propagate information through several time steps, which is much\neasier when some linear computations (with some directional derivatives being of\nmagnitude near 1) are involved. One of the best-performing recurrent network\n194\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\narchitectures, the LSTM, propagates information through time via summation{\\textemdash}a\nparticular straightforward kind of such linear activation. This is discussed further\nin section 10.10.\n6.3.2 Logistic Sigmoid and Hyperbolic Tangent\nPrior to the introduction of rectified linear units, most neural networks used the\nlogistic sigmoid activation function\ng(z) = {\\sigma}(z) (6.38)\nor the hyperbolic tangent activation function\ng(z) = tanh(z). (6.39)\nThese activation functions are closely related because tanh(z) = 2{\\sigma}(2z) {-} 1. We have already seen sigmoid units as output units, used to predict the\nprobability that a binary variable is 1. Unlike piecewise linear units, sigmoidal\nunits saturate across most of their domain{\\textemdash}",
                    "they saturate to a high value when\nz is very positive, saturate to a low value when z is very negative, and are only\nstrongly sensitive to their input when z is near 0. The widespread saturation of\nsigmoidal units can make gradient-based learning very difficult. For this reason,\ntheir use as hidden units in feedforward networks is now discouraged. Their use\nas output units is compatible with the use of gradient-based learning when an\nappropriate cost function can undo the saturation of the sigmoid in the output\nlayer. When a sigmoidal activation function must be used, the hyperbolic tangent\nactivation function typically performs better than the logistic sigmoid. It resembles\nthe identity function more closely, in the sense that tanh(0) = 0 while {\\sigma}(0) = 1\n2\n. Because tanh is similar to the identity function near 0, training a deep neural\nnetwork y{\\textasciicircum} = w tanh(U  tanh(V\nx)) resembles training a linear model y{\\textasciicircum} = w UV\nx so long as the activations of the network can be kept small. This\nmakes training the tanh network easier.\nSigmoidal activation functions are more common in settings other than feedforward networks. Recurrent networks, many probabilistic models, and some\nautoencoders have additional requirements that rule out the use of piecewise\nlinear activation functions and make sigmoidal units more appealing despite the\ndrawbacks of saturation.\n195\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.3.3 Other Hidden Units\nMany other types of hidden units are possible, but are used less frequently.\nIn general, a wide variety of differentiable functions perform perfectly well.\nMany unpublished activation functions perform just as well as the popular ones. To provide a concrete example, the authors tested a feedforward network using\nh = cos(Wx + b) on the MNIST dataset and obtained an error rate of less than\n1{\\%}, which is competitive with results obtained using more conventional activation\nfunctions. During research and development of new techniques, it is common\nto test many different activation functions and find that several variations on\nstandard practice perform comparably. This means that usually new hidden unit\ntypes are published only if they are clearly demonstrated to provide a significant\nimprovement. New hidden unit types that perform roughly comparably to known\n",
                    "types are so common as to be uninteresting.\nIt would be impractical to list all of the hidden unit types that have appeared\nin the literature. We highlight a few especially useful and distinctive ones. One possibility is to not have an activation g (z) at all. One can also think of\nthis as using the identity function as the activation function. We have already\nseen that a linear unit can be useful as the output of a neural network. It may\nalso be used as a hidden unit. If every layer of the neural network consists of only\nlinear transformations, then the network as a whole will be linear. However, it\nis acceptable for some layers of the neural network to be purely linear. Consider\na neural network layer with n inputs and p outputs, h = g(Wx + b). We may\nreplace this with two layers, with one layer using weight matrix U and the other\nusing weight matrix V . If the first layer has no activation function, then we have\nessentially factored the weight matrix of the original layer based on W . The\nfactored approach is to compute h = g(V\nU\n x +b). If U produces q outputs, then U and V together contain only (n + p)q parameters, while W contains np\nparameters. For small q, this can be a considerable saving in parameters. It\ncomes at the cost of constraining the linear transformation to be low-rank, but\nthese low-rank relationships are often sufficient. Linear hidden units thus offer an\neffective way of reducing the number of parameters in a network.\nSoftmax units are another kind of unit that is usually used as an output (as\ndescribed in section 6.2.2.3) but may sometimes be used as a hidden unit. Softmax\nunits naturally represent a probability distribution over a discrete variable with k\npossible values, so they may be used as a kind of switch. These kinds of hidden\nunits are usually only used in more advanced architectures that explicitly learn to\nmanipulate memory, described in section 10.12.\n196\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nA few other reasonably common hidden unit types include:\n Radial basis function or RBF unit: hi = exp {-}\n1{\\sigma}\n2\ni\n||W:,i {-} x||2\n. This\nfunction becomes more active as x approaches a template W:,i. Because it\nsaturates to 0 for",
                    " most x, it can be difficult to optimize.\n Softplus: g(a) = {\\zeta}(a) = log(1+e\na\n). This is a smooth version of the rectifier,\nintroduced by Dugas et al. (2001) for function approximation and by Nair\nand Hinton (2010) for the conditional distributions of undirected probabilistic\nmodels. Glorot et al. (2011a) compared the softplus and rectifier and found\nbetter results with the latter. The use of the softplus is generally discouraged.\nThe softplus demonstrates that the performance of hidden unit types can\nbe very counterintuitive{\\textemdash}one might expect it to have an advantage over\nthe rectifier due to being differentiable everywhere or due to saturating less\ncompletely, but empirically it does not.\n Hard tanh: this is shaped similarly to the tanh and the rectifier but unlike\nthe latter, it is bounded, g(a) = max({-}1, min(1 , a)). It was introduced\nby Collobert (2004). Hidden unit design remains an active area of research and many useful hidden\nunit types remain to be discovered.\n6.4 Architecture Design\nAnother key design consideration for neural networks is determining the architecture.\nThe word architecture refers to the overall structure of the network: how many\nunits it should have and how these units should be connected to each other.\nMost neural networks are organized into groups of units called layers. Most\nneural network architectures arrange these layers in a chain structure, with each\nlayer being a function of the layer that preceded it. In this structure, the first layer\nis given by\nh\n(1) = g\n(1) W(1)x + b\n(1)\n, (6.40)\nthe second layer is given by\nh\n(2) = g\n(2) W(2) h\n(1) + b\n(2)\n, (6.41)\nand so on.\n197\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nIn these chain-based architectures, the main architectural considerations are\nto choose the depth of the network and the width of each layer. As we will see, a network with even one hidden layer is sufficient to fit the training set. Deeper\nnetworks often are able to use far fewer units per layer and far fewer parameters\nand often generalize to",
                    " the test set, but are also often harder to optimize. The\nideal network architecture for a task must be found via experimentation guided by\nmonitoring the validation set error.\n6.4.1 Universal Approximation Properties and Depth\nA linear model, mapping from features to outputs via matrix multiplication, can\nby definition represent only linear functions. It has the advantage of being easy to\ntrain because many loss functions result in convex optimization problems when\napplied to linear models. Unfortunately, we often want to learn nonlinear functions. At first glance, we might presume that learning a nonlinear function requires\ndesigning a specialized model family for the kind of nonlinearity we want to learn.\nFortunately, feedforward networks with hidden layers provide a universal approximation framework. Specifically, the universal approximation theorem (Hornik\net al., 1989; Cybenko, 1989) states that a feedforward network with a linear output\nlayer and at least one hidden layer with any {\\textquotedblleft}squashing{\\textquotedblright} activation function (such\nas the logistic sigmoid activation function) can approximate any Borel measurable\nfunction from one finite-dimensional space to another with any desired non-zero\namount of error, provided that the network is given enough hidden units. The\nderivatives of the feedforward network can also approximate the derivatives of the\nfunction arbitrarily well (Hornik et al., 1990). The concept of Borel measurability\nis beyond the scope of this book; for our purposes it suffices to say that any\ncontinuous function on a closed and bounded subset of Rn is Borel measurable\nand therefore may be approximated by a neural network. A neural network may\nalso approximate any function mapping from any finite dimensional discrete space\nto another. While the original theorems were first stated in terms of units with\nactivation functions that saturate both for very negative and for very positive\narguments, universal approximation theorems have also been proved for a wider\nclass of activation functions, which includes the now commonly used rectified linear\nunit (Leshno et al., 1993). The universal approximation theorem means that regardless of what function\nwe are trying to learn, we know that a large MLP will be able to represent this\nfunction. However, we are not guaranteed that the training algorithm will be able\nto learn that function. Even if the MLP is able to represent the function, learning\ncan fail for two different reasons. First, the",
                    " optimization algorithm used for training\n198\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nmay not be able to find the value of the parameters that corresponds to the desired\nfunction. Second, the training algorithm might choose the wrong function due to\noverfitting. Recall from section 5.2.1 that the {\\textquotedblleft}no free lunch{\\textquotedblright} theorem shows that\nthere is no universally superior machine learning algorithm. Feedforward networks\nprovide a universal system for representing functions, in the sense that, given a\nfunction, there exists a feedforward network that approximates the function. There\nis no universal procedure for examining a training set of specific examples and\nchoosing a function that will generalize to points not in the training set. The universal approximation theorem says that there exists a network large\nenough to achieve any degree of accuracy we desire, but the theorem does not\nsay how large this network will be. Barron (1993) provides some bounds on the\nsize of a single-layer network needed to approximate a broad class of functions. Unfortunately, in the worse case, an exponential number of hidden units (possibly\nwith one hidden unit corresponding to each input configuration that needs to be\ndistinguished) may be required. This is easiest to see in the binary case: the\nnumber of possible binary functions on vectors v {\\in} {\\{}0, 1{\\}}\nn\nis 2\n2 n and selecting\none such function requires 2\nn bits, which will in general require O(2\nn\n) degrees of\nfreedom.\nIn summary, a feedforward network with a single layer is sufficient to represent\nany function, but the layer may be infeasibly large and may fail to learn and\ngeneralize correctly. In many circumstances, using deeper models can reduce the\nnumber of units required to represent the desired function and can reduce the\namount of generalization error.\nThere exist families of functions which can be approximated efficiently by an\narchitecture with depth greater than some value d, but which require a much larger\nmodel if depth is restricted to be less than or equal to d. In many cases, the number\nof hidden units required by the shallow model is exponential in n. Such results\nwere first proved for models that do not resemble the continuous, differentiable\nneural networks used for machine learning, but have since been extended to these\nmodels. The first results were for circuits of logic gates (H{\\r{a}}stad,",
                    " 1986). Later\nwork extended these results to linear threshold units with non-negative weights\n(H{\\r{a}}stad and Goldmann, 1991; Hajnal et al., 1993), and then to networks with\ncontinuous-valued activations (Maass, 1992; Maass et al., 1994). Many modern\nneural networks use rectified linear units. Leshno et al. (1993) demonstrated\nthat shallow networks with a broad family of non-polynomial activation functions,\nincluding rectified linear units, have universal approximation properties, but these\nresults do not address the questions of depth or efficiency{\\textemdash}they specify only that\na sufficiently wide rectifier network could represent any function. Montufar et al.\n199\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n(2014) showed that functions representable with a deep rectifier net can require\nan exponential number of hidden units with a shallow (one hidden layer) network.\nMore precisely, they showed that piecewise linear networks (which can be obtained\nfrom rectifier nonlinearities or maxout units) can represent functions with a number\nof regions that is exponential in the depth of the network. Figure 6.5 illustrates how\na network with absolute value rectification creates mirror images of the function\ncomputed on top of some hidden unit, with respect to the input of that hidden\nunit. Each hidden unit specifies where to fold the input space in order to create\nmirror responses (on both sides of the absolute value nonlinearity). By composing\nthese folding operations, we obtain an exponentially large number of piecewise\nlinear regions which can capture all kinds of regular (e.g., repeating) patterns.\nFigure 6.5: An intuitive, geometric explanation of the exponential advantage of deeper\nrectifier networks formally by Montufar et al. (2014). (Left)An absolute value rectification\nunit has the same output for every pair of mirror points in its input. The mirror axis\nof symmetry is given by the hyperplane defined by the weights and bias of the unit. A\nfunction computed on top of that unit (the green decision surface) will be a mirror image\nof a simpler pattern across that axis of symmetry. (Center)The function can be obtained\nby folding the space around the axis of symmetry. (Right)Another repeating pattern can\nbe folded on top of the first (by another downstream unit) to obtain another symmetry\n(which is now repeated four times, with two",
                    " hidden layers). Figure reproduced with\npermission from Montufar et al. (2014). More precisely, the main theorem in Montufar et al. (2014) states that the\nnumber of linear regions carved out by a deep rectifier network with d inputs, depth l, and n units per hidden layer, is\nO \nn\nd\nd(l{-}1)\nn\nd\n, (6.42)\ni.e., exponential in the depth l. In the case of maxout networks with k filters per\nunit, the number of linear regions is\nO\n\nk\n(l{-}1)+d\n. (6.43)\n200\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nOf course, there is no guarantee that the kinds of functions we want to learn in\napplications of machine learning (and in particular for AI) share such a property. We may also want to choose a deep model for statistical reasons. Any time\nwe choose a specific machine learning algorithm, we are implicitly stating some\nset of prior beliefs we have about what kind of function the algorithm should\nlearn. Choosing a deep model encodes a very general belief that the function we\nwant to learn should involve composition of several simpler functions. This can be\ninterpreted from a representation learning point of view as saying that we believe\nthe learning problem consists of discovering a set of underlying factors of variation\nthat can in turn be described in terms of other, simpler underlying factors of\nvariation. Alternately, we can interpret the use of a deep architecture as expressing\na belief that the function we want to learn is a computer program consisting of\nmultiple steps, where each step makes use of the previous step`s output. These\nintermediate outputs are not necessarily factors of variation, but can instead be\nanalogous to counters or pointers that the network uses to organize its internal\nprocessing. Empirically, greater depth does seem to result in better generalization\nfor a wide variety of tasks (Bengio et al., 2007; Erhan et al., 2009; Bengio, 2009;\nMesnil et al., 2011; Ciresan et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Farabet et al., 2013; Couprie et al., 2013; Kahou et al., 2013; Goodfellow\net al., 2014d; Szegedy et al., 2014a",
                    "). See figure 6.6 and figure 6.7 for examples of\nsome of these empirical results. This suggests that using deep architectures does\nindeed express a useful prior over the space of functions the model learns.\n6.4.2 Other Architectural Considerations\nSo far we have described neural networks as being simple chains of layers, with the\nmain considerations being the depth of the network and the width of each layer. In practice, neural networks show considerably more diversity. Many neural network architectures have been developed for specific tasks. Specialized architectures for computer vision called convolutional networks are\ndescribed in chapter 9. Feedforward networks may also be generalized to the\nrecurrent neural networks for sequence processing, described in chapter 10, which\nhave their own architectural considerations.\nIn general, the layers need not be connected in a chain, even though this is the\nmost common practice. Many architectures build a main chain but then add extra\narchitectural features to it, such as skip connections going from layer i to layer\ni + 2 or higher. These skip connections make it easier for the gradient to flow from\noutput layers to layers nearer the input.\n201\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n3 4 5 6 7 8 9 10 11\nNumber of hidden layers\n92.0\n92.5\n93.0\n93.5\n94.0\n94.5\n95.0\n95.5\n96.0\n96.5\nTest a\nc\nc\nura\nc\ny (p\nerc\ne\nnt) Figure 6.6: Empirical results showing that deeper networks generalize better when used\nto transcribe multi-digit numbers from photographs of addresses. Data from Goodfellow\net al. (2014d). The test set accuracy consistently increases with increasing depth. See\nfigure 6.7 for a control experiment demonstrating that other increases to the model size\ndo not yield the same effect.\nAnother key consideration of architecture design is exactly how to connect a\npair of layers to each other. In the default neural network layer described by a linear\ntransformation via a matrix W , every input unit is connected to every output\nunit. Many specialized networks in the chapters ahead have fewer connections, so\nthat each unit in the input layer is connected to only a small subset of units in\nthe output layer. These strategies for reducing the number of connections reduce\nthe number of parameters and the amount of computation required to evaluate\nthe network, but",
                    " are often highly problem-dependent. For example, convolutional\nnetworks, described in chapter 9, use specialized patterns of sparse connections\nthat are very effective for computer vision problems. In this chapter, it is difficult\nto give much more specific advice concerning the architecture of a generic neural\nnetwork. Subsequent chapters develop the particular architectural strategies that\nhave been found to work well for different application domains.\n202\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n0.0 0.2 0.4 0.6 0.8 1.0\nNumber of parameters {\\texttimes}10\n8\n91\n92\n93\n94\n95\n96\n97\nTest a\nc\nc\nura\nc\ny (p\nerc\ne\nnt)\n3, convolutional\n3, fully connected\n11, convolutional\nFigure 6.7: Deeper models tend to perform better. This is not merely because the model is\nlarger. This experiment from Goodfellow et al. (2014d) shows that increasing the number\nof parameters in layers of convolutional networks without increasing their depth is not\nnearly as effective at increasing test set performance. The legend indicates the depth of\nnetwork used to make each curve and whether the curve represents variation in the size of\nthe convolutional or the fully connected layers. We observe that shallow models in this\ncontext overfit at around 20 million parameters while deep ones can benefit from having\nover 60 million. This suggests that using a deep model expresses a useful preference over\nthe space of functions the model can learn. Specifically, it expresses a belief that the\nfunction should consist of many simpler functions composed together. This could result\neither in learning a representation that is composed in turn of simpler representations (e.g.,\ncorners defined in terms of edges) or in learning a program with sequentially dependent\nsteps (e.g., first locate a set of objects, then segment them from each other, then recognize\nthem).\n203\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.5 Back-Propagation and Other Differentiation Algorithms\nWhen we use a feedforward neural network to accept an input x and produce an\noutput y{\\textasciicircum}, information flows forward through the network. The inputs x provide\nthe initial information that then propagates up to the hidden units at each layer\nand finally produces y{\\textasciicircum}. This is called",
                    " forward propagation. During training,\nforward propagation can continue onward until it produces a scalar cost J({\\theta}). The back-propagation algorithm (Rumelhart et al., 1986a), often simply called\nbackprop, allows the information from the cost to then flow backwards through\nthe network, in order to compute the gradient. Computing an analytical expression for the gradient is straightforward, but\nnumerically evaluating such an expression can be computationally expensive. The\nback-propagation algorithm does so using a simple and inexpensive procedure.\nThe term back-propagation is often misunderstood as meaning the whole\nlearning algorithm for multi-layer neural networks. Actually, back-propagation\nrefers only to the method for computing the gradient, while another algorithm,\nsuch as stochastic gradient descent, is used to perform learning using this gradient.\nFurthermore, back-propagation is often misunderstood as being specific to multilayer neural networks, but in principle it can compute derivatives of any function\n(for some functions, the correct response is to report that the derivative of the\nfunction is undefined). Specifically, we will describe how to compute the gradient\n{\\nabla}xf(x, y) for an arbitrary function f , wherex is a set of variables whose derivatives\nare desired, and y is an additional set of variables that are inputs to the function\nbut whose derivatives are not required. In learning algorithms, the gradient we most\noften require is the gradient of the cost function with respect to the parameters, {\\nabla}{\\theta}J({\\theta}). Many machine learning tasks involve computing other derivatives, either\nas part of the learning process, or to analyze the learned model. The back- propagation algorithm can be applied to these tasks as well, and is not restricted\nto computing the gradient of the cost function with respect to the parameters. The\nidea of computing derivatives by propagating information through a network is\nvery general, and can be used to compute values such as the Jacobian of a function\nf with multiple outputs. We restrict our description here to the most commonly\nused case where f has a single output.\n204\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.5.1 Computational Graphs\nSo far we have discussed neural networks with a relatively informal graph language.\nTo describe the back-propagation algorithm more precisely, it is helpful to have a\nmore precise computational graph language.\nMany ways",
                    " of formalizing computation as graphs are possible.\nHere, we use each node in the graph to indicate a variable. The variable may\nbe a scalar, vector, matrix, tensor, or even a variable of another type.\nTo formalize our graphs, we also need to introduce the idea of an operation. An operation is a simple function of one or more variables. Our graph language\nis accompanied by a set of allowable operations. Functions more complicated\nthan the operations in this set may be described by composing many operations\ntogether.\nWithout loss of generality, we define an operation to return only a single\noutput variable. This does not lose generality because the output variable can have\nmultiple entries, such as a vector. Software implementations of back-propagation\nusually support operations with multiple outputs, but we avoid this case in our\ndescription because it introduces many extra details that are not important to\nconceptual understanding.\nIf a variable y is computed by applying an operation to a variable x, then\nwe draw a directed edge from x to y. We sometimes annotate the output node\nwith the name of the operation applied, and other times omit this label when the\noperation is clear from context. Examples of computational graphs are shown in figure 6.8.\n6.5.2 Chain Rule of Calculus\nThe chain rule of calculus (not to be confused with the chain rule of probability) is\nused to compute the derivatives of functions formed by composing other functions\nwhose derivatives are known. Back-propagation is an algorithm that computes the\nchain rule, with a specific order of operations that is highly efficient. Let x be a real number, and let f and g both be functions mapping from a real\nnumber to a real number. Suppose that y = g(x) and z = f(g(x)) = f (y). Then\nthe chain rule states that\ndz\ndx =\ndz\ndy\ndy\ndx\n. (6.44)\nWe can generalize this beyond the scalar case. Suppose that x {\\in} Rm, y {\\in} Rn\n,\n205\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx y\n(a)\n{\\texttimes}\nx w\n(b)\nu\n(1)\ndot\nb\nu\n(2)\n+\ny{\\textasciicircum} {\\sigma}\n(c)\nX W\n",
                    "U\n(1)\nmatmul\nb\nU\n(2)\n+\nH\nrelu\nx w\n(d)\ny{\\textasciicircum}\ndot\n{\\lambda}\nu\n(1)\nsqr\nu\n(2)\nsum\nu\n(3)\n{\\texttimes}\nFigure 6.8: Examples of computational graphs. (a)The graph using the {\\texttimes} operation to\ncompute z = xy. (b)The graph for the logistic regression prediction y{\\textasciicircum} = {\\sigma}\nx\nw + b\n. Some of the intermediate expressions do not have names in the algebraic expression\nbut need names in the graph. We simply name the i-th such variable u\n(i)\n. (c)The\ncomputational graph for the expression H = max{\\{}0,XW + b{\\}}, which computes a design\nmatrix of rectified linear unit activations H given a design matrix containing a minibatch\nof inputs X. (d)Examples a{\\textendash}c applied at most one operation to each variable, but it\nis possible to apply more than one operation. Here we show a computation graph that\napplies more than one operation to the weights w of a linear regression model. The\nweights are used to make both the prediction y{\\textasciicircum} and the weight decay penalty {\\lambda} \ni w2\ni\n.\n206\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ng maps from R\nm to R\nn\n, and f maps from R\nn\nto R. If y = g(x) and z = f(y), then\n{\\partial}z\n{\\partial}xi = \nj\n{\\partial}z\n{\\partial}yj\n{\\partial}yj\n{\\partial}xi\n. (6.45)\nIn vector notation, this may be equivalently written as\n{\\nabla}xz =\n\n{\\partial}y\n{\\partial}x\n\n{\\nabla}y z, (6.46)\nwhere {\\partial}y\n{\\partial}x\nis the n {\\texttimes} m Jacobian matrix of g. From this we see that the gradient of a variable x can be obtained by multiplying\na Jacobian matrix {\\partial}y",
                    "\n{\\partial}x by a gradient {\\nabla}yz. The back-propagation algorithm consists\nof performing such a Jacobian-gradient product for each operation in the graph.\nUsually we do not apply the back-propagation algorithm merely to vectors, but rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the\nsame as back-propagation with vectors. The only difference is how the numbers\nare arranged in a grid to form a tensor. We could imagine flattening each tensor\ninto a vector before we run back-propagation, computing a vector-valued gradient,\nand then reshaping the gradient back into a tensor. In this rearranged view,\nback-propagation is still just multiplying Jacobians by gradients. To denote the gradient of a value z with respect to a tensor X, we write {\\nabla}Xz,\njust as if X were a vector. The indices into X now have multiple coordinates{\\textemdash}for\nexample, a 3-D tensor is indexed by three coordinates. We can abstract this away\nby using a single variable i to represent the complete tuple of indices. For all\npossible index tuples i, ({\\nabla}X z)i gives {\\partial}z {\\partial}Xi . This is exactly the same as how for all\npossible integer indices i into a vector, ({\\nabla}xz)i gives {\\partial}z {\\partial}xi . Using this notation, we\ncan write the chain rule as it applies to tensors. If Y = g(X) and z = f(Y), then\n{\\nabla}X z =\nj\n({\\nabla}XYj ) {\\partial}z\n{\\partial}Yj\n. (6.47)\n6.5.3 Recursively Applying the Chain Rule to Obtain Backprop\nUsing the chain rule, it is straightforward to write down an algebraic expression for\nthe gradient of a scalar with respect to any node in the computational graph that\nproduced that scalar. However, actually evaluating that expression in a computer\nintroduces some extra considerations. Specifically, many subexpressions may be repeated several times within the\noverall expression for the gradient. Any procedure that computes the gradient\n207\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nwill need to choose whether to store these subexpressions or",
                    " to recompute them\nseveral times. An example of how these repeated subexpressions arise is given in\nfigure 6.9. In some cases, computing the same subexpression twice would simply\nbe wasteful. For complicated graphs, there can be exponentially many of these\nwasted computations, making a naive implementation of the chain rule infeasible.\nIn other cases, computing the same subexpression twice could be a valid way to\nreduce memory consumption at the cost of higher runtime.\nWe first begin by a version of the back-propagation algorithm that specifies the\nactual gradient computation directly (algorithm 6.2 along with algorithm 6.1 for the\nassociated forward computation), in the order it will actually be done and according\nto the recursive application of chain rule. One could either directly perform these\ncomputations or view the description of the algorithm as a symbolic specification\nof the computational graph for computing the back-propagation. However, this\nformulation does not make explicit the manipulation and the construction of the\nsymbolic graph that performs the gradient computation. Such a formulation is\npresented below in section 6.5.6, with algorithm 6.5, where we also generalize to\nnodes that contain arbitrary tensors. First consider a computational graph describing how to compute a single scalar\nu\n(n) (say the loss on a training example). This scalar is the quantity whose\ngradient we want to obtain, with respect to the ni input nodes u\n(1) to u\n(ni )\n. In\nother words we wish to compute {\\partial}u\n(n)\n{\\partial}u(i) for all i {\\in} {\\{}1, 2, . . . , ni{\\}}. In the application\nof back-propagation to computing gradients for gradient descent over parameters, u\n(n) will be the cost associated with an example or a minibatch, while u\n(1) to u\n(ni)\ncorrespond to the parameters of the model.\nWe will assume that the nodes of the graph have been ordered in such a way\nthat we can compute their output one after the other, starting at u\n(ni +1) and\ngoing up to u\n(n)\n. As defined in algorithm 6.1, each node u\n(i)\nis associated with an\noperation f\n(i) and is computed by evaluating the function\nu\n(i)",
                    " = f(A\n(i) ) (6.48)\nwhere A(i)\nis the set of all nodes that are parents of u\n(i)\n. That algorithm specifies the forward propagation computation, which we could\nput in a graph G. In order to perform back-propagation, we can construct a\ncomputational graph that depends on G and adds to it an extra set of nodes. These\nform a subgraph B with one node per node of G. Computation in B proceeds in\nexactly the reverse of the order of computation in G, and each node of B computes\nthe derivative {\\partial}u\n(n)\n{\\partial}u(i) associated with the forward graph node u\n(i)\n. This is done\n208\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.1 A procedure that performs the computations mapping ni inputs\nu\n(1) to u\n(ni) to an output u\n(n)\n. This defines a computational graph where each node\ncomputes numerical value u\n(i) by applying a function f\n(i) to the set of arguments\nA\n(i) that comprises the values of previous nodes u\n(j)\n, j {<} i, with j {\\in} Pa(u\n(i)). The\ninput to the computational graph is the vector x, and is set into the first ni nodes\nu\n(1) to u\n(ni )\n. The output of the computational graph is read off the last (output)\nnode u\n(n)\n. for i = 1, . . . , ni do\nu\n(i) {\\textleftarrow} xi\nend for\nfor i = ni + 1, . . . , n do\nA\n(i) {\\textleftarrow} {\\{}u\n(j)\n| j {\\in} Pa(u\n(i) ){\\}}\nu\n(i) {\\textleftarrow} f\n(i)(A\n(i))\nend for\nreturn u\n(n)\nusing the chain rule with respect to scalar output u\n(n)\n:\n{\\partial}u\n(n)\n{\\partial}u\n(j) = \ni:j{\\in}Pa(u\n(i) )\n{\\partial}u\n(n)\n{\\partial}u\n(i",
                    ")\n{\\partial}u\n(i)\n{\\partial}u\n(j) (6.49)\nas specified by algorithm 6.2. The subgraph B contains exactly one edge for each\nedge from node u\n(j) to node u\n(i) of G. The edge from u\n(j) to u\n(i)\nis associated with\nthe computation of {\\partial}u\n(i)\n{\\partial}u(j) . In addition, a dot product is performed for each node,\nbetween the gradient already computed with respect to nodes u\n(i) that are children\nof u\n(j) and the vector containing the partial derivatives {\\partial}u\n(i)\n{\\partial}u(j) for the same children\nnodes u\n(i)\n. To summarize, the amount of computation required for performing\nthe back-propagation scales linearly with the number of edges in G, where the\ncomputation for each edge corresponds to computing a partial derivative (of one\nnode with respect to one of its parents) as well as performing one multiplication\nand one addition. Below, we generalize this analysis to tensor-valued nodes, which\nis just a way to group multiple scalar values in the same node and enable more\nefficient implementations. The back-propagation algorithm is designed to reduce the number of common\nsubexpressions without regard to memory. Specifically, it performs on the order\nof one Jacobian product per node in the graph. This can be seen from the fact\nthat backprop (algorithm 6.2) visits each edge from node u\n(j) to node u\n(i) of\nthe graph exactly once in order to obtain the associated partial derivative {\\partial}u\n(i)\n{\\partial}u(j) .\n209\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.2 Simplified version of the back-propagation algorithm for computing\nthe derivatives of u\n(n) with respect to the variables in the graph. This example is\nintended to further understanding by showing a simplified case where all variables\nare scalars, and we wish to compute the derivatives with respect to u\n(1)\n, . . . , u\n(ni )\n. This simplified version computes the derivatives of all nodes in the graph. The\ncomputational cost of this algorithm is proportional to the number of edges in\n",
                    "the graph, assuming that the partial derivative associated with each edge requires\na constant time. This is of the same order as the number of computations for\nthe forward propagation. Each {\\partial}u\n(i)\n{\\partial}u(j) is a function of the parents u\n(j) of u\n(i)\n, thus\nlinking the nodes of the forward graph to those added for the back-propagation\ngraph.\nRun forward propagation (algorithm 6.1 for this example) to obtain the activations of the network\nInitialize grad{\\_}table, a data structure that will store the derivatives that have\nbeen computed. The entry grad{\\_}table[u\n(i)\n] will store the computed value of\n{\\partial}u\n(n)\n{\\partial}u(i) . grad{\\_}table[u\n(n)\n] {\\textleftarrow} 1\nfor j = n {-} 1 down to 1 do\nThe next line computes {\\partial}u\n(n)\n{\\partial}u(j) = \ni:j{\\in}Pa(u\n(i) )\n{\\partial}u\n(n)\n{\\partial}u(i)\n{\\partial}u\n(i)\n{\\partial}u(j) using stored values:\ngrad{\\_}table[u\n(j)\n] {\\textleftarrow} \ni:j{\\in}Pa(u(i)) grad{\\_}table[u\n(i)\n] {\\partial}u\n(i)\n{\\partial}u\n(j)\nend for\nreturn {\\{}grad{\\_}table[u\n(i)\n] | i = 1, . . . , ni{\\}}\nBack-propagation thus avoids the exponential explosion in repeated subexpressions. However, other algorithms may be able to avoid more subexpressions by performing\nsimplifications on the computational graph, or may be able to conserve memory by\nrecomputing rather than storing some subexpressions. We will revisit these ideas\nafter describing the back-propagation algorithm itself.\n6.5.4 Back-Propagation Computation in Fully-Connected MLP\nTo clarify the above definition of the back-propagation computation, let us consider\nthe specific graph associated with a fully-connected multi-layer MLP. Algorithm 6.3",
                    " first shows the forward propagation, which maps parameters to\nthe supervised loss L(y{\\textasciicircum}, y) associated with a single (input,target) training example\n(x, y), with {\\textasciicircum}y the output of the neural network when x is provided in input.\nAlgorithm 6.4 then shows the corresponding computation to be done for\n210\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nFigure 6.9: A computational graph that results in repeated subexpressions when computing\nthe gradient. Let w {\\in} R be the input to the graph. We use the same function f : R {\\textrightarrow} R\nas the operation that we apply at every step of a chain: x = f(w), y = f(x), z = f(y). To compute\n{\\partial}z{\\partial}w\n, we apply equation 6.44 and obtain:\n{\\partial}z\n{\\partial}w\n(6.50) =\n{\\partial}z\n{\\partial}y\n{\\partial}y\n{\\partial}x\n{\\partial}x\n{\\partial}w\n(6.51) =f\n(y)f\n(x)f\n(w) (6.52) =f\n(f(f(w)))f\n(f(w))f\n(w) (6.53)\nEquation 6.52 suggests an implementation in which we compute the value of f (w) only\nonce and store it in the variable x. This is the approach taken by the back-propagation\nalgorithm. An alternative approach is suggested by equation 6.53, where the subexpression\nf(w) appears more than once. In the alternative approach, f (w) is recomputed each time\nit is needed. When the memory required to store the value of these expressions is low, the\nback-propagation approach of equation 6.52 is clearly preferable because of its reduced\nruntime. However, equation 6.53 is also a valid implementation of the chain rule, and is\nuseful when memory is limited.\n211\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\napplying the back-propagation algorithm to this graph.\nAlgorithms 6.3 and 6.4 are demonstrations that are chosen to be simple and\nstraightforward to",
                    " understand. However, they are specialized to one specific\nproblem.\nModern software implementations are based on the generalized form of backpropagation described in section 6.5.6 below, which can accommodate any computational graph by explicitly manipulating a data structure for representing symbolic\ncomputation.\nAlgorithm 6.3 Forward propagation through a typical deep neural network and\nthe computation of the cost function. The loss L(y{\\textasciicircum}, y) depends on the output\ny{\\textasciicircum} and on the target y (see section 6.2.1.1 for examples of loss functions). To\nobtain the total cost J, the loss may be added to a regularizer {\\Omega}({\\theta} ), where {\\theta}\ncontains all the parameters (weights and biases). Algorithm 6.4 shows how to\ncompute gradients of J with respect to parameters W and b. For simplicity, this\ndemonstration uses only a single input example x. Practical applications should\nuse a minibatch. See section 6.5.7 for a more realistic demonstration.\nRequire: Network depth, l\nRequire: W(i)\n,i {\\in} {\\{}1, . . . , l{\\}}, the weight matrices of the model\nRequire: b\n(i)\n,i {\\in} {\\{}1, . . . , l{\\}}, the bias parameters of the model\nRequire: x, the input to process\nRequire: y, the target output\nh\n(0) = x\nfor k = 1, . . . , l do\na\n(k) = b\n(k) + W(k)h\n(k{-}1)\nh\n(k) = f(a\n(k) )\nend for\ny{\\textasciicircum} = h\n(l)\nJ = L(y{\\textasciicircum}, y) + {\\lambda}{\\Omega}({\\theta})\n6.5.5 Symbol-to-Symbol Derivatives\nAlgebraic expressions and computational graphs both operate on symbols, or\nvariables that do not have specific values. These algebraic and graph-based\nrepresentations are called symbolic representations. When we actually use or\ntrain a neural network, we must assign specific values to these symbols. We\nreplace a symbolic input",
                    " to the network x with a specific numeric value, such as\n[1.2, 3.765, {-}1.8].\n212\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.4 Backward computation for the deep neural network of algorithm 6.3, which uses in addition to the input x a target y. This computation\nyields the gradients on the activations a\n(k) for each layer k, starting from the\noutput layer and going backwards to the first hidden layer. From these gradients, which can be interpreted as an indication of how each layer`s output should change\nto reduce error, one can obtain the gradient on the parameters of each layer. The\ngradients on weights and biases can be immediately used as part of a stochas- tic gradient update (performing the update right after the gradients have been\ncomputed) or used with other gradient-based optimization methods. After the forward computation, compute the gradient on the output layer:\ng {\\textleftarrow} {\\nabla}y{\\textasciicircum}J = {\\nabla}y{\\textasciicircum}L(y{\\textasciicircum}, y)\nfor k = l, l {-} 1, . . . , 1 do\nConvert the gradient on the layer`s output into a gradient into the prenonlinearity activation (element-wise multiplication if f is element-wise):\ng {\\textleftarrow} {\\nabla}a(k)J = g  f\n(a\n(k))\nCompute gradients on weights and biases (including the regularization term,\nwhere needed):\n{\\nabla}b\n(k)J = g + {\\lambda}{\\nabla}b\n(k){\\Omega}({\\theta})\n{\\nabla}W (k)J = g h\n(k{-}1) + {\\lambda}{\\nabla}W(k){\\Omega}({\\theta})\nPropagate the gradients w.r.t. the next lower-level hidden layer`s activations:\ng {\\textleftarrow} {\\nabla}h(k{-}1) J = W(k) g\nend for\n213\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf",
                    "\nf\nf\nz\nx\ny\nw\nf\nf\nf\ndzdy\nf\n\ndy\ndx\nf\n dzdx {\\texttimes}\ndx\ndw\nf\n dz dw\n{\\texttimes}\nFigure 6.10: An example of the symbol-to-symbol approach to computing derivatives. In\nthis approach, the back-propagation algorithm does not need to ever access any actual\nspecific numeric values. Instead, it adds nodes to a computational graph describing how\nto compute these derivatives. A generic graph evaluation engine can later compute the\nderivatives for any specific numeric values. (Left)In this example, we begin with a graph\nrepresenting z = f (f(f (w))). (Right)We run the back-propagation algorithm, instructing\nit to construct the graph for the expression corresponding to\ndzdw\n. In this example, we do\nnot explain how the back-propagation algorithm works. The purpose is only to illustrate\nwhat the desired result is: a computational graph with a symbolic description of the\nderivative.\nSome approaches to back-propagation take a computational graph and a set\nof numerical values for the inputs to the graph, then return a set of numerical\nvalues describing the gradient at those input values. We call this approach {\\textquotedblleft}symbolto-number{\\textquotedblright} differentiation. This is the approach used by libraries such as Torch\n(Collobert et al., 2011b) and Caffe (Jia, 2013). Another approach is to take a computational graph and add additional nodes\nto the graph that provide a symbolic description of the desired derivatives. This\nis the approach taken by Theano (Bergstra et al., 2010; Bastien et al., 2012)\nand TensorFlow (Abadi et al., 2015). An example of how this approach works\nis illustrated in figure 6.10. The primary advantage of this approach is that\nthe derivatives are described in the same language as the original expression. Because the derivatives are just another computational graph, it is possible to run\nback-propagation again, differentiating the derivatives in order to obtain higher\nderivatives. Computation of higher-order derivatives is described in section 6.5.10. We will use the latter approach and describe the back-propagation algorithm in\n214\nCHAPTER 6. DEEP FEED",
                    "FORWARD NETWORKS\nterms of constructing a computational graph for the derivatives. Any subset of the\ngraph may then be evaluated using specific numerical values at a later time. This\nallows us to avoid specifying exactly when each operation should be computed.\nInstead, a generic graph evaluation engine can evaluate every node as soon as its\nparents` values are available.\nThe description of the symbol-to-symbol based approach subsumes the symbolto-number approach. The symbol-to-number approach can be understood as\nperforming exactly the same computations as are done in the graph built by the\nsymbol-to-symbol approach. The key difference is that the symbol-to-number\napproach does not expose the graph.\n6.5.6 General Back-Propagation\nThe back-propagation algorithm is very simple. To compute the gradient of some\nscalar z with respect to one of its ancestors x in the graph, we begin by observing\nthat the gradient with respect to z is given by\ndzdz = 1. We can then compute\nthe gradient with respect to each parent of z in the graph by multiplying the\ncurrent gradient by the Jacobian of the operation that produced z. We continue\nmultiplying by Jacobians traveling backwards through the graph in this way until\nwe reach x. For any node that may be reached by going backwards from z through\ntwo or more paths, we simply sum the gradients arriving from different paths at\nthat node.\nMore formally, each node in the graph G corresponds to a variable. To achieve\nmaximum generality, we describe this variable as being a tensor V. Tensor can\nin general have any number of dimensions. They subsume scalars, vectors, and\nmatrices. We assume that each variable V is associated with the following subroutines:\n get{\\_}operation(V): This returns the operation that computes V, represented by the edges coming into V in the computational graph. For example,\nthere may be a Python or C++ class representing the matrix multiplication\noperation, and the get{\\_}operation function. Suppose we have a variable that\nis created by matrix multiplication, C = AB. Then get{\\_}operation(V)\nreturns a pointer to an instance of the corresponding C++ class.\n get{\\_}consumers(V, G): This returns the list of variables that are children of\nV in the computational graph G.\n get{\\_}",
                    "inputs(V, G): This returns the list of variables that are parents of V\nin the computational graph G.\n215\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nEach operation op is also associated with a bprop operation. This bprop\noperation can compute a Jacobian-vector product as described by equation 6.47. This is how the back-propagation algorithm is able to achieve great generality. Each operation is responsible for knowing how to back-propagate through the\nedges in the graph that it participates in. For example, we might use a matrix\nmultiplication operation to create a variable C = AB. Suppose that the gradient\nof a scalar z with respect to C is given by G. The matrix multiplication operation\nis responsible for defining two back-propagation rules, one for each of its input\narguments. If we call the bprop method to request the gradient with respect to\nA given that the gradient on the output is G, then the bprop method of the\nmatrix multiplication operation must state that the gradient with respect to A\nis given by GB. Likewise, if we call the bprop method to request the gradient\nwith respect to B, then the matrix operation is responsible for implementing the\nbprop method and specifying that the desired gradient is given by AG. The\nback-propagation algorithm itself does not need to know any differentiation rules. It\nonly needs to call each operation`s bprop rules with the right arguments. Formally, op.bprop(inputs, X, G) must return\n\ni\n({\\nabla}Xop.f(inputs)i) Gi, (6.54)\nwhich is just an implementation of the chain rule as expressed in equation 6.47. Here, inputs is a list of inputs that are supplied to the operation, op.f is the\nmathematical function that the operation implements, X is the input whose gradient\nwe wish to compute, and G is the gradient on the output of the operation.\nThe op.bprop method should always pretend that all of its inputs are distinct\nfrom each other, even if they are not. For example, if the mul operator is passed\ntwo copies of x to compute x\n2\n, the op.bprop method should still return x as the\nderivative with respect to both inputs. The back-propagation algorithm will later\nadd both of these arguments together to obtain 2x",
                    ", which is the correct total\nderivative on x. Software implementations of back-propagation usually provide both the operations and their bprop methods, so that users of deep learning software libraries are\nable to back-propagate through graphs built using common operations like matrix\nmultiplication, exponents, logarithms, and so on. Software engineers who build a\nnew implementation of back-propagation or advanced users who need to add their\nown operation to an existing library must usually derive the op.bprop method for\nany new operations manually. The back-propagation algorithm is formally described in algorithm 6.5.\n216\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.5 The outermost skeleton of the back-propagation algorithm. This\nportion does simple setup and cleanup work. Most of the important work happens\nin the build{\\_}grad subroutine of algorithm 6.6\n.Require: T, the target set of variables whose gradients must be computed.\nRequire: G, the computational graph\nRequire: z, the variable to be differentiated\nLet G\n be G pruned to contain only nodes that are ancestors of z and descendents\nof nodes in T. Initialize grad{\\_}table, a data structure associating tensors to their gradients\ngrad{\\_}table[z] {\\textleftarrow} 1\nfor V in T do\nbuild{\\_}grad(V, G, G\n, grad{\\_}table)\nend for\nReturn grad{\\_}table restricted to T\nIn section 6.5.2, we explained that back-propagation was developed in order to\navoid computing the same subexpression in the chain rule multiple times. The naive\nalgorithm could have exponential runtime due to these repeated subexpressions. Now that we have specified the back-propagation algorithm, we can understand its\ncomputational cost. If we assume that each operation evaluation has roughly the\nsame cost, then we may analyze the computational cost in terms of the number\nof operations executed. Keep in mind here that we refer to an operation as the\nfundamental unit of our computational graph, which might actually consist of very\nmany arithmetic operations (for example, we might have a graph that treats matrix\nmultiplication as a single operation). Computing a gradient in a graph with n nodes\nwill never execute more than O(n\n2) operations or store the output of",
                    " more than\nO(n\n2) operations. Here we are counting operations in the computational graph, not\nindividual operations executed by the underlying hardware, so it is important to\nremember that the runtime of each operation may be highly variable. For example,\nmultiplying two matrices that each contain millions of entries might correspond to\na single operation in the graph. We can see that computing the gradient requires as\nmost O(n\n2) operations because the forward propagation stage will at worst execute\nall n nodes in the original graph (depending on which values we want to compute,\nwe may not need to execute the entire graph). The back-propagation algorithm\nadds one Jacobian-vector product, which should be expressed with O(1) nodes, per\nedge in the original graph. Because the computational graph is a directed acyclic\ngraph it has at most O(n\n2 ) edges. For the kinds of graphs that are commonly used\nin practice, the situation is even better. Most neural network cost functions are\n217\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.6 The inner loop subroutine build{\\_}grad(V, G, G\n, grad{\\_}table) of\nthe back-propagation algorithm, called by the back-propagation algorithm defined\nin algorithm 6.5. Require: V, the variable whose gradient should be added to G and grad{\\_}table. Require: G, the graph to modify. Require: G\n , the restriction of G to nodes that participate in the gradient.\nRequire: grad{\\_}table, a data structure mapping nodes to their gradients\nif V is in grad{\\_}table then\nReturn grad{\\_}table[V]\nend if\ni {\\textleftarrow} 1\nfor C in get{\\_}consumers(V, G\n) do\nop {\\textleftarrow} get{\\_}operation(C)\nD {\\textleftarrow} build{\\_}grad(C, G,G\n, grad{\\_}table)\nG\n(i) {\\textleftarrow} op.bprop(get{\\_}inputs(C, G\n), V, D)\ni {\\textleftarrow} i + 1\nend for\nG {\\textleftarrow}\niG\n(i)\ngrad{\\_}table[V] = G\nInsert G",
                    " and the operations creating it into G\nReturn G\nroughly chain-structured, causing back-propagation to have O(n) cost. This is far\nbetter than the naive approach, which might need to execute exponentially many\nnodes. This potentially exponential cost can be seen by expanding and rewriting\nthe recursive chain rule (equation 6.49) non-recursively:\n{\\partial}u\n(n)\n{\\partial}u(j) = \npath(u\n({\\pi}1)\n,u\n({\\pi}2)\n,...,u\n({\\pi}t) ), from{\\pi}1=j to {\\pi}t=n \nt\nk=2\n{\\partial}u\n({\\pi}k)\n{\\partial}u({\\pi}k{-}1 )\n. (6.55)\nSince the number of paths from node j to node n can grow exponentially in the\nlength of these paths, the number of terms in the above sum, which is the number\nof such paths, can grow exponentially with the depth of the forward propagation\ngraph. This large cost would be incurred because the same computation for\n{\\partial}u\n(i)\n{\\partial}u(j) would be redone many times. To avoid such recomputation, we can think\nof back-propagation as a table-filling algorithm that takes advantage of storing\nintermediate results {\\partial}u\n(n)\n{\\partial}u(i) . Each node in the graph has a corresponding slot in a\ntable to store the gradient for that node. By filling in these table entries in order,\n218\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nback-propagation avoids repeating many common subexpressions. This table-filling\nstrategy is sometimes called dynamic programming.\n6.5.7 Example: Back-Propagation for MLP Training\nAs an example, we walk through the back-propagation algorithm as it is used to\ntrain a multilayer perceptron.\nHere we develop a very simple multilayer perception with a single hidden\nlayer. To train this model, we will use minibatch stochastic gradient descent. The back-propagation algorithm is used to compute the gradient of the cost on a\nsingle minibatch. Specifically, we use a minibatch of examples from the training\nset",
                    " formatted as a design matrix X and a vector of associated class labels y. The network computes a layer of hidden features H = max{\\{}0, XW (1){\\}}. To\nsimplify the presentation we do not use biases in this model. We assume that our\ngraph language includes a relu operation that can compute max{\\{}0, Z{\\}} element- wise. The predictions of the unnormalized log probabilities over classes are then\ngiven by HW(2)\n. We assume that our graph language includes a cross{\\_}entropy\noperation that computes the cross-entropy between the targets y and the probability\ndistribution defined by these unnormalized log probabilities. The resulting cross- entropy defines the cost JMLE. Minimizing this cross-entropy performs maximum\nlikelihood estimation of the classifier. However, to make this example more realistic,\nwe also include a regularization term. The total cost\nJ = JMLE + {\\lambda} \n\ni,j\nW\n(1)\ni,j 2\n+ \ni,j\nW\n(2)\ni,j 2\n (6.56)\nconsists of the cross-entropy and a weight decay term with coefficient {\\lambda}. The\ncomputational graph is illustrated in figure 6.11. The computational graph for the gradient of this example is large enough that\nit would be tedious to draw or to read. This demonstrates one of the benefits\nof the back-propagation algorithm, which is that it can automatically generate\ngradients that would be straightforward but tedious for a software engineer to\nderive manually. We can roughly trace out the behavior of the back-propagation algorithm\nby looking at the forward propagation graph in figure 6.11. To train, we wish\nto compute both {\\nabla}W (1)J and {\\nabla}W (2)J. There are two different paths leading\nbackward from J to the weights: one through the cross-entropy cost, and one\nthrough the weight decay cost. The weight decay cost is relatively simple; it will\nalways contribute 2{\\lambda}W(i) to the gradient on W(i)\n.\n219\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nX W (1)\nU\n(1)\nmatmul\nH\nrelu\nU\n(3) sqr u\n(4)",
                    " sum\nu {\\lambda} W (2) (7)\nU\n(2)\nmatmul\ny\nJMLE\ncross{\\_}entropy\nU\n(5) sqr u\n(6) sum\nu\n(8)\nJ\n+\n{\\texttimes}\n+\nFigure 6.11: The computational graph used to compute the cost used to train our example\nof a single-layer MLP using the cross-entropy loss and weight decay.\nThe other path through the cross-entropy cost is slightly more complicated.\nLet G be the gradient on the unnormalized log probabilities U(2) provided by\nthe cross{\\_}entropy operation. The back-propagation algorithm now needs to\nexplore two different branches. On the shorter branch, it adds HG to the\ngradient on W(2)\n, using the back-propagation rule for the second argument to\nthe matrix multiplication operation. The other branch corresponds to the longer\nchain descending further along the network. First, the back-propagation algorithm\ncomputes {\\nabla}HJ = GW (2) using the back-propagation rule for the first argument\nto the matrix multiplication operation. Next, the relu operation uses its back- propagation rule to zero out components of the gradient corresponding to entries\nof U(1) that were less than 0. Let the result be called G . The last step of the\nback-propagation algorithm is to use the back-propagation rule for the second\nargument of the matmul operation to add XG to the gradient on W(1)\n. After these gradients have been computed, it is the responsibility of the gradient\ndescent algorithm, or another optimization algorithm, to use these gradients to\nupdate the parameters. For the MLP, the computational cost is dominated by the cost of matrix\nmultiplication. During the forward propagation stage, we multiply by each weight\n220\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nmatrix, resulting in O(w) multiply-adds, where w is the number of weights. During\nthe backward propagation stage, we multiply by the transpose of each weight\nmatrix, which has the same computational cost. The main memory cost of the\nalgorithm is that we need to store the input to the nonlinearity of the hidden layer. This value is stored from the time it is computed until the backward pass",
                    " has\nreturned to the same point. The memory cost is thus O(mnh), where m is the\nnumber of examples in the minibatch and nh is the number of hidden units.\n6.5.8 Complications\nOur description of the back-propagation algorithm here is simpler than the implementations actually used in practice.\nAs noted above, we have restricted the definition of an operation to be a\nfunction that returns a single tensor. Most software implementations need to\nsupport operations that can return more than one tensor. For example, if we wish\nto compute both the maximum value in a tensor and the index of that value, it is\nbest to compute both in a single pass through memory, so it is most efficient to\nimplement this procedure as a single operation with two outputs. We have not described how to control the memory consumption of back- propagation. Back-propagation often involves summation of many tensors together.\nIn the naive approach, each of these tensors would be computed separately, then\nall of them would be added in a second step. The naive approach has an overly\nhigh memory bottleneck that can be avoided by maintaining a single buffer and\nadding each value to that buffer as it is computed.\nReal-world implementations of back-propagation also need to handle various\ndata types, such as 32-bit floating point, 64-bit floating point, and integer values. The policy for handling each of these types takes special care to design.\nSome operations have undefined gradients, and it is important to track these\ncases and determine whether the gradient requested by the user is undefined.\nVarious other technicalities make real-world differentiation more complicated.\nThese technicalities are not insurmountable, and this chapter has described the key\nintellectual tools needed to compute derivatives, but it is important to be aware\nthat many more subtleties exist.\n6.5.9 Differentiation outside the Deep Learning Community\nThe deep learning community has been somewhat isolated from the broader\ncomputer science community and has largely developed its own cultural attitudes\n221\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nconcerning how to perform differentiation. More generally, the field of automatic\ndifferentiation is concerned with how to compute derivatives algorithmically. The back-propagation algorithm described here is only one approach to automatic\ndifferentiation. It is a special case of a broader class of techniques called reverse\nmode accumulation. Other approaches evaluate the subexpressions of",
                    " the chain\nrule in different orders. In general, determining the order of evaluation that\nresults in the lowest computational cost is a difficult problem. Finding the optimal\nsequence of operations to compute the gradient is NP-complete (Naumann, 2008),\nin the sense that it may require simplifying algebraic expressions into their least\nexpensive form.\nFor example, suppose we have variables p1, p2, . . . , pn representing probabilities\nand variables z1, z2 , . . . , zn representing unnormalized log probabilities. Suppose\nwe define\nqi =\nexp(zi)\n\ni exp(zi)\n, (6.57)\nwhere we build the softmax function out of exponentiation, summation and division\noperations, and construct a cross-entropy loss J = {-} \ni pilog qi. A human\nmathematician can observe that the derivative of J with respect to zi takes a very\nsimple form: qi {-} pi. The back-propagation algorithm is not capable of simplifying\nthe gradient this way, and will instead explicitly propagate gradients through all of\nthe logarithm and exponentiation operations in the original graph. Some software\nlibraries such as Theano (Bergstra et al., 2010; Bastien et al., 2012) are able to\nperform some kinds of algebraic substitution to improve over the graph proposed\nby the pure back-propagation algorithm.\nWhen the forward graph G has a single output node and each partial derivative\n{\\partial}u\n(i)\n{\\partial}u(j) can be computed with a constant amount of computation, back-propagation\nguarantees that the number of computations for the gradient computation is of\nthe same order as the number of computations for the forward computation: this\ncan be seen in algorithm 6.2 because each local partial derivative {\\partial}u\n(i)\n{\\partial}u(j) needs to\nbe computed only once along with an associated multiplication and addition for\nthe recursive chain-rule formulation (equation 6.49). The overall computation is\ntherefore O({\\#} edges). However, it can potentially be reduced by simplifying the\ncomputational graph constructed by back-propagation, and this is an NP-complete\ntask. Implementations such as Theano and TensorFlow use heuristics based on\nmatching known simplification",
                    " patterns in order to iteratively attempt to simplify\nthe graph. We defined back-propagation only for the computation of a gradient of a\nscalar output but back-propagation can be extended to compute a Jacobian (either\nof k different scalar nodes in the graph, or of a tensor-valued node containing k\nvalues). A naive implementation may then need k times more computation: for\n222\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\neach scalar internal node in the original forward graph, the naive implementation\ncomputes k gradients instead of a single gradient. When the number of outputs of\nthe graph is larger than the number of inputs, it is sometimes preferable to use\nanother form of automatic differentiation called forward mode accumulation. Forward mode computation has been proposed for obtaining real-time computation\nof gradients in recurrent networks, for example (Williams and Zipser, 1989). This\nalso avoids the need to store the values and gradients for the whole graph, trading\noff computational efficiency for memory. The relationship between forward mode\nand backward mode is analogous to the relationship between left-multiplying versus\nright-multiplying a sequence of matrices, such as\nABCD, (6.58)\nwhere the matrices can be thought of as Jacobian matrices. For example, if D\nis a column vector while A has many rows, this corresponds to a graph with a\nsingle output and many inputs, and starting the multiplications from the end\nand going backwards only requires matrix-vector products. This corresponds to\nthe backward mode. Instead, starting to multiply from the left would involve a\nseries of matrix-matrix products, which makes the whole computation much more\nexpensive. However, if A has fewer rows than D has columns, it is cheaper to run\nthe multiplications left-to-right, corresponding to the forward mode.\nIn many communities outside of machine learning, it is more common to implement differentiation software that acts directly on traditional programming\nlanguage code, such as Python or C code, and automatically generates programs\nthat differentiate functions written in these languages. In the deep learning community, computational graphs are usually represented by explicit data structures\ncreated by specialized libraries. The specialized approach has the drawback of\nrequiring the library developer to define the bprop methods for every operation\nand limiting the user of the library to only those operations that have been defined.\nHowever, the specialized approach also has the benefit of allowing customized\nback-prop",
                    "agation rules to be developed for each operation, allowing the developer\nto improve speed or stability in non-obvious ways that an automatic procedure\nwould presumably be unable to replicate.\nBack-propagation is therefore not the only way or the optimal way of computing\nthe gradient, but it is a very practical method that continues to serve the deep\nlearning community very well. In the future, differentiation technology for deep\nnetworks may improve as deep learning practitioners become more aware of advances\nin the broader field of automatic differentiation.\n223\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.5.10 Higher-Order Derivatives\nSome software frameworks support the use of higher-order derivatives. Among the\ndeep learning software frameworks, this includes at least Theano and TensorFlow.\nThese libraries use the same kind of data structure to describe the expressions for\nderivatives as they use to describe the original function being differentiated. This\nmeans that the symbolic differentiation machinery can be applied to derivatives.\nIn the context of deep learning, it is rare to compute a single second derivative\nof a scalar function. Instead, we are usually interested in properties of the Hessian\nmatrix. If we have a function f : R\nn {\\textrightarrow} R, then the Hessian matrix is of size n {\\texttimes} n. In typical deep learning applications, n will be the number of parameters in the\nmodel, which could easily number in the billions. The entire Hessian matrix is\nthus infeasible to even represent.\nInstead of explicitly computing the Hessian, the typical deep learning approach\nis to use Krylov methods. Krylov methods are a set of iterative techniques for\nperforming various operations like approximately inverting a matrix or finding\napproximations to its eigenvectors or eigenvalues, without using any operation\nother than matrix-vector products.\nIn order to use Krylov methods on the Hessian, we only need to be able to\ncompute the product between the Hessian matrix H and an arbitrary vector v. A\nstraightforward technique (Christianson, 1992) for doing so is to compute\nHv = {\\nabla}x\n\n({\\nabla}xf(x))\nv\n\n. (6.59)\nBoth of the gradient computations in this expression may be computed automatically by the appropriate software library. Note that the outer gradient expression\ntakes the gradient of a function of the",
                    " inner gradient expression.\nIf v is itself a vector produced by a computational graph, it is important to\nspecify that the automatic differentiation software should not differentiate through\nthe graph that produced v. While computing the Hessian is usually not advisable, it is possible to do with\nHessian vector products. One simply computes He\n(i) for all i = 1, . . . , n, where\ne\n(i)\nis the one-hot vector with e\n(i)\ni = 1 and all other entries equal to 0.\n6.6 Historical Notes\nFeedforward networks can be seen as efficient nonlinear function approximators\nbased on using gradient descent to minimize the error in a function approximation.\n224\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nFrom this point of view, the modern feedforward network is the culmination of\ncenturies of progress on the general function approximation task. The chain rule that underlies the back-propagation algorithm was invented\nin the 17th century (Leibniz, 1676; L`Hopital, 1696). Calculus and algebra have\nlong been used to solve optimization problems in closed form, but gradient descent\nwas not introduced as a technique for iteratively approximating the solution to\noptimization problems until the 19th century (Cauchy, 1847). Beginning in the 1940s, these function approximation techniques were used to\nmotivate machine learning models such as the perceptron. However, the earliest\nmodels were based on linear models. Critics including Marvin Minsky pointed out\nseveral of the flaws of the linear model family, such as its inability to learn the\nXOR function, which led to a backlash against the entire neural network approach.\nLearning nonlinear functions required the development of a multilayer per- ceptron and a means of computing the gradient through such a model. Efficient\napplications of the chain rule based on dynamic programming began to appear\nin the 1960s and 1970s, mostly for control applications (Kelley, 1960; Bryson and\nDenham, 1961; Dreyfus, 1962; Bryson and Ho, 1969; Dreyfus, 1973) but also for\nsensitivity analysis (Linnainmaa, 1976). Werbos (1981) proposed applying these\ntechniques to training artificial neural networks. The idea was finally developed\nin practice after being independently rediscovered in different ways (LeCun, 1985;\nPark",
                    "er, 1985; Rumelhart et al., 1986a). The book Parallel Distributed Pro- cessing presented the results of some of the first successful experiments with\nback-propagation in a chapter (Rumelhart et al., 1986b) that contributed greatly\nto the popularization of back-propagation and initiated a very active period of\nresearch in multi-layer neural networks. However, the ideas put forward by the\nauthors of that book and in particular by Rumelhart and Hinton go much beyond\nback-propagation. They include crucial ideas about the possible computational\nimplementation of several central aspects of cognition and learning, which came\nunder the name of {\\textquotedblleft}connectionism{\\textquotedblright} because of the importance this school of thought\nplaces on the connections between neurons as the locus of learning and memory. In particular, these ideas include the notion of distributed representation (Hinton\net al., 1986). Following the success of back-propagation, neural network research gained popularity and reached a peak in the early 1990s. Afterwards, other machine learning\ntechniques became more popular until the modern deep learning renaissance that\nbegan in 2006.\nThe core ideas behind modern feedforward networks have not changed substantially since the 1980s. The same back-propagation algorithm and the same\n225\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\napproaches to gradient descent are still in use. Most of the improvement in neural\nnetwork performance from 1986 to 2015 can be attributed to two factors. First,\nlarger datasets have reduced the degree to which statistical generalization is a\nchallenge for neural networks. Second, neural networks have become much larger,\ndue to more powerful computers, and better software infrastructure. However, a\nsmall number of algorithmic changes have improved the performance of neural\nnetworks noticeably. One of these algorithmic changes was the replacement of mean squared error\nwith the cross-entropy family of loss functions. Mean squared error was popular in\nthe 1980s and 1990s, but was gradually replaced by cross-entropy losses and the\nprinciple of maximum likelihood as ideas spread between the statistics community\nand the machine learning community. The use of cross-entropy losses greatly\nimproved the performance of models with sigmoid and softmax outputs, which\nhad previously suffered from saturation and slow learning when using the mean\nsquared error loss. The other major algorithmic change that has greatly improved the performance\n",
                    "of feedforward networks was the replacement of sigmoid hidden units with piecewise\nlinear hidden units, such as rectified linear units. Rectification using the max{\\{}0, z{\\}}\nfunction was introduced in early neural network models and dates back at least\nas far as the Cognitron and Neocognitron (Fukushima, 1975, 1980). These early\nmodels did not use rectified linear units, but instead applied rectification to\nnonlinear functions. Despite the early popularity of rectification, rectification was\nlargely replaced by sigmoids in the 1980s, perhaps because sigmoids perform better\nwhen neural networks are very small. As of the early 2000s, rectified linear units\nwere avoided due to a somewhat superstitious belief that activation functions with\nnon-differentiable points must be avoided. This began to change in about 2009.\nJarrett et al. (2009) observed that {\\textquotedblleft}using a rectifying nonlinearity is the single most\nimportant factor in improving the performance of a recognition system{\\textquotedblright} among\nseveral different factors of neural network architecture design.\nFor small datasets, Jarrett et al. (2009) observed that using rectifying nonlinearities is even more important than learning the weights of the hidden layers. Random weights are sufficient to propagate useful information through a rectified\nlinear network, allowing the classifier layer at the top to learn how to map different\nfeature vectors to class identities. When more data is available, learning begins to extract enough useful knowledge\nto exceed the performance of randomly chosen parameters. Glorot et al. (2011a)\nshowed that learning is far easier in deep rectified linear networks than in deep\nnetworks that have curvature or two-sided saturation in their activation functions.\n226\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nRectified linear units are also of historical interest because they show that\nneuroscience has continued to have an influence on the development of deep\nlearning algorithms. Glorot et al. (2011a) motivate rectified linear units from\nbiological considerations. The half-rectifying nonlinearity was intended to capture\nthese properties of biological neurons: 1) For some inputs, biological neurons are\ncompletely inactive. 2) For some inputs, a biological neuron`s output is proportional\nto its input. 3) Most of the time, biological neurons operate in the regime where\nthey are inactive (i.e.,",
                    " they should have sparse activations). When the modern resurgence of deep learning began in 2006, feedforward\nnetworks continued to have a bad reputation. From about 2006-2012, it was widely\nbelieved that feedforward networks would not perform well unless they were assisted\nby other models, such as probabilistic models. Today, it is now known that with the\nright resources and engineering practices, feedforward networks perform very well.\nToday, gradient-based learning in feedforward networks is used as a tool to develop\nprobabilistic models, such as the variational autoencoder and generative adversarial\nnetworks, described in chapter 20. Rather than being viewed as an unreliable\ntechnology that must be supported by other techniques, gradient-based learning in\nfeedforward networks has been viewed since 2012 as a powerful technology that\nmay be applied to many other machine learning tasks. In 2006, the community\nused unsupervised learning to support supervised learning, and now, ironically, it\nis more common to use supervised learning to support unsupervised learning.\nFeedforward networks continue to have unfulfilled potential. In the future, we\nexpect they will be applied to many more tasks, and that advances in optimization\nalgorithms and model design will improve their performance even further. This\nchapter has primarily described the neural network family of models. In the\nsubsequent chapters, we turn to how to use these models{\\textemdash}how to regularize and\ntrain them."
                ]
            },
            {
                "8": [
                    "Chapter 8\nOptimization for Training Deep\nModels\nDeep learning algorithms involve optimization in many contexts. For example,\nperforming inference in models such as PCA involves solving an optimization\nproblem. We often use analytical optimization to write proofs or design algorithms. Of all of the many optimization problems involved in deep learning, the most\ndifficult is neural network training. It is quite common to invest days to months of\ntime on hundreds of machines in order to solve even a single instance of the neural\nnetwork training problem. Because this problem is so important and so expensive, a specialized set of optimization techniques have been developed for solving it.\nThis chapter presents these optimization techniques for neural network training.\nIf you are unfamiliar with the basic principles of gradient-based optimization,\nwe suggest reviewing chapter 4. That chapter includes a brief overview of numerical\noptimization in general.\nThis chapter focuses on one particular case of optimization: finding the parameters {\\theta} of a neural network that significantly reduce a cost function J({\\theta}), which\ntypically includes a performance measure evaluated on the entire training set as\nwell as additional regularization terms. We begin with a description of how optimization used as a training algorithm\nfor a machine learning task differs from pure optimization. Next, we present several\nof the concrete challenges that make optimization of neural networks difficult. We\nthen define several practical algorithms, including both optimization algorithms\nthemselves and strategies for initializing the parameters. More advanced algorithms\nadapt their learning rates during training or leverage information contained in\nthe second derivatives of the cost function. Finally, we conclude with a review of\nseveral optimization strategies that are formed by combining simple optimization\nalgorithms into higher-level procedures.\n\n\n\n8.1 How Learning Differs from Pure Optimization\nOptimization algorithms used for training of deep models differ from traditional\noptimization algorithms in several ways. Machine learning usually acts indirectly. In most machine learning scenarios, we care about some performance measure\nP, that is defined with respect to the test set and may also be intractable. We\ntherefore optimize P only indirectly. We reduce a different cost function J({\\theta}) in\nthe hope that doing so will improve P . This is in contrast to pure optimization,\nwhere minimizing J is a goal in and of itself. Optimization algorithms for training\ndeep models also typically include some specialization on the specific structure of\nmachine learning objective functions. Typically, the cost function can be written as an average over the training set, such",
                    " as\nJ({\\theta}) = E(x,y){\\sim}p{\\textasciicircum}dataL(f(x; {\\theta}), y), (8.1)\nwhere L is the per-example loss function, f (x; {\\theta}) is the predicted output when\nthe input is x, p{\\textasciicircum}data is the empirical distribution. In the supervised learning case, y is the target output. Throughout this chapter, we develop the unregularized\nsupervised case, where the arguments to L are f(x; {\\theta}) and y. However, it is trivial\nto extend this development, for example, to include {\\theta} or x as arguments, or to\nexclude y as arguments, in order to develop various forms of regularization or\nunsupervised learning.\nEquation 8.1 defines an objective function with respect to the training set. We\nwould usually prefer to minimize the corresponding objective function where the\nexpectation is taken across the data generating distribution pdata rather than just\nover the finite training set:\nJ\n{*}({\\theta}) = E(x,y){\\sim}pdataL(f(x; {\\theta}), y). (8.2)\n\n\n\n8.1.1 Empirical Risk Minimization\nThe goal of a machine learning algorithm is to reduce the expected generalization\nerror given by equation 8.2. This quantity is known as the risk. We emphasize here\nthat the expectation is taken over the true underlying distribution pdata. If we knew\nthe true distribution pdata(x, y), risk minimization would be an optimization task\nsolvable by an optimization algorithm. However, when we do not know pdata(x, y)\nbut only have a training set of samples, we have a machine learning problem.\nThe simplest way to convert a machine learning problem back into an optimization problem is to minimize the expected loss on the training set. This\nmeans replacing the true distribution p(x, y) with the empirical distribution p{\\textasciicircum}(x, y)\ndefined by the training set. We now minimize the empirical risk\nEx,y{\\sim}p{\\textasciicircum}data(x,y)\n[L(f(x; {\\theta}), y)] =\n1\nm m\ni=",
                    "1\nL(f(x\n(i)\n; {\\theta}), y\n(i) ) (8.3)\nwhere m is the number of training examples. The training process based on minimizing this average training error is known\nas empirical risk minimization. In this setting, machine learning is still very\nsimilar to straightforward optimization. Rather than optimizing the risk directly, we optimize the empirical risk, and hope that the risk decreases significantly as\nwell. A variety of theoretical results establish conditions under which the true risk\ncan be expected to decrease by various amounts. However, empirical risk minimization is prone to overfitting. Models with\nhigh capacity can simply memorize the training set. In many cases, empirical\nrisk minimization is not really feasible. The most effective modern optimization\nalgorithms are based on gradient descent, but many useful loss functions, such\nas 0-1 loss, have no useful derivatives (the derivative is either zero or undefined\neverywhere). These two problems mean that, in the context of deep learning, we\nrarely use empirical risk minimization. Instead, we must use a slightly different\napproach, in which the quantity that we actually optimize is even more different\nfrom the quantity that we truly want to optimize.\n\n\n8.1.2 Surrogate Loss Functions and Early Stopping\nSometimes, the loss function we actually care about (say classification error) is not\none that can be optimized efficiently. For example, exactly minimizing expected 0-1\nloss is typically intractable (exponential in the input dimension), even for a linear\nclassifier (Marcotte and Savard, 1992). In such situations, one typically optimizes\na surrogate loss function instead, which acts as a proxy but has advantages. For example, the negative log-likelihood of the correct class is typically used as a\nsurrogate for the 0-1 loss. The negative log-likelihood allows the model to estimate\nthe conditional probability of the classes, given the input, and if the model can\ndo that well, then it can pick the classes that yield the least classification error in\nexpectation.\n\n\n\n\n\nIn some cases, a surrogate loss function actually results in being able to learn\nmore. For example, the test set 0-1 loss often continues to decrease for a long\ntime after the training set 0-1 loss has reached zero, when training using the\nlog-likelihood surrogate. This is because even when the expected 0-1 loss",
                    " is zero,\none can improve the robustness of the classifier by further pushing the classes apart\nfrom each other, obtaining a more confident and reliable classifier, thus extracting\nmore information from the training data than would have been possible by simply\nminimizing the average 0-1 loss on the training set. A very important difference between optimization in general and optimization\nas we use it for training algorithms is that training algorithms do not usually halt\nat a local minimum. Instead, a machine learning algorithm usually minimizes\na surrogate loss function but halts when a convergence criterion based on early\nstopping (section 7.8) is satisfied. Typically the early stopping criterion is based\non the true underlying loss function, such as 0-1 loss measured on a validation set, and is designed to cause the algorithm to halt whenever overfitting begins to occur.\nTraining often halts while the surrogate loss function still has large derivatives, which is very different from the pure optimization setting, where an optimization\nalgorithm is considered to have converged when the gradient becomes very small.\n\n\n\n\n8.1.3 Batch and Minibatch Algorithms\nOne aspect of machine learning algorithms that separates them from general\noptimization algorithms is that the objective function usually decomposes as a sum\nover the training examples. Optimization algorithms for machine learning typically\ncompute each update to the parameters based on an expected value of the cost\nfunction estimated using only a subset of the terms of the full cost function.\nFor example, maximum likelihood estimation problems, when viewed in log\nspace, decompose into a sum over each example:\n{\\theta}ML = arg max\n{\\theta} m\ni=1\nlog pmodel(x\n(i)\n, y\n(i)\n; {\\theta}). (8.4)\nMaximizing this sum is equivalent to maximizing the expectation over the\nempirical distribution defined by the training set:\nJ({\\theta}) = Ex,y{\\sim}p{\\textasciicircum}data\nlog pmodel(x, y; {\\theta}). (8.5)\nMost of the properties of the objective function J used by most of our optimization algorithms are also expectations over the training set. For example, the\nmost commonly used property is the gradient:\n{\\nabla}{\\theta}J({\\theta}) = Ex,y{\\sim}p{\\textasciicircum}data",
                    "{\\nabla}{\\theta} log pmodel(x, y; {\\theta}). (8.6)\nComputing this expectation exactly is very expensive because it requires\nevaluating the model on every example in the entire dataset. In practice, we can\ncompute these expectations by randomly sampling a small number of examples\nfrom the dataset, then taking the average over only those examples. Recall that the standard error of the mean (equation 5.46) estimated from n\nsamples is given by {\\sigma}/ {\\sqrt{}}\nn, where {\\sigma} is the true standard deviation of the value of\nthe samples. The denominator of {\\sqrt{}}\nn shows that there are less than linear returns\nto using more examples to estimate the gradient. Compare two hypothetical\nestimates of the gradient, one based on 100 examples and another based on 10,000\nexamples. The latter requires 100 times more computation than the former, but\nreduces the standard error of the mean only by a factor of 10. Most optimization\nalgorithms converge much faster (in terms of total computation, not in terms of\nnumber of updates) if they are allowed to rapidly compute approximate estimates\nof the gradient rather than slowly computing the exact gradient.\nAnother consideration motivating statistical estimation of the gradient from a\nsmall number of samples is redundancy in the training set. In the worst case, all\nm samples in the training set could be identical copies of each other. A samplingbased estimate of the gradient could compute the correct gradient with a single\nsample, using m times less computation than the naive approach. In practice, we\nare unlikely to truly encounter this worst-case situation, but we may find large\nnumbers of examples that all make very similar contributions to the gradient. Optimization algorithms that use the entire training set are called batch or\ndeterministic gradient methods, because they process all of the training examples\nsimultaneously in a large batch. This terminology can be somewhat confusing\nbecause the word {\\textquotedblleft}batch{\\textquotedblright} is also often used to describe the minibatch used by\nminibatch stochastic gradient descent. Typically the term {\\textquotedblleft}batch gradient descent{\\textquotedblright}\nimplies the use of the full training set, while the use of the term {\\textquotedblleft}batch{\\textquotedblright} to describe\na group of examples does not",
                    ". For example, it is very common to use the term\n{\\textquotedblleft}batch size{\\textquotedblright} to describe the size of a minibatch.\nOptimization algorithms that use only a single example at a time are sometimes\ncalled stochastic or sometimes online methods. The term online is usually\nreserved for the case where the examples are drawn from a stream of continually\ncreated examples rather than from a fixed-size training set over which several\npasses are made.\nMost algorithms used for deep learning fall somewhere in between, using more\nthan one but less than all of the training examples. These were traditionally called\nminibatch or minibatch stochastic methods and it is now common to simply\ncall them stochastic methods. The canonical example of a stochastic method is stochastic gradient descent,\npresented in detail in section 8.3.1. Minibatch sizes are generally driven by the following factors:\n Larger batches provide a more accurate estimate of the gradient, but with\nless than linear returns.\n Multicore architectures are usually underutilized by extremely small batches. This motivates using some absolute minimum batch size, below which there\nis no reduction in the time to process a minibatch.\n If all examples in the batch are to be processed in parallel (as is typically\nthe case), then the amount of memory scales with the batch size. For many\nhardware setups this is the limiting factor in batch size.\n Some kinds of hardware achieve better runtime with specific sizes of arrays. Especially when using GPUs, it is common for power of 2 batch sizes to offer\nbetter runtime. Typical power of 2 batch sizes range from 32 to 256, with 16\nsometimes being attempted for large models.\n Small batches can offer a regularizing effect (Wilson and Martinez, 2003), perhaps due to the noise they add to the learning process. Generalization\nerror is often best for a batch size of 1. Training with such a small batch\nsize might require a small learning rate to maintain stability due to the high\nvariance in the estimate of the gradient. The total runtime can be very high\ndue to the need to make more steps, both because of the reduced learning\nrate and because it takes more steps to observe the entire training set. Different kinds of algorithms use different kinds of information from the minibatch in different ways. Some algorithms are more sensitive to sampling error than\nothers, either because they use information that is difficult",
                    " to estimate accurately\nwith few samples, or because they use information in ways that amplify sampling\nerrors more. Methods that compute updates based only on the gradient g are\nusually relatively robust and can handle smaller batch sizes like 100. Second-order\nmethods, which use also the Hessian matrix H and compute updates such as\nH{-}1g, typically require much larger batch sizes like 10,000. These large batch\nsizes are required to minimize fluctuations in the estimates of H{-}1g. Suppose\nthat H is estimated perfectly but has a poor condition number. Multiplication by\nH or its inverse amplifies pre-existing errors, in this case, estimation errors in g. Very small changes in the estimate of g can thus cause large changes in the update\nH{-}1g, even if H were estimated perfectly. Of course, H will be estimated only\napproximately, so the update H{-}1g will contain even more error than we would\npredict from applying a poorly conditioned operation to the estimate of g.\nIt is also crucial that the minibatches be selected randomly. Computing an\nunbiased estimate of the expected gradient from a set of samples requires that those\nsamples be independent. We also wish for two subsequent gradient estimates to be\nindependent from each other, so two subsequent minibatches of examples should\nalso be independent from each other. Many datasets are most naturally arranged\nin a way where successive examples are highly correlated. For example, we might\nhave a dataset of medical data with a long list of blood sample test results. This\nlist might be arranged so that first we have five blood samples taken at different\ntimes from the first patient, then we have three blood samples taken from the\nsecond patient, then the blood samples from the third patient, and so on. If we\nwere to draw examples in order from this list, then each of our minibatches would\nbe extremely biased, because it would represent primarily one patient out of the\nmany patients in the dataset. In cases such as these where the order of the dataset\nholds some significance, it is necessary to shuffle the examples before selecting\nminibatches. For very large datasets, for example datasets containing billions of\nexamples in a data center, it can be impractical to sample examples truly uniformly\nat random every time we want to construct a minibatch. Fortunately, in practice\nit is usually sufficient to shuffle the order of the dataset once and then store it in\nshuff",
                    "led fashion. This will impose a fixed set of possible minibatches of consecutive\nexamples that all models trained thereafter will use, and each individual model\nwill be forced to reuse this ordering every time it passes through the training\ndata. However, this deviation from true random selection does not seem to have a\nsignificant detrimental effect. Failing to ever shuffle the examples in any way can\nseriously reduce the effectiveness of the algorithm.\nMany optimization problems in machine learning decompose over examples\nwell enough that we can compute entire separate updates over different examples\nin parallel. In other words, we can compute the update that minimizes J(X) for\none minibatch of examples X at the same time that we compute the update for\nseveral other minibatches. Such asynchronous parallel distributed approaches are\ndiscussed further in section 12.1.3. An interesting motivation for minibatch stochastic gradient descent is that it\nfollows the gradient of the true generalization error (equation 8.2) so long as no\nexamples are repeated. Most implementations of minibatch stochastic gradient\ndescent shuffle the dataset once and then pass through it multiple times. On the\nfirst pass, each minibatch is used to compute an unbiased estimate of the true\ngeneralization error. On the second pass, the estimate becomes biased because it is\nformed by re-sampling values that have already been used, rather than obtaining\nnew fair samples from the data generating distribution.\nThe fact that stochastic gradient descent minimizes generalization error is\neasiest to see in the online learning case, where examples or minibatches are drawn\nfrom a stream of data. In other words, instead of receiving a fixed-size training\nset, the learner is similar to a living being who sees a new example at each instant,\nwith every example (x, y) coming from the data generating distribution pdata(x, y). In this scenario, examples are never repeated; every experience is a fair sample\nfrom pdata. The equivalence is easiest to derive when both x and y are discrete. In this\ncase, the generalization error (equation 8.2) can be written as a sum\nJ\n{*}({\\theta}) = \nx \ny\npdata(x, y)L(f(x; {\\theta}), y), (8.7)\nwith the exact gradient\ng = {\\nabla}{\\",
                    "theta}J\n{*}({\\theta}) = \nx \ny\npdata(x, y){\\nabla}{\\theta}L(f(x; {\\theta}), y). (8.8)\nWe have already seen the same fact demonstrated for the log-likelihood in equation 8.5 and equation 8.6; we observe now that this holds for other functions L\nbesides the likelihood. A similar result can be derived when x and y are continuous, under mild assumptions regarding pdata and L. Hence, we can obtain an unbiased estimator of the exact gradient of the\ngeneralization error by sampling a minibatch of examples {\\{}x\n(1)\n, . . . x\n(m){\\}} with corresponding targets y\n(i) from the data generating distribution pdata , and computing\nthe gradient of the loss with respect to the parameters for that minibatch:\n{\\textasciicircum}g =\n1\nm\n{\\nabla}{\\theta} \ni\nL(f(x\n(i)\n; {\\theta}), y\n(i)). (8.9)\nUpdating {\\theta} in the direction of g{\\textasciicircum} performs SGD on the generalization error.\nOf course, this interpretation only applies when examples are not reused.\nNonetheless, it is usually best to make several passes through the training set, unless the training set is extremely large. When multiple such epochs are used,\nonly the first epoch follows the unbiased gradient of the generalization error, but\nof course, the additional epochs usually provide enough benefit due to decreased\ntraining error to offset the harm they cause by increasing the gap between training\nerror and test error.\nWith some datasets growing rapidly in size, faster than computing power, it\nis becoming more common for machine learning applications to use each training\nexample only once or even to make an incomplete pass through the training\nset. When using an extremely large training set, overfitting is not an issue, so\nunderfitting and computational efficiency become the predominant concerns. See\nalso Bottou and Bousquet (2008) for a discussion of the effect of computational\nbottlenecks on generalization error, as the number of training examples grows.\n\n\n\n\n\n\n8.2 Challenges in Neural Network Optimization\nOptimization in general is an extremely difficult task. Traditionally,",
                    " machine\nlearning has avoided the difficulty of general optimization by carefully designing\nthe objective function and constraints to ensure that the optimization problem is\nconvex. When training neural networks, we must confront the general non-convex\ncase. Even convex optimization is not without its complications. In this section,\nwe summarize several of the most prominent challenges involved in optimization\nfor training deep models.\n\n\n\n\n\n\n8.2.1 Ill-Conditioning\nSome challenges arise even when optimizing convex functions. Of these, the most\nprominent is ill-conditioning of the Hessian matrix H. This is a very general\nproblem in most numerical optimization, convex or otherwise, and is described in\nmore detail in section 4.3.1. The ill-conditioning problem is generally believed to be present in neural\nnetwork training problems. Ill-conditioning can manifest by causing SGD to get\n{\\textquotedblleft}stuck{\\textquotedblright} in the sense that even very small steps increase the cost function.\nRecall from equation 4.9 that a second-order Taylor series expansion of the\ncost function predicts that a gradient descent step of {-}g will add\n1\n2\n2gHg {-} g\ng (8.10)\nto the cost. Ill-conditioning of the gradient becomes a problem when 1\n2\n2g Hg\nexceeds g g. To determine whether ill-conditioning is detrimental to a neural\nnetwork training task, one can monitor the squared gradient norm g g and\n{-}50 0 50 100 150 200 250\nTraining time (epochs)\n{-}2\n0\n2\n4\n6\n8\n10\n12\n14\n16\nGradient norm\n0 50 100 150 200 250\nTraining time (epochs)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nClassific\nation error rate\nFigure 8.1: Gradient descent often does not arrive at a critical point of any kind. In this\nexample, the gradient norm increases throughout training of a convolutional network used\nfor object detection. (Left)A scatterplot showing how the norms of individual gradient\nevaluations are distributed over time. To improve legibility, only one gradient norm\nis plotted per epoch. The running average",
                    " of all gradient norms is plotted as a solid\ncurve. The gradient norm clearly increases over time, rather than decreasing as we would\nexpect if the training process converged to a critical point. (Right)Despite the increasing\ngradient, the training process is reasonably successful. The validation set classification\nerror decreases to a low level.\nthe gHg term. In many cases, the gradient norm does not shrink significantly\nthroughout learning, but the gHg term grows by more than an order of magnitude.\nThe result is that learning becomes very slow despite the presence of a strong\ngradient because the learning rate must be shrunk to compensate for even stronger\ncurvature. Figure 8.1 shows an example of the gradient increasing significantly\nduring the successful training of a neural network.\nThough ill-conditioning is present in other settings besides neural network\ntraining, some of the techniques used to combat it in other contexts are less\napplicable to neural networks. For example, Newton`s method is an excellent tool\nfor minimizing convex functions with poorly conditioned Hessian matrices, but in\nthe subsequent sections we will argue that Newton`s method requires significant\nmodification before it can be applied to neural networks.\n\n\n\n\n\n\n8.2.2 Local Minima\nOne of the most prominent features of a convex optimization problem is that it\ncan be reduced to the problem of finding a local minimum. Any local minimum is\nguaranteed to be a global minimum. Some convex functions have a flat region at\nthe bottom rather than a single global minimum point, but any point within such\na flat region is an acceptable solution. When optimizing a convex function, we\nknow that we have reached a good solution if we find a critical point of any kind.\nWith non-convex functions, such as neural nets, it is possible to have many\nlocal minima. Indeed, nearly any deep model is essentially guaranteed to have\nan extremely large number of local minima. However, as we will see, this is not\nnecessarily a major problem.\nNeural networks and any models with multiple equivalently parametrized latent\nvariables all have multiple local minima because of the model identifiability\nproblem. A model is said to be identifiable if a sufficiently large training set can\nrule out all but one setting of the model`s parameters. Models with latent variables\nare often not identifiable because we can obtain equivalent models by exchanging\nlatent variables with each other.",
                    " For example, we could take a neural network and\nmodify layer 1 by swapping the incoming weight vector for unit i with the incoming\nweight vector for unit j, then doing the same for the outgoing weight vectors. If we\nhave m layers with n units each, then there are n!m ways of arranging the hidden\nunits. This kind of non-identifiability is known as weight space symmetry.\nIn addition to weight space symmetry, many kinds of neural networks have\nadditional causes of non-identifiability. For example, in any rectified linear or\nmaxout network, we can scale all of the incoming weights and biases of a unit by\n{\\alpha} if we also scale all of its outgoing weights by 1{\\alpha}\n. This means that{\\textemdash}if the cost\nfunction does not include terms such as weight decay that depend directly on the\nweights rather than the models` outputs{\\textemdash}every local minimum of a rectified linear\nor maxout network lies on an (m {\\texttimes} n)-dimensional hyperbola of equivalent local\nminima.\nThese model identifiability issues mean that there can be an extremely large\nor even uncountably infinite amount of local minima in a neural network cost\nfunction. However, all of these local minima arising from non-identifiability are\nequivalent to each other in cost function value. As a result, these local minima are\nnot a problematic form of non-convexity. Local minima can be problematic if they have high cost in comparison to the\nglobal minimum. One can construct small neural networks, even without hidden\nunits, that have local minima with higher cost than the global minimum (Sontag\nand Sussman, 1989; Brady et al., 1989; Gori and Tesi, 1992). If local minima\nwith high cost are common, this could pose a serious problem for gradient-based\noptimization algorithms.\nIt remains an open question whether there are many local minima of high cost\nfor networks of practical interest and whether optimization algorithms encounter\nthem. For many years, most practitioners believed that local minima were a\ncommon problem plaguing neural network optimization. Today, that does not\nappear to be the case. The problem remains an active area of research, but experts\nnow suspect that, for sufficiently large neural networks, most local minima have a\nlow cost function value, and that it is not important to find a true global",
                    " minimum\nrather than to find a point in parameter space that has low but not minimal cost\n(Saxe et al., 2013; Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska\net al., 2014). Many practitioners attribute nearly all difficulty with neural network optimization to local minima. We encourage practitioners to carefully test for specific\nproblems. A test that can rule out local minima as the problem is to plot the\nnorm of the gradient over time. If the norm of the gradient does not shrink to\ninsignificant size, the problem is neither local minima nor any other kind of critical\npoint. This kind of negative test can rule out local minima. In high dimensional\nspaces, it can be very difficult to positively establish that local minima are the\nproblem. Many structures other than local minima also have small gradients.\n\n\n\n8.2.3 Plateaus, Saddle Points and Other Flat Regions\nFor many high-dimensional non-convex functions, local minima (and maxima)\nare in fact rare compared to another kind of point with zero gradient: a saddle\npoint. Some points around a saddle point have greater cost than the saddle point,\nwhile others have a lower cost. At a saddle point, the Hessian matrix has both\npositive and negative eigenvalues. Points lying along eigenvectors associated with\npositive eigenvalues have greater cost than the saddle point, while points lying\nalong negative eigenvalues have lower value. We can think of a saddle point as\nbeing a local minimum along one cross-section of the cost function and a local\nmaximum along another cross-section. See figure 4.5 for an illustration.\nMany classes of random functions exhibit the following behavior: in lowdimensional spaces, local minima are common. In higher dimensional spaces, local\nminima are rare and saddle points are more common. For a function f : R\nn {\\textrightarrow} R of\nthis type, the expected ratio of the number of saddle points to local minima grows\nexponentially with n. To understand the intuition behind this behavior, observe\nthat the Hessian matrix at a local minimum has only positive eigenvalues. The\nHessian matrix at a saddle point has a mixture of positive and negative eigenvalues. Imagine that the sign of each eigenvalue is generated by flipping a coin. In a single\ndimension, it is easy to obtain a local minimum",
                    " by tossing a coin and getting heads\nonce. In n-dimensional space, it is exponentially unlikely that all n coin tosses will\nbe heads. See Dauphin et al. (2014) for a review of the relevant theoretical work.\nAn amazing property of many random functions is that the eigenvalues of the\nHessian become more likely to be positive as we reach regions of lower cost. In\nour coin tossing analogy, this means we are more likely to have our coin come up\nheads n times if we are at a critical point with low cost. This means that local\nminima are much more likely to have low cost than high cost. Critical points with\nhigh cost are far more likely to be saddle points. Critical points with extremely\nhigh cost are more likely to be local maxima.\nThis happens for many classes of random functions. Does it happen for neural\nnetworks? Baldi and Hornik (1989) showed theoretically that shallow autoencoders\n(feedforward networks trained to copy their input to their output, described in\nchapter 14) with no nonlinearities have global minima and saddle points but no\nlocal minima with higher cost than the global minimum. They observed without\nproof that these results extend to deeper networks without nonlinearities. The\noutput of such networks is a linear function of their input, but they are useful\nto study as a model of nonlinear neural networks because their loss function is\na non-convex function of their parameters. Such networks are essentially just\nmultiple matrices composed together. Saxe et al. (2013) provided exact solutions\nto the complete learning dynamics in such networks and showed that learning in\nthese models captures many of the qualitative features observed in the training of\ndeep models with nonlinear activation functions. Dauphin et al. (2014) showed\nexperimentally that real neural networks also have loss functions that contain very\nmany high-cost saddle points. Choromanska et al. (2014) provided additional\ntheoretical arguments, showing that another class of high-dimensional random\nfunctions related to neural networks does so as well.\nWhat are the implications of the proliferation of saddle points for training algorithms? For first-order optimization algorithms that use only gradient information,\nthe situation is unclear. The gradient can often become very small near a saddle\npoint. On the other hand, gradient descent empirically seems to be able to escape\nsaddle points in many cases. Goodfellow et al",
                    ". (2015) provided visualizations of\nseveral learning trajectories of state-of-the-art neural networks, with an example\ngiven in figure 8.2. These visualizations show a flattening of the cost function near\na prominent saddle point where the weights are all zero, but they also show the\ngradient descent trajectory rapidly escaping this region. Goodfellow et al. (2015)\nalso argue that continuous-time gradient descent may be shown analytically to be\nrepelled from, rather than attracted to, a nearby saddle point, but the situation\nmay be different for more realistic uses of gradient descent. For Newton`s method, it is clear that saddle points constitute a problem.\nProjection 2 of {\\theta}\nProjection 1 of {\\theta}\nJ( ) {\\theta}\n\n\n\nFigure 8.2: A visualization of the cost function of a neural network. Image adapted\nwith permission from Goodfellow et al. (2015). These visualizations appear similar for\nfeedforward neural networks, convolutional networks, and recurrent networks applied\nto real object recognition and natural language processing tasks. Surprisingly, these\nvisualizations usually do not show many conspicuous obstacles. Prior to the success of\nstochastic gradient descent for training very large models beginning in roughly 2012,\nneural net cost function surfaces were generally believed to have much more non-convex\nstructure than is revealed by these projections. The primary obstacle revealed by this\nprojection is a saddle point of high cost near where the parameters are initialized, but, as\nindicated by the blue path, the SGD training trajectory escapes this saddle point readily. Most of training time is spent traversing the relatively flat valley of the cost function, which may be due to high noise in the gradient, poor conditioning of the Hessian matrix\nin this region, or simply the need to circumnavigate the tall {\\textquotedblleft}mountain{\\textquotedblright} visible in the\nfigure via an indirect arcing path.\nGradient descent is designed to move {\\textquotedblleft}downhill{\\textquotedblright} and is not explicitly designed\nto seek a critical point. Newton`s method, however, is designed to solve for a\npoint where the gradient is zero. Without appropriate modification, it can jump\nto a saddle point. The proliferation of saddle points in high dimensional spaces\npresumably explains why second-order methods have not succeeded in replacing\ngradient",
                    " descent for neural network training. Dauphin et al. (2014) introduced a\nsaddle-free Newton method for second-order optimization and showed that it\nimproves significantly over the traditional version. Second-order methods remain\ndifficult to scale to large neural networks, but this saddle-free approach holds\npromise if it could be scaled.\nThere are other kinds of points with zero gradient besides minima and saddle\npoints. There are also maxima, which are much like saddle points from the\nperspective of optimization{\\textemdash}many algorithms are not attracted to them, but\nunmodified Newton`s method is. Maxima of many classes of random functions\nbecome exponentially rare in high dimensional space, just like minima do.\nThere may also be wide, flat regions of constant value. In these locations, the\ngradient and also the Hessian are all zero. Such degenerate locations pose major\nproblems for all numerical optimization algorithms. In a convex problem, a wide,\nflat region must consist entirely of global minima, but in a general optimization\nproblem, such a region could correspond to a high value of the objective function.\n\n\n8.2.4 Cliffs and Exploding Gradients\nNeural networks with many layers often have extremely steep regions resembling\ncliffs, as illustrated in figure 8.3. These result from the multiplication of several\nlarge weights together. On the face of an extremely steep cliff structure, the\ngradient update step can move the parameters extremely far, usually jumping off\nof the cliff structure altogether.\n\n\n  \nFigure 8.3: The objective function for highly nonlinear deep neural networks or for\nrecurrent neural networks often contains sharp nonlinearities in parameter space resulting\nfrom the multiplication of several parameters. These nonlinearities give rise to very\nhigh derivatives in some places. When the parameters get close to such a cliff region, a\ngradient descent update can catapult the parameters very far, possibly losing most of the\noptimization work that had been done. Figure adapted with permission from Pascanu\net al. (2013).\nThe cliff can be dangerous whether we approach it from above or from below,\nbut fortunately its most serious consequences can be avoided using the gradient\nclipping heuristic described in section 10.11.1. The basic idea is to recall that\nthe gradient does not specify the optimal step size, but only the optimal direction\nwithin an infinitesimal region. When the traditional gradient descent algorithm",
                    "\nproposes to make a very large step, the gradient clipping heuristic intervenes to\nreduce the step size to be small enough that it is less likely to go outside the region\nwhere the gradient indicates the direction of approximately steepest descent. Cliff\nstructures are most common in the cost functions for recurrent neural networks, because such models involve a multiplication of many factors, with one factor\nfor each time step. Long temporal sequences thus incur an extreme amount of\nmultiplication.\n\n\n\n\n\n8.2.5 Long-Term Dependencies\nAnother difficulty that neural network optimization algorithms must overcome\narises when the computational graph becomes extremely deep. Feedforward\nnetworks with many layers have such deep computational graphs. So do recurrent\nnetworks, described in chapter 10, which construct very deep computational graphs\nby repeatedly applying the same operation at each time step of a long temporal\nsequence. Repeated application of the same parameters gives rise to especially\npronounced difficulties. For example, suppose that a computational graph contains a path that consists\nof repeatedly multiplying by a matrix W. After t steps, this is equivalent to multiplying by Wt\n. Suppose that W has an eigendecomposition W = V diag({\\lambda})V\n{-}1\n. In this simple case, it is straightforward to see that\nW t =\nV diag({\\lambda})V\n{-}1t = V diag({\\lambda})\ntV\n{-}1\n. (8.11)\nAny eigenvalues {\\lambda}i that are not near an absolute value of 1 will either explode if they\nare greater than 1 in magnitude or vanish if they are less than 1 in magnitude. The\nvanishing and exploding gradient problem refers to the fact that gradients\nthrough such a graph are also scaled according to diag({\\lambda})\nt\n. Vanishing gradients\nmake it difficult to know which direction the parameters should move to improve\nthe cost function, while exploding gradients can make learning unstable. The cliff\nstructures described earlier that motivate gradient clipping are an example of the\nexploding gradient phenomenon.\nThe repeated multiplication by W at each time step described here is very\nsimilar to the power method algorithm used to find the largest eigenvalue of\na matrix W and the corresponding eigenvector. From this point of view it is\nnot surprising that x\nWt will eventually discard all components of x that are\northogonal to the principal eigen",
                    "vector of W. Recurrent networks use the same matrix W at each time step, but feedforward\nnetworks do not, so even very deep feedforward networks can largely avoid the\nvanishing and exploding gradient problem (Sussillo, 2014). We defer a further discussion of the challenges of training recurrent networks\nuntil section 10.7, after recurrent networks have been described in more detail.\n\n\n\n\n\n\n8.2.6 Inexact Gradients\nMost optimization algorithms are designed with the assumption that we have\naccess to the exact gradient or Hessian matrix. In practice, we usually only have\na noisy or even biased estimate of these quantities. Nearly every deep learning\nalgorithm relies on sampling-based estimates at least insofar as using a minibatch\nof training examples to compute the gradient.\nIn other cases, the objective function we want to minimize is actually intractable.\nWhen the objective function is intractable, typically its gradient is intractable as\nwell. In such cases we can only approximate the gradient. These issues mostly arise\n290\nCHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\nwith the more advanced models in part III. For example, contrastive divergence\ngives a technique for approximating the gradient of the intractable log-likelihood\nof a Boltzmann machine.\nVarious neural network optimization algorithms are designed to account for\nimperfections in the gradient estimate. One can also avoid the problem by choosing\na surrogate loss function that is easier to approximate than the true loss.\n\n\n\n\n\n\n8.2.7 Poor Correspondence between Local and Global Structure\nMany of the problems we have discussed so far correspond to properties of the\nloss function at a single point{\\textemdash}it can be difficult to make a single step if J({\\theta} ) is\npoorly conditioned at the current point {\\theta}, or if {\\theta} lies on a cliff, or if {\\theta} is a saddle\npoint hiding the opportunity to make progress downhill from the gradient.\nIt is possible to overcome all of these problems at a single point and still\nperform poorly if the direction that results in the most improvement locally does\nnot point toward distant regions of much lower cost. Goodfellow et al. (2015) argue that much of the runtime of training is due to\nthe length of the trajectory needed to arrive at the solution. Figure 8.2 shows that\nthe learning trajectory spends most of its time tracing",
                    " out a wide arc around a\nmountain-shaped structure.\nMuch of research into the difficulties of optimization has focused on whether\ntraining arrives at a global minimum, a local minimum, or a saddle point, but in\npractice neural networks do not arrive at a critical point of any kind. Figure 8.1\nshows that neural networks often do not arrive at a region of small gradient. Indeed,\nsuch critical points do not even necessarily exist. For example, the loss function {-}log p(y | x; {\\theta}) can lack a global minimum point and instead asymptotically\napproach some value as the model becomes more confident. For a classifier with\ndiscrete y and p(y | x) provided by a softmax, the negative log-likelihood can\nbecome arbitrarily close to zero if the model is able to correctly classify every\nexample in the training set, but it is impossible to actually reach the value of\nzero. Likewise, a model of real values p(y | x) = N (y; f({\\theta}), {\\beta} {-}1) can have negative\nlog-likelihood that asymptotes to negative infinity{\\textemdash}if f({\\theta}) is able to correctly\npredict the value of all training set y targets, the learning algorithm will increase\n{\\beta} without bound. See figure 8.4 for an example of a failure of local optimization to\nfind a good cost function value even in the absence of any local minima or saddle\npoints. Future research will need to develop further understanding of the factors that\ninfluence the length of the learning trajectory and better characterize the outcome\n{\\theta}\nJ( ) {\\theta}\n\n\nFigure 8.4: Optimization based on local downhill moves can fail if the local surface does\nnot point toward the global solution. Here we provide an example of how this can occur, even if there are no saddle points and no local minima. This example cost function\ncontains only asymptotes toward low values, not minima. The main cause of difficulty in\nthis case is being initialized on the wrong side of the {\\textquotedblleft}mountain{\\textquotedblright} and not being able to\ntraverse it. In higher dimensional space, learning algorithms can often circumnavigate\nsuch mountains but the trajectory associated with doing so may be long and result in\nexcessive training time, as illustrated in",
                    " figure 8.2.\nof the process. Many existing research directions are aimed at finding good initial points for\nproblems that have difficult global structure, rather than developing algorithms\nthat use non-local moves. Gradient descent and essentially all learning algorithms that are effective for\ntraining neural networks are based on making small, local moves. The previous\nsections have primarily focused on how the correct direction of these local moves\ncan be difficult to compute. We may be able to compute some properties of the\nobjective function, such as its gradient, only approximately, with bias or variance\nin our estimate of the correct direction. In these cases, local descent may or may\nnot define a reasonably short path to a valid solution, but we are not actually\nable to follow the local descent path. The objective function may have issues\nsuch as poor conditioning or discontinuous gradients, causing the region where\nthe gradient provides a good model of the objective function to be very small. In\nthese cases, local descent with steps of size  may define a reasonably short path\nto the solution, but we are only able to compute the local descent direction with\nsteps of size {\\delta}  . In these cases, local descent may or may not define a path\nto the solution, but the path contains many steps, so following the path incurs a\nhigh computational cost. Sometimes local information provides us no guide, when\nthe function has a wide flat region, or if we manage to land exactly on a critical\npoint (usually this latter scenario only happens to methods that solve explicitly\nfor critical points, such as Newton`s method). In these cases, local descent does\nnot define a path to a solution at all. In other cases, local moves can be too greedy\nand lead us along a path that moves downhill but away from any solution, as in\nfigure 8.4, or along an unnecessarily long trajectory to the solution, as in figure 8.2. Currently, we do not understand which of these problems are most relevant to\nmaking neural network optimization difficult, and this is an active area of research.\nRegardless of which of these problems are most significant, all of them might be\navoided if there exists a region of space connected reasonably directly to a solution\nby a path that local descent can follow, and if we are able to initialize learning\nwithin that well-behaved region. This last view suggests research into choosing\ngood initial points for traditional optimization algorithms to use.\n\n\n\n\n\n8.2.8 The",
                    "oretical Limits of Optimization\nSeveral theoretical results show that there are limits on the performance of any\noptimization algorithm we might design for neural networks (Blum and Rivest, 1992; Judd, 1989; Wolpert and MacReady, 1997). Typically these results have\nlittle bearing on the use of neural networks in practice.\nSome theoretical results apply only to the case where the units of a neural\nnetwork output discrete values. However, most neural network units output\nsmoothly increasing values that make optimization via local search feasible. Some\ntheoretical results show that there exist problem classes that are intractable, but\nit can be difficult to tell whether a particular problem falls into that class. Other\nresults show that finding a solution for a network of a given size is intractable, but\nin practice we can find a solution easily by using a larger network for which many\nmore parameter settings correspond to an acceptable solution. Moreover, in the\ncontext of neural network training, we usually do not care about finding the exact\nminimum of a function, but seek only to reduce its value sufficiently to obtain good\ngeneralization error. Theoretical analysis of whether an optimization algorithm\ncan accomplish this goal is extremely difficult. Developing more realistic bounds\non the performance of optimization algorithms therefore remains an important\ngoal for machine learning research.\n\n\n\n\n8.3 Basic Algorithms\nWe have previously introduced the gradient descent (section 4.3) algorithm that\nfollows the gradient of an entire training set downhill. This may be accelerated\nconsiderably by using stochastic gradient descent to follow the gradient of randomly\nselected minibatches downhill, as discussed in section 5.9 and section 8.1.3.\n\n\n\n\n\n\n8.3.1 Stochastic Gradient Descent\nStochastic gradient descent (SGD) and its variants are probably the most used\noptimization algorithms for machine learning in general and for deep learning\nin particular. As discussed in section 8.1.3, it is possible to obtain an unbiased\nestimate of the gradient by taking the average gradient on a minibatch of m\nexamples drawn i.i.d from the data generating distribution.\nAlgorithm 8.1 shows how to follow this estimate of the gradient downhill.\nAlgorithm 8.1 Stochastic gradient descent (SGD) update at training iteration k\nRequire: Learning rate k. Require: Initial parameter {\\theta}\nwhile stopping criterion not met do\nSample a min",
                    "ibatch of m examples from the training set {\\{}x\n(1)\n, . . . ,x\n(m){\\}} with\ncorresponding targets y\n(i)\n. Compute gradient estimate: g{\\textasciicircum} {\\textleftarrow} +\n1m {\\nabla}{\\theta} \ni L(f(x\n(i)\n; {\\theta}), y\n(i))\nApply update: {\\theta} {\\textleftarrow} {\\theta} {-} {\\textasciicircum}g\nend while\nA crucial parameter for the SGD algorithm is the learning rate. Previously, we\nhave described SGD as using a fixed learning rate . In practice, it is necessary to\ngradually decrease the learning rate over time, so we now denote the learning rate\nat iteration k as k. This is because the SGD gradient estimator introduces a source of noise (the\nrandom sampling of m training examples) that does not vanish even when we arrive\nat a minimum. By comparison, the true gradient of the total cost function becomes\nsmall and then 0 when we approach and reach a minimum using batch gradient\ndescent, so batch gradient descent can use a fixed learning rate. A sufficient\ncondition to guarantee convergence of SGD is that\n{\\infty}\nk=1\nk = {\\infty}, and (8.12)\n{\\infty}\nk=1\n\n2\nk {<} {\\infty}. (8.13)\nIn practice, it is common to decay the learning rate linearly until iteration {\\tau}:\nk = (1 {-} {\\alpha})0 + {\\alpha}{\\tau} (8.14)\nwith {\\alpha} = k\n{\\tau}\n. After iteration {\\tau} , it is common to leave  constant.\nThe learning rate may be chosen by trial and error, but it is usually best\nto choose it by monitoring learning curves that plot the objective function as a\nfunction of time. This is more of an art than a science, and most guidance on this\nsubject should be regarded with some skepticism. When using the linear schedule,\nthe parameters to choose are 0, {\\tau} , and {\\tau} . Usually {\\tau} may be set to the number of\niterations required to make a few hundred passes through the training set. Usually\n{\\t",
                    "au} should be set to roughly 1{\\%} the value of 0. The main question is how to set 0. If it is too large, the learning curve will show violent oscillations, with the cost\nfunction often increasing significantly. Gentle oscillations are fine, especially if\ntraining with a stochastic cost function such as the cost function arising from the\nuse of dropout. If the learning rate is too low, learning proceeds slowly, and if the\ninitial learning rate is too low, learning may become stuck with a high cost value.\nTypically, the optimal initial learning rate, in terms of total training time and the\nfinal cost value, is higher than the learning rate that yields the best performance\nafter the first 100 iterations or so. Therefore, it is usually best to monitor the first\nseveral iterations and use a learning rate that is higher than the best-performing\nlearning rate at this time, but not so high that it causes severe instability. The most important property of SGD and related minibatch or online gradient- based optimization is that computation time per update does not grow with the\nnumber of training examples. This allows convergence even when the number\nof training examples becomes very large. For a large enough dataset, SGD may\nconverge to within some fixed tolerance of its final test set error before it has\nprocessed the entire training set. To study the convergence rate of an optimization algorithm it is common to\nmeasure the excess error J({\\theta}){-} min{\\theta} J({\\theta}), which is the amount that the current\ncost function exceeds the minimum possible cost. When SGD is applied to a convex\nproblem, the excess error is O ({\\sqrt{}}\n1\nk\n) after k iterations, while in the strongly convex\ncase it is O(\n1\nk\n). These bounds cannot be improved unless extra conditions are\nassumed. Batch gradient descent enjoys better convergence rates than stochastic\ngradient descent in theory. However, the Cram{\\'e}r-Rao bound (Cram{\\'e}r, 1946; Rao, 1945) states that generalization error cannot decrease faster than O(\n1\nk\n). Bottou\nand Bousquet (2008) argue that it therefore may not be worthwhile to pursue\nan optimization algorithm that converges faster than O(\n1\nk\n) for machine learning\ntasks{\\textemdash}faster convergence presumably corresponds",
                    " to overfitting. Moreover, the\nasymptotic analysis obscures many advantages that stochastic gradient descent\nhas after a small number of steps. With large datasets, the ability of SGD to make\nrapid initial progress while evaluating the gradient for only very few examples\noutweighs its slow asymptotic convergence. Most of the algorithms described in\nthe remainder of this chapter achieve benefits that matter in practice but are lost\nin the constant factors obscured by the O(\n1\nk\n) asymptotic analysis. One can also\ntrade off the benefits of both batch and stochastic gradient descent by gradually\nincreasing the minibatch size during the course of learning.\nFor more information on SGD, see Bottou (1998).\n\n\n\n\n\n\n8.3.2 Momentum\nWhile stochastic gradient descent remains a very popular optimization strategy,\nlearning with it can sometimes be slow. The method of momentum (Polyak, 1964)\nis designed to accelerate learning, especially in the face of high curvature, small but\nconsistent gradients, or noisy gradients. The momentum algorithm accumulates\nan exponentially decaying moving average of past gradients and continues to move\nin their direction. The effect of momentum is illustrated in figure 8.5. Formally, the momentum algorithm introduces a variable v that plays the role\nof velocity{\\textemdash}it is the direction and speed at which the parameters move through\nparameter space. The velocity is set to an exponentially decaying average of the\nnegative gradient. The name momentum derives from a physical analogy, in\nwhich the negative gradient is a force moving a particle through parameter space,\naccording to Newton`s laws of motion. Momentum in physics is mass times velocity. In the momentum learning algorithm, we assume unit mass, so the velocity vector v\nmay also be regarded as the momentum of the particle. A hyperparameter {\\alpha} {\\in} [0, 1)\ndetermines how quickly the contributions of previous gradients exponentially decay. The update rule is given by:\nv {\\textleftarrow} {\\alpha}v {-} {\\nabla}{\\theta} \n1\nm m\ni=1\nL(f(x\n(i)\n; {\\theta}), y\n(i))\n, (8.15)\n{\\theta} {\\textleftarrow} {\\theta} + v. (8.16)\nThe velocity v accumulates",
                    " the gradient elements {\\nabla}{\\theta} \n1m m\ni=1 L(f(x\n(i)\n; {\\theta}), y\n(i) )\n. The larger {\\alpha} is relative to , the more previous gradients affect the current direction.\nThe SGD algorithm with momentum is given in algorithm 8.2.\n{-}30 {-}20 {-}10 0 10 20 {-}30\n{-}20\n{-}10\n0\n10\n20\nFigure 8.5: Momentum aims primarily to solve two problems: poor conditioning of the\nHessian matrix and variance in the stochastic gradient. Here, we illustrate how momentum\novercomes the first of these two problems. The contour lines depict a quadratic loss\nfunction with a poorly conditioned Hessian matrix. The red path cutting across the\ncontours indicates the path followed by the momentum learning rule as it minimizes this\nfunction. At each step along the way, we draw an arrow indicating the step that gradient\ndescent would take at that point. We can see that a poorly conditioned quadratic objective\nlooks like a long, narrow valley or canyon with steep sides. Momentum correctly traverses\nthe canyon lengthwise, while gradient steps waste time moving back and forth across the\nnarrow axis of the canyon. Compare also figure 4.6, which shows the behavior of gradient\ndescent without momentum.\nPreviously, the size of the step was simply the norm of the gradient multiplied\nby the learning rate. Now, the size of the step depends on how large and how\naligned a sequence of gradients are. The step size is largest when many successive\ngradients point in exactly the same direction. If the momentum algorithm always\nobserves gradient g, then it will accelerate in the direction of {-}g, until reaching a\nterminal velocity where the size of each step is\n||g||\n1 {-} {\\alpha}\n. (8.17)\nIt is thus helpful to think of the momentum hyperparameter in terms of 1\n1{-}{\\alpha}\n. For\nexample, {\\alpha} = .9 corresponds to multiplying the maximum speed by 10 relative to\nthe gradient descent algorithm.\nCommon values of {\\alpha} used in practice include .5, .9, and .99. Like the learning\nrate, {\\alpha} may also be adapted over time. Typically",
                    " it begins with a small value and\nis later raised. It is less important to adapt {\\alpha} over time than to shrink  over time.\nAlgorithm 8.2 Stochastic gradient descent (SGD) with momentum\nRequire: Learning rate , momentum parameter {\\alpha}. Require: Initial parameter {\\theta}, initial velocity v. while stopping criterion not met do\nSample a minibatch of m examples from the training set {\\{}x\n(1)\n, . . . ,x\n(m){\\}} with\ncorresponding targets y\n(i)\n. Compute gradient estimate: g {\\textleftarrow} 1m{\\nabla}{\\theta} \ni L(f(x\n(i)\n; {\\theta}), y\n(i) )\nCompute velocity update: v {\\textleftarrow} {\\alpha}v {-} g\nApply update: {\\theta} {\\textleftarrow} {\\theta} + v\nend while\nWe can view the momentum algorithm as simulating a particle subject to\ncontinuous-time Newtonian dynamics. The physical analogy can help to build\nintuition for how the momentum and gradient descent algorithms behave. The position of the particle at any point in time is given by {\\theta}(t). The particle\nexperiences net force f(t). This force causes the particle to accelerate:\nf(t) =\n{\\partial}\n2\n{\\partial}t2 {\\theta}(t). (8.18)\nRather than viewing this as a second-order differential equation of the position,\nwe can introduce the variable v(t) representing the velocity of the particle at time\nt and rewrite the Newtonian dynamics as a first-order differential equation:\nv(t) =\n{\\partial}\n{\\partial}t\n{\\theta}(t), (8.19)\nf(t) =\n{\\partial}\n{\\partial}t\nv(t). (8.20)\nThe momentum algorithm then consists of solving the differential equations via\nnumerical simulation. A simple numerical method for solving differential equations\nis Euler`s method, which simply consists of simulating the dynamics defined by\nthe equation by taking small, finite steps in the direction of each gradient.\nThis explains the basic form of the momentum update, but what specifically are\nthe forces? One force is proportional to the negative gradient of the cost function",
                    ": {-}{\\nabla}{\\theta} J ({\\theta}). This force pushes the particle downhill along the cost function surface.\nThe gradient descent algorithm would simply take a single step based on each\ngradient, but the Newtonian scenario used by the momentum algorithm instead\nuses this force to alter the velocity of the particle. We can think of the particle\nas being like a hockey puck sliding down an icy surface. Whenever it descends a\nsteep part of the surface, it gathers speed and continues sliding in that direction\nuntil it begins to go uphill again.\nOne other force is necessary. If the only force is the gradient of the cost function,\nthen the particle might never come to rest. Imagine a hockey puck sliding down\none side of a valley and straight up the other side, oscillating back and forth forever,\nassuming the ice is perfectly frictionless. To resolve this problem, we add one\nother force, proportional to {-}v(t). In physics terminology, this force corresponds\nto viscous drag, as if the particle must push through a resistant medium such as\nsyrup. This causes the particle to gradually lose energy over time and eventually\nconverge to a local minimum.\nWhy do we use {-}v(t) and viscous drag in particular? Part of the reason to\nuse {-}v(t) is mathematical convenience{\\textemdash}an integer power of the velocity is easy\nto work with. However, other physical systems have other kinds of drag based\non other integer powers of the velocity. For example, a particle traveling through\nthe air experiences turbulent drag, with force proportional to the square of the\nvelocity, while a particle moving along the ground experiences dry friction, with a\nforce of constant magnitude. We can reject each of these options. Turbulent drag,\nproportional to the square of the velocity, becomes very weak when the velocity is\nsmall. It is not powerful enough to force the particle to come to rest. A particle\nwith a non-zero initial velocity that experiences only the force of turbulent drag\nwill move away from its initial position forever, with the distance from the starting\npoint growing like O(log t). We must therefore use a lower power of the velocity. If we use a power of zero, representing dry friction, then the force is too strong.\nWhen the force due to the gradient of the cost function is small but non-zero, the\nconstant force",
                    " due to friction can cause the particle to come to rest before reaching\na local minimum. Viscous drag avoids both of these problems{\\textemdash}it is weak enough\nthat the gradient can continue to cause motion until a minimum is reached, but\nstrong enough to prevent motion if the gradient does not justify moving.\n\n\n8.3.3 Nesterov Momentum\nSutskever et al. (2013) introduced a variant of the momentum algorithm that was\ninspired by Nesterov`s accelerated gradient method (Nesterov, 1983, 2004). The\nupdate rules in this case are given by:\nv {\\textleftarrow} {\\alpha}v {-} {\\nabla}{\\theta}\n\n1\nm m\ni=1\nL\n\nf(x\n(i)\n; {\\theta} + {\\alpha}v), y\n(i)\n\n, (8.21)\n{\\theta} {\\textleftarrow} {\\theta} + v, (8.22)\nwhere the parameters {\\alpha} and  play a similar role as in the standard momentum\nmethod. The difference between Nesterov momentum and standard momentum is\nwhere the gradient is evaluated. With Nesterov momentum the gradient is evaluated\nafter the current velocity is applied. Thus one can interpret Nesterov momentum\nas attempting to add a correction factor to the standard method of momentum.\nThe complete Nesterov momentum algorithm is presented in algorithm 8.3.\nIn the convex batch gradient case, Nesterov momentum brings the rate of\nconvergence of the excess error from O(1/k) (after k steps) to O(1/k2) as shown\nby Nesterov (1983). Unfortunately, in the stochastic gradient case, Nesterov\nmomentum does not improve the rate of convergence.\n\n\n\nAlgorithm 8.3 Stochastic gradient descent (SGD) with Nesterov momentum\nRequire: Learning rate , momentum parameter {\\alpha}. Require: Initial parameter {\\theta}, initial velocity v. while stopping criterion not met do\nSample a minibatch of m examples from the training set {\\{}x\n(1)\n, . . . ,x\n(m){\\}} with\ncorresponding labels y\n(i)\n. Apply interim update: {\\theta}{\\textasciitilde} {\\textleftarrow} {\\theta} + {\\",
                    "alpha}v\nCompute gradient (at interim point): g {\\textleftarrow} 1m {\\nabla}{\\theta}{\\textasciitilde} \ni L(f(x\n(i)\n; {\\theta}{\\textasciitilde}), y\n(i))\nCompute velocity update: v {\\textleftarrow} {\\alpha}v {-} g\nApply update: {\\theta} {\\textleftarrow} {\\theta} + v\nend while"
                ]
            },
            {
                "9": [
                    "Chapter 9\nConvolutional Networks\nConvolutional networks (LeCun, 1989), also known as convolutional neural\nnetworks or CNNs, are a specialized kind of neural network for processing data\nthat has a known, grid-like topology. Examples include time-series data, which can\nbe thought of as a 1D grid taking samples at regular time intervals, and image data,\nwhich can be thought of as a 2D grid of pixels. Convolutional networks have been\ntremendously successful in practical applications. The name {\\textquotedblleft}convolutional neural\nnetwork{\\textquotedblright} indicates that the network employs a mathematical operation called\nconvolution. Convolution is a specialized kind of linear operation. Convolutional\nnetworks are simply neural networks that use convolution in place of general matrix\nmultiplication in at least one of their layers.\nIn this chapter, we will first describe what convolution is. Next, we will\nexplain the motivation behind using convolution in a neural network. We will then\ndescribe an operation called pooling, which almost all convolutional networks\nemploy. Usually, the operation used in a convolutional neural network does not\ncorrespond precisely to the definition of convolution as used in other fields such\nas engineering or pure mathematics. We will describe several variants on the\nconvolution function that are widely used in practice for neural networks. We\nwill also show how convolution may be applied to many kinds of data, with\ndifferent numbers of dimensions. We then discuss means of making convolution\nmore efficient. Convolutional networks stand out as an example of neuroscientific\nprinciples influencing deep learning. We will discuss these neuroscientific principles, then conclude with comments about the role convolutional networks have played\nin the history of deep learning. One topic this chapter does not address is how to\nchoose the architecture of your convolutional network. The goal of this chapter is\nto describe the kinds of tools that convolutional networks provide, while chapter 11\n330\nCHAPTER 9. CONVOLUTIONAL NETWORKS\ndescribes general guidelines for choosing which tools to use in which circumstances.\nResearch into convolutional network architectures proceeds so rapidly that a new\nbest architecture for a given benchmark is announced every few weeks to months, rendering it impractical to describe the best architecture in print. However, the\nbest architectures have consistently been composed of the building blocks described\nhere.\n9.",
                    "1 The Convolution Operation\nIn its most general form, convolution is an operation on two functions of a realvalued argument. To motivate the definition of convolution, we start with examples\nof two functions we might use. Suppose we are tracking the location of a spaceship with a laser sensor. Our\nlaser sensor provides a single output x(t), the position of the spaceship at time\nt. Both x and t are real-valued, i.e., we can get a different reading from the laser\nsensor at any instant in time.\nNow suppose that our laser sensor is somewhat noisy. To obtain a less noisy\nestimate of the spaceship`s position, we would like to average together several\nmeasurements. Of course, more recent measurements are more relevant, so we will\nwant this to be a weighted average that gives more weight to recent measurements. We can do this with a weighting function w(a), where a is the age of a measurement.\nIf we apply such a weighted average operation at every moment, we obtain a new\nfunction s providing a smoothed estimate of the position of the spaceship:\ns(t) =\n x(a)w(t {-} a)da (9.1)\nThis operation is called convolution. The convolution operation is typically\ndenoted with an asterisk:\ns(t) = (x {*} w)(t) (9.2)\nIn our example, w needs to be a valid probability density function, or the\noutput is not a weighted average. Also, w needs to be 0 for all negative arguments, or it will look into the future, which is presumably beyond our capabilities. These\nlimitations are particular to our example though. In general, convolution is defined\nfor any functions for which the above integral is defined, and may be used for other\npurposes besides taking weighted averages.\nIn convolutional network terminology, the first argument (in this example, the\nfunction x) to the convolution is often referred to as the input and the second\n331\nCHAPTER 9. CONVOLUTIONAL NETWORKS\nargument (in this example, the function w) as the kernel. The output is sometimes\nreferred to as the feature map.\nIn our example, the idea of a laser sensor that can provide measurements\nat every instant in time is not realistic. Usually, when we work with data on a\ncomputer, time will be discretized, and our sensor will provide",
                    " data at regular\nintervals. In our example, it might be more realistic to assume that our laser\nprovides a measurement once per second. The time index t can then take on only\ninteger values. If we now assume that x and w are defined only on integer t, we\ncan define the discrete convolution:\ns(t) = (x {*} w)(t) = {\\infty}\na={-}{\\infty}\nx(a)w(t {-} a) (9.3)\nIn machine learning applications, the input is usually a multidimensional array\nof data and the kernel is usually a multidimensional array of parameters that are\nadapted by the learning algorithm. We will refer to these multidimensional arrays\nas tensors. Because each element of the input and kernel must be explicitly stored\nseparately, we usually assume that these functions are zero everywhere but the\nfinite set of points for which we store the values. This means that in practice we\ncan implement the infinite summation as a summation over a finite number of\narray elements. Finally, we often use convolutions over more than one axis at a time. For\nexample, if we use a two-dimensional image I as our input, we probably also want\nto use a two-dimensional kernel K:\nS(i, j) = (I {*} K)(i, j) = \nm \nn\nI(m, n)K(i {-} m, j {-} n). (9.4)\nConvolution is commutative, meaning we can equivalently write:\nS(i, j) = (K {*} I)(i, j) = \nm \nn\nI(i {-} m, j {-} n)K(m, n). (9.5)\nUsually the latter formula is more straightforward to implement in a machine\nlearning library, because there is less variation in the range of valid values of m\nand n. The commutative property of convolution arises because we have flipped the\nkernel relative to the input, in the sense that as m increases, the index into the\ninput increases, but the index into the kernel decreases. The only reason to flip\nthe kernel is to obtain the commutative property. While the commutative property\n332\nCHAPTER 9. CONVOLUTIONAL NETWORKS\nis useful for writing proofs, it is",
                    " not usually an important property of a neural\nnetwork implementation. Instead, many neural network libraries implement a\nrelated function called the cross-correlation, which is the same as convolution\nbut without flipping the kernel:\nS(i, j) = (I {*} K)(i, j) = \nm \nn\nI(i + m, j + n)K(m, n). (9.6)\nMany machine learning libraries implement cross-correlation but call it convolution.\nIn this text we will follow this convention of calling both operations convolution,\nand specify whether we mean to flip the kernel or not in contexts where kernel\nflipping is relevant. In the context of machine learning, the learning algorithm will\nlearn the appropriate values of the kernel in the appropriate place, so an algorithm\nbased on convolution with kernel flipping will learn a kernel that is flipped relative\nto the kernel learned by an algorithm without the flipping. It is also rare for\nconvolution to be used alone in machine learning; instead convolution is used\nsimultaneously with other functions, and the combination of these functions does\nnot commute regardless of whether the convolution operation flips its kernel or\nnot.\nSee figure 9.1 for an example of convolution (without kernel flipping) applied\nto a 2-D tensor.\nDiscrete convolution can be viewed as multiplication by a matrix. However, the\nmatrix has several entries constrained to be equal to other entries. For example,\nfor univariate discrete convolution, each row of the matrix is constrained to be\nequal to the row above shifted by one element. This is known as a Toeplitz\nmatrix. In two dimensions, a doubly block circulant matrix corresponds to\nconvolution. In addition to these constraints that several elements be equal to\neach other, convolution usually corresponds to a very sparse matrix (a matrix\nwhose entries are mostly equal to zero). This is because the kernel is usually much\nsmaller than the input image. Any neural network algorithm that works with\nmatrix multiplication and does not depend on specific properties of the matrix\nstructure should work with convolution, without requiring any further changes\nto the neural network. Typical convolutional neural networks do make use of\nfurther specializations in order to deal with large inputs efficiently, but these are\nnot strictly necessary from a theoretical perspective.\n333\nCHAPTER 9. CONVOLUTIONAL NETWORKS\na b c d\ne",
                    " f g h\ni j k l\nw x\ny z\naw + bx +\ney + fz aw + bx +\ney + fz\nbw + cx +\nfy + gz\nbw + cx +\nfy + gz\ncw + dx +\ngy + hz\ncw + dx +\ngy + hz\new + fx +\niy + jz\new + fx +\niy + jz fw + gx +\njy + kz fw + gx +\njy + kz gw + hx +\nky + lz gw + hx +\nky + lz\nInput\nKernel\nOutput\nFigure 9.1: An example of 2-D convolution without kernel-flipping. In this case we restrict\nthe output to only positions where the kernel lies entirely within the image, called {\\textquotedblleft}valid{\\textquotedblright} convolution in some contexts. We draw boxes with arrows to indicate how the upper-left\nelement of the output tensor is formed by applying the kernel to the corresponding\nupper-left region of the input tensor.\n334\nCHAPTER 9. CONVOLUTIONAL NETWORKS\n9.2 Motivation\nConvolution leverages three important ideas that can help improve a machine\nlearning system: sparse interactions, parameter sharing and equivariant\nrepresentations. Moreover, convolution provides a means for working with\ninputs of variable size. We now describe each of these ideas in turn.\nTraditional neural network layers use matrix multiplication by a matrix of\nparameters with a separate parameter describing the interaction between each input\nunit and each output unit. This means every output unit interacts with every input\nunit. Convolutional networks, however, typically have sparse interactions (also\nreferred to as sparse connectivity or sparse weights). This is accomplished by\nmaking the kernel smaller than the input. For example, when processing an image,\nthe input image might have thousands or millions of pixels, but we can detect small,\nmeaningful features such as edges with kernels that occupy only tens or hundreds of\npixels. This means that we need to store fewer parameters, which both reduces the\nmemory requirements of the model and improves its statistical efficiency. It also\nmeans that computing the output requires fewer operations. These improvements\nin efficiency are usually quite large. If there are m inputs and n outputs, then\nmatrix",
                    " multiplication requires m{\\texttimes}n parameters and the algorithms used in practice\nhave O(m {\\texttimes} n) runtime (per example). If we limit the number of connections\neach output may have to k, then the sparsely connected approach requires only\nk {\\texttimes} n parameters and O(k {\\texttimes} n) runtime. For many practical applications, it is\npossible to obtain good performance on the machine learning task while keeping\nk several orders of magnitude smaller than m. For graphical demonstrations of\nsparse connectivity, see figure 9.2 and figure 9.3. In a deep convolutional network,\nunits in the deeper layers may indirectly interact with a larger portion of the input,\nas shown in figure 9.4. This allows the network to efficiently describe complicated\ninteractions between many variables by constructing such interactions from simple\nbuilding blocks that each describe only sparse interactions. Parameter sharing refers to using the same parameter for more than one\nfunction in a model. In a traditional neural net, each element of the weight matrix\nis used exactly once when computing the output of a layer. It is multiplied by\none element of the input and then never revisited. As a synonym for parameter\nsharing, one can say that a network has tied weights, because the value of the\nweight applied to one input is tied to the value of a weight applied elsewhere. In\na convolutional neural net, each member of the kernel is used at every position\nof the input (except perhaps some of the boundary pixels, depending on the\ndesign decisions regarding the boundary). The parameter sharing used by the\nconvolution operation means that rather than learning a separate set of parameters\n335\nCHAPTER 9. CONVOLUTIONAL NETWORKS\nx1 x2 x3\ns1 s2 s3\nx4\ns4\nx5\ns5\nx1 x2 x3\ns1 s2 s3\nx4\ns4\nx5\ns5\nFigure 9.2: Sparse connectivity, viewed from below: We highlight one input unit, x3, and also highlight the output units in s that are affected by this unit. (Top)When s is\nformed by convolution with a kernel of width 3, only three outputs are affected by x. (Bottom)When s is formed by matrix multiplication, connectivity is no longer sparse, so\nall of the outputs are affected by x3.\n336\nCHAPTER 9. CONVOLUTIONAL NETWORKS",
                    "\nx1 x2 x3\ns1 s2 s3\nx4\ns4\nx5\ns5\nx1 x2 x3\ns1 s2 s3\nx4\ns4\nx5\ns5\nFigure 9.3: Sparse connectivity, viewed from above: We highlight one output unit, s3, and also highlight the input units in x that affect this unit. These units are known\nas the receptive field of s3. (Top)When s is formed by convolution with a kernel of\nwidth 3, only three inputs affect s3. (Bottom)When s is formed by matrix multiplication, connectivity is no longer sparse, so all of the inputs affect s3.\nx1 x2 x3\nh1 h2 h3\nx4\nh4\nx5\nh5\ng1 g2 g3 g4 g5\nFigure 9.4: The receptive field of the units in the deeper layers of a convolutional network\nis larger than the receptive field of the units in the shallow layers. This effect increases if\nthe network includes architectural features like strided convolution (figure 9.12) or pooling\n(section 9.3). This means that even though direct connections in a convolutional net are\nvery sparse, units in the deeper layers can be indirectly connected to all or most of the\ninput image.\n337\nCHAPTER 9. CONVOLUTIONAL NETWORKS\nx1 x2 x3\ns1 s2 s3\nx4\ns4\nx5\ns5\nx1 x2 x3 x4 x5\ns1 s2 s3 s4 s5\nFigure 9.5: Parameter sharing: Black arrows indicate the connections that use a particular\nparameter in two different models. (Top)The black arrows indicate uses of the central\nelement of a 3-element kernel in a convolutional model. Due to parameter sharing, this\nsingle parameter is used at all input locations. (Bottom)The single black arrow indicates\nthe use of the central element of the weight matrix in a fully connected model. This model\nhas no parameter sharing so the parameter is used only once.\nfor every location, we learn only one set. This does not affect the runtime of\nforward propagation{\\textemdash}it is still O(k {\\texttimes} n){\\textemdash}but it does further reduce the storage\nrequirements of the model to k",
                    " parameters. Recall that k is usually several orders\nof magnitude less than m. Since m and n are usually roughly the same size, k is\npractically insignificant compared to m{\\texttimes}n. Convolution is thus dramatically more\nefficient than dense matrix multiplication in terms of the memory requirements\nand statistical efficiency. For a graphical depiction of how parameter sharing works, see figure 9.5. As an example of both of these first two principles in action, figure 9.6 shows\nhow sparse connectivity and parameter sharing can dramatically improve the\nefficiency of a linear function for detecting edges in an image.\nIn the case of convolution, the particular form of parameter sharing causes the\nlayer to have a property called equivariance to translation. To say a function is\nequivariant means that if the input changes, the output changes in the same way. Specifically, a function f(x) is equivariant to a function g if f(g(x)) = g(f (x)). In the case of convolution, if we let g be any function that translates the input,\ni.e., shifts it, then the convolution function is equivariant to g. For example, let I\nbe a function giving image brightness at integer coordinates. Let g be a function\n338\nCHAPTER 9. CONVOLUTIONAL NETWORKS\nmapping one image function to another image function, such that I\n = g(I ) is\nthe image function with I\n(x, y) = I(x {-} 1, y). This shifts every pixel of I one\nunit to the right. If we apply this transformation to I, then apply convolution,\nthe result will be the same as if we applied convolution to I\n, then applied the\ntransformation g to the output. When processing time series data, this means\nthat convolution produces a sort of timeline that shows when different features\nappear in the input. If we move an event later in time in the input, the exact\nsame representation of it will appear in the output, just later in time. Similarly\nwith images, convolution creates a 2-D map of where certain features appear in\nthe input. If we move the object in the input, its representation will move the\nsame amount in the output. This is useful for when we know that some function\nof a small number of neighboring pixels is useful when applied to multiple input\nlocations. For example, when processing images, it is useful to detect edges",
                    " in\nthe first layer of a convolutional network. The same edges appear more or less\neverywhere in the image, so it is practical to share parameters across the entire\nimage. In some cases, we may not wish to share parameters across the entire\nimage. For example, if we are processing images that are cropped to be centered\non an individual`s face, we probably want to extract different features at different\nlocations{\\textemdash}the part of the network processing the top of the face needs to look for\neyebrows, while the part of the network processing the bottom of the face needs to\nlook for a chin.\nConvolution is not naturally equivariant to some other transformations, such\nas changes in the scale or rotation of an image. Other mechanisms are necessary\nfor handling these kinds of transformations. Finally, some kinds of data cannot be processed by neural networks defined by\nmatrix multiplication with a fixed-shape matrix. Convolution enables processing\nof some of these kinds of data."
                ]
            },
            {
                "14": [
                    "Chapter 14\nAutoencoders\nAn autoencoder is a neural network that is trained to attempt to copy its input\nto its output. Internally, it has a hidden layer h that describes a code used to\nrepresent the input. The network may be viewed as consisting of two parts: an\nencoder function h = f(x) and a decoder that produces a reconstruction r = g(h). This architecture is presented in figure 14.1. If an autoencoder succeeds in simply\nlearning to set g(f(x)) = x everywhere, then it is not especially useful. Instead,\nautoencoders are designed to be unable to learn to copy perfectly. Usually they are\nrestricted in ways that allow them to copy only approximately, and to copy only\ninput that resembles the training data. Because the model is forced to prioritize\nwhich aspects of the input should be copied, it often learns useful properties of the\ndata.\nModern autoencoders have generalized the idea of an encoder and a decoder beyond deterministic functions to stochastic mappings pencoder(h | x) and\npdecoder (x | h). The idea of autoencoders has been part of the historical landscape of neural\nnetworks for decades (LeCun, 1987; Bourlard and Kamp, 1988; Hinton and Zemel, 1994). Traditionally, autoencoders were used for dimensionality reduction or\nfeature learning. Recently, theoretical connections between autoencoders and\nlatent variable models have brought autoencoders to the forefront of generative\nmodeling, as we will see in chapter 20. Autoencoders may be thought of as being\na special case of feedforward networks, and may be trained with all of the same\ntechniques, typically minibatch gradient descent following gradients computed\nby back-propagation. Unlike general feedforward networks, autoencoders may\nalso be trained using recirculation (Hinton and McClelland, 1988), a learning\nalgorithm based on comparing the activations of the network on the original input\n502\nCHAPTER 14. AUTOENCODERS\nto the activations on the reconstructed input. Recirculation is regarded as more\nbiologically plausible than back-propagation, but is rarely used for machine learning\napplications.\nx r\nh\nf g\nFigure 14.1: The general structure of an autoencoder, mapping",
                    " an input x to an output\n(called reconstruction) r through an internal representation or code h. The autoencoder\nhas two components: the encoder f (mapping x to h) and the decoder g (mapping h to\nr).\n14.1 Undercomplete Autoencoders\nCopying the input to the output may sound useless, but we are typically not\ninterested in the output of the decoder. Instead, we hope that training the\nautoencoder to perform the input copying task will result in h taking on useful\nproperties. One way to obtain useful features from the autoencoder is to constrain h to\nhave smaller dimension than x. An autoencoder whose code dimension is less\nthan the input dimension is called undercomplete. Learning an undercomplete\nrepresentation forces the autoencoder to capture the most salient features of the\ntraining data.\nThe learning process is described simply as minimizing a loss function\nL(x, g(f(x))) (14.1)\nwhere L is a loss function penalizing g(f(x)) for being dissimilar from x, such as\nthe mean squared error.\nWhen the decoder is linear and L is the mean squared error, an undercomplete\nautoencoder learns to span the same subspace as PCA. In this case, an autoencoder\ntrained to perform the copying task has learned the principal subspace of the\ntraining data as a side-effect.\nAutoencoders with nonlinear encoder functions f and nonlinear decoder functions g can thus learn a more powerful nonlinear generalization of PCA. Unfortu503\nCHAPTER 14. AUTOENCODERS\nnately, if the encoder and decoder are allowed too much capacity, the autoencoder\ncan learn to perform the copying task without extracting useful information about\nthe distribution of the data. Theoretically, one could imagine that an autoencoder\nwith a one-dimensional code but a very powerful nonlinear encoder could learn to\nrepresent each training example x\n(i) with the code i. The decoder could learn to\nmap these integer indices back to the values of specific training examples. This\nspecific scenario does not occur in practice, but it illustrates clearly that an autoencoder trained to perform the copying task can fail to learn anything useful about\nthe dataset if the capacity of the autoencoder is allowed to become too great."
                ]
            }
        ]
    },
    "Stanford CS229": {
        "authors": [
            "Andrew Ng",
            "Tengyu Ma"
        ],
        "year": 2023,
        "chapters": [
            {
                "5": [
                    "Chapter 5\nKernel methods\n5.1 Feature maps\nRecall that in our discussion about linear regression, we considered the prob-\nlem of predicting the price of a house (denoted by y) from the living area of\nthe house (denoted by x), and we fit a linear function of xto the training\ndata. What if the price ycan be more accurately represented as a non-linear\nfunction of x? In this case, we need a more expressive family of models than\nlinear models.\nWe start by considering fitting cubic functions y={\\theta}3x3+{\\theta}2x2+{\\theta}1x+{\\theta}0.\nIt turns out that we can view the cubic function as a linear function over\nthe a different set of feature variables (defined below). Concretely, let the\nfunction{\\varphi}:R{\\textrightarrow}R4be defined as\n{\\varphi}(x) =\n1\nx\nx2\nx3\n{\\in}R4. (5.1)\nLet{\\theta}{\\in}R4be the vector containing {\\theta}0,{\\theta}1,{\\theta}2,{\\theta}3as entries. Then we can\nrewrite the cubic function in xas:\n{\\theta}3x3+{\\theta}2x2+{\\theta}1x+{\\theta}0={\\theta}T{\\varphi}(x)\nThus, a cubic function of the variable xcan be viewed as a linear function\nover the variables {\\varphi}(x). To distinguish between these two sets of variables,\nin the context of kernel methods, we will call the {\\textquotedblleft}original{\\textquotedblright} input value the\ninput attributes of a problem (in this case, x, the living area). When the\n48\n49\noriginal input is mapped to some new set of quantities {\\varphi}(x), we will call those\nnew quantities the features variables. (Unfortunately, different authors use\ndifferent terms to describe these two things in different contexts.) We will\ncall{\\varphi}afeature map , which maps the attributes to the features.\n5.2 LMS (least mean squares) with features\nWe will derive the gradient descent algorithm for fitting the model {\\theta}T{\\varphi}(x).",
                    "\nFirst recall that for ordinary least square problem where we were to fit {\\theta}Tx,\nthe batch gradient descent update is (see the first lecture note for its deriva-\ntion):\n{\\theta}:={\\theta}+{\\alpha}n{\\sum}\ni=1(\ny(i){-}h{\\theta}(x(i)))\nx(i)\n:={\\theta}+{\\alpha}n{\\sum}\ni=1(\ny(i){-}{\\theta}Tx(i))\nx(i). (5.2)\nLet{\\varphi}:Rd{\\textrightarrow}Rpbe a feature map that maps attribute x(inRd) to the\nfeatures{\\varphi}(x) inRp. (In the motivating example in the previous subsection,\nwe haved= 1 andp= 4.) Now our goal is to fit the function {\\theta}T{\\varphi}(x), with\n{\\theta}being a vector in Rpinstead of Rd. We can replace all the occurrences of\nx(i)in the algorithm above by {\\varphi}(x(i)) to obtain the new update:\n{\\theta}:={\\theta}+{\\alpha}n{\\sum}\ni=1(\ny(i){-}{\\theta}T{\\varphi}(x(i)))\n{\\varphi}(x(i)) (5.3)\nSimilarly, the corresponding stochastic gradient descent update rule is\n{\\theta}:={\\theta}+{\\alpha}(\ny(i){-}{\\theta}T{\\varphi}(x(i)))\n{\\varphi}(x(i)) (5.4)\n5.3 LMS with the kernel trick\nThe gradient descent update, or stochastic gradient update above becomes\ncomputationally expensive when the features {\\varphi}(x) is high-dimensional. For\nexample, consider the direct extension of the feature map in equation (5.1)\nto high-dimensional input x: supposex{\\in}Rd, and let{\\varphi}(x) be the vector that\n50\ncontains all the monomials of xwith degree{\\leq}3\n{\\varphi}(x) =\n1\nx1\nx2\n...\nx2\n1",
                    "\nx1x2\nx1x3\n...\nx2x1\n...\nx3\n1\nx2\n1x2\n...\n. (5.5)\nThe dimension of the features {\\varphi}(x) is on the order of d3.1This is a pro-\nhibitively long vector for computational purpose {\\textemdash} when d= 1000, each\nupdate requires at least computing and storing a 10003= 109dimensional\nvector, which is 106times slower than the update rule for for ordinary least\nsquares updates (5.2).\nIt may appear at first that such d3runtime per update and memory usage\nare inevitable, because the vector {\\theta}itself is of dimension p{\\approx}d3, and we may\nneed to update every entry of {\\theta}and store it. However, we will introduce the\nkernel trick with which we will not need to store {\\theta}explicitly, and the runtime\ncan be significantly improved.\nFor simplicity, we assume the initialize the value {\\theta}= 0, and we focus\non the iterative update (5.3). The main observation is that at any time, {\\theta}\ncan be represented as a linear combination of the vectors {\\varphi}(x(1)),...,{\\varphi} (x(n)).\nIndeed, we can show this inductively as follows. At initialization, {\\theta}= 0 ={\\sum}n\ni=10{\\textperiodcentered}{\\varphi}(x(i)). Assume at some point, {\\theta}can be represented as\n{\\theta}=n{\\sum}\ni=1{\\beta}i{\\varphi}(x(i)) (5.6)\n1Here, for simplicity, we include all the monomials with repetitions (so that, e.g., x1x2x3\nandx2x3x1both appear in {\\varphi}(x)). Therefore, there are totally 1 + d+d2+d3entries in\n{\\varphi}(x).\n51\nfor some{\\beta}1,...,{\\beta}n{\\in}R. Then we claim that in the next round, {\\theta}is still a\nlinear combination of {\\varphi}(x(1)),...,{\\varphi} (x(n)) because\n{\\the",
                    "ta}:={\\theta}+{\\alpha}n{\\sum}\ni=1(\ny(i){-}{\\theta}T{\\varphi}(x(i)))\n{\\varphi}(x(i))\n=n{\\sum}\ni=1{\\beta}i{\\varphi}(x(i)) +{\\alpha}n{\\sum}\ni=1(\ny(i){-}{\\theta}T{\\varphi}(x(i)))\n{\\varphi}(x(i))\n=n{\\sum}\ni=1({\\beta}i+{\\alpha}(\ny(i){-}{\\theta}T{\\varphi}(x(i)))\n)\nnew{\\beta}i{\\varphi}(x(i)) (5.7)\nYou may realize that our general strategy is to implicitly represent the p-\ndimensional vector {\\theta}by a set of coefficients {\\beta}1,...,{\\beta}n. Towards doing this,\nwe derive the update rule of the coefficients {\\beta}1,...,{\\beta}n. Using the equation\nabove, we see that the new {\\beta}idepends on the old one via\n{\\beta}i:={\\beta}i+{\\alpha}(\ny(i){-}{\\theta}T{\\varphi}(x(i)))\n(5.8)\nHere we still have the old {\\theta}on the RHS of the equation. Replacing {\\theta}by\n{\\theta}={\\sum}n\nj=1{\\beta}j{\\varphi}(x(j)) gives\n{\\forall}i{\\in}{\\{}1,...,n{\\}},{\\beta}i:={\\beta}i+{\\alpha}(\ny(i){-}n{\\sum}\nj=1{\\beta}j{\\varphi}(x(j))T{\\varphi}(x(i)))\nWe often rewrite {\\varphi}(x(j))T{\\varphi}(x(i)) as{\\langle}{\\varphi}(x(j)),{\\varphi}(x(i)){\\rangle}to emphasize that it`s the\ninner product of the two feature vectors. Viewing {\\beta}i`s as the new representa-\ntion of{\\theta},",
                    " we have successfully translated the batch gradient descent algorithm\ninto an algorithm that updates the value of {\\beta}iteratively. It may appear that\nat every iteration, we still need to compute the values of {\\langle}{\\varphi}(x(j)),{\\varphi}(x(i)){\\rangle}for\nall pairs of i,j, each of which may take roughly O(p) operation. However,\ntwo important properties come to rescue:\n1. We can pre-compute the pairwise inner products {\\langle}{\\varphi}(x(j)),{\\varphi}(x(i)){\\rangle}for all\npairs ofi,jbefore the loop starts.\n2. For the feature map {\\varphi}defined in (5.5) (or many other interesting fea-\nture maps), computing {\\langle}{\\varphi}(x(j)),{\\varphi}(x(i)){\\rangle}can be efficient and does not\n52\nnecessarily require computing {\\varphi}(x(i)) explicitly. This is because:\n{\\langle}{\\varphi}(x),{\\varphi}(z){\\rangle}= 1 +d{\\sum}\ni=1xizi+{\\sum}\ni,j{\\in}{\\{}1,...,d{\\}}xixjzizj+{\\sum}\ni,j,k{\\in}{\\{}1,...,d{\\}}xixjxkzizjzk\n= 1 +d{\\sum}\ni=1xizi+(d{\\sum}\ni=1xizi)2\n+(d{\\sum}\ni=1xizi)3\n= 1 +{\\langle}x,z{\\rangle}+{\\langle}x,z{\\rangle}2+{\\langle}x,z{\\rangle}3(5.9)\nTherefore, to compute {\\langle}{\\varphi}(x),{\\varphi}(z){\\rangle}, we can first compute {\\langle}x,z{\\rangle}with\nO(d) time and then take another constant number of operations to com-\npute 1 +{\\langle}x,z{\\rangle}+{\\langle}x,z{\\rangle}2+{\\langle}x,z{\\",
                    "rangle}3.\nAs you will see, the inner products between the features {\\langle}{\\varphi}(x),{\\varphi}(z){\\rangle}are\nessential here. We define the Kernel corresponding to the feature map {\\varphi}as\na function that maps X{\\texttimes}X{\\textrightarrow} Rsatisfying:2\nK(x,z){\\triangleq}{\\langle}{\\varphi}(x),{\\varphi}(z){\\rangle} (5.10)\nTo wrap up the discussion, we write the down the final algorithm as\nfollows:\n1. Compute all the values K(x(i),x(j)){\\triangleq}{\\langle}{\\varphi}(x(i)),{\\varphi}(x(j)){\\rangle}using equa-\ntion (5.9) for all i,j{\\in}{\\{}1,...,n{\\}}. Set{\\beta}:= 0.\n2.Loop:\n{\\forall}i{\\in}{\\{}1,...,n{\\}},{\\beta}i:={\\beta}i+{\\alpha}(\ny(i){-}n{\\sum}\nj=1{\\beta}jK(x(i),x(j)))\n(5.11)\nOr in vector notation, letting Kbe then{\\texttimes}nmatrix with Kij=\nK(x(i),x(j)), we have\n{\\beta}:={\\beta}+{\\alpha}( y{-}K{\\beta})\nWith the algorithm above, we can update the representation {\\beta}of the\nvector{\\theta}efficiently with O(n) time per update. Finally, we need to show that\n2Recall thatXis the space of the input x. In our running example, X=Rd\n53\nthe knowledge of the representation {\\beta}suffices to compute the prediction\n{\\theta}T{\\varphi}(x). Indeed, we have\n{\\theta}T{\\varphi}(x) =n{\\sum}\ni=1{\\beta}i{\\varphi}(x(i))T{\\varphi}(x) =n{\\sum}\ni=1{\\beta}iK(x(i),x) (5.12)\nYou may realize",
                    " that fundamentally all we need to know about the feature\nmap{\\varphi}({\\textperiodcentered}) is encapsulated in the corresponding kernel function K({\\textperiodcentered},{\\textperiodcentered}). We\nwill expand on this in the next section.\n5.4 Properties of kernels\nIn the last subsection, we started with an explicitly defined feature map {\\varphi},\nwhich induces the kernel function K(x,z){\\triangleq}{\\langle}{\\varphi}(x),{\\varphi}(z){\\rangle}. Then we saw that\nthe kernel function is so intrinsic so that as long as the kernel function is\ndefined, the whole training algorithm can be written entirely in the language\nof the kernel without referring to the feature map {\\varphi}, so can the prediction of\na test example x(equation (5.12).)\nTherefore, it would be tempted to define other kernel function K({\\textperiodcentered},{\\textperiodcentered}) and\nrun the algorithm (5.11). Note that the algorithm (5.11) does not need to\nexplicitly access the feature map {\\varphi}, and therefore we only need to ensure the\nexistence of the feature map {\\varphi}, but do not necessarily need to be able to\nexplicitly write {\\varphi}down.\nWhat kinds of functions K({\\textperiodcentered},{\\textperiodcentered}) can correspond to some feature map {\\varphi}? In\nother words, can we tell if there is some feature mapping {\\varphi}so thatK(x,z) =\n{\\varphi}(x)T{\\varphi}(z) for allx,z?\nIf we can answer this question by giving a precise characterization of valid\nkernel functions, then we can completely change the interface of selecting\nfeature maps {\\varphi}to the interface of selecting kernel function K. Concretely,\nwe can pick a function K, verify that it satisfies the characterization (so\nthat there exists a feature map {\\varphi}thatKcorresponds to), and then we can\nrun update rule (5.11). The benefit here is that we don`t have to be able\nto compute {\\varphi}or write it down analytically, and we only need to know its\nexistence. We will answer this question at the end of this subsection after\nwe go through several concrete examples of kernels.\nSupposex,z{\\in",
                    "}Rd, and let`s first consider the function K({\\textperiodcentered},{\\textperiodcentered}) defined as:\nK(x,z) = (xTz)2.\n54\nWe can also write this as\nK(x,z) =(d{\\sum}\ni=1xizi)(d{\\sum}\nj=1xjzj)\n=d{\\sum}\ni=1d{\\sum}\nj=1xixjzizj\n=d{\\sum}\ni,j=1(xixj)(zizj)\nThus, we see that K(x,z) ={\\langle}{\\varphi}(x),{\\varphi}(z){\\rangle}is the kernel function that corre-\nsponds to the the feature mapping {\\varphi}given (shown here for the case of d= 3)\nby\n{\\varphi}(x) =\nx1x1\nx1x2\nx1x3\nx2x1\nx2x2\nx2x3\nx3x1\nx3x2\nx3x3\n.\nRevisiting the computational efficiency perspective of kernel, note that whereas\ncalculating the high-dimensional {\\varphi}(x) requiresO(d2) time, finding K(x,z)\ntakes onlyO(d) time{\\textemdash}linear in the dimension of the input attributes.\nFor another related example, also consider K({\\textperiodcentered},{\\textperiodcentered}) defined by\nK(x,z) = (xTz+c)2\n=d{\\sum}\ni,j=1(xixj)(zizj) +d{\\sum}\ni=1({\\sqrt{}}\n2cxi)({\\sqrt{}}\n2czi) +c2.\n(Check this yourself.) This function Kis a kernel function that corresponds\n55\nto the feature mapping (again shown for d= 3)\n{\\varphi}(x) =\nx1x1\nx1x2\nx1x3\nx2x1\nx2x2\nx2x3\nx3x1\nx3x2\nx3x3{\\sqrt{}}\n2cx1{\\sqrt{}}\n2cx2{\\sqrt",
                    "{}}\n2cx3\nc\n,\nand the parameter ccontrols the relative weighting between the xi(first\norder) and the xixj(second order) terms.\nMore broadly, the kernel K(x,z) = (xTz+c)kcorresponds to a feature\nmapping to an(d+k\nk)\nfeature space, corresponding of all monomials of the\nformxi1xi2...xikthat are up to order k. However, despite working in this\nO(dk)-dimensional space, computing K(x,z) still takes only O(d) time, and\nhence we never need to explicitly represent feature vectors in this very high\ndimensional feature space.\nKernels as similarity metrics. Now, let`s talk about a slightly different\nview of kernels. Intuitively, (and there are things wrong with this intuition,\nbut nevermind), if {\\varphi}(x) and{\\varphi}(z) are close together, then we might expect\nK(x,z) ={\\varphi}(x)T{\\varphi}(z) to be large. Conversely, if {\\varphi}(x) and{\\varphi}(z) are far apart{\\textemdash}\nsay nearly orthogonal to each other{\\textemdash}then K(x,z) ={\\varphi}(x)T{\\varphi}(z) will be small.\nSo, we can think of K(x,z) as some measurement of how similar are {\\varphi}(x)\nand{\\varphi}(z), or of how similar are xandz.\nGiven this intuition, suppose that for some learning problem that you`re\nworking on, you`ve come up with some function K(x,z) that you think might\nbe a reasonable measure of how similar xandzare. For instance, perhaps\nyou chose\nK(x,z) = exp(\n{-}||x{-}z||2\n2{\\sigma}2)\n.\nThis is a reasonable measure of xandz`s similarity, and is close to 1 when\nxandzare close, and near 0 when xandzare far apart. Does there exist\n56\na feature map {\\varphi}such that the kernel Kdefined above satisfies K(x,z) =\n{\\",
                    "varphi}(x)T{\\varphi}(z)? In this particular example, the answer is yes. This kernel is called\ntheGaussian kernel , and corresponds to an infinite dimensional feature\nmapping{\\varphi}. We will give a precise characterization about what properties\na functionKneeds to satisfy so that it can be a valid kernel function that\ncorresponds to some feature map {\\varphi}.\nNecessary conditions for valid kernels. Suppose for now that Kis\nindeed a valid kernel corresponding to some feature mapping {\\varphi}, and we will\nfirst see what properties it satisfies. Now, consider some finite set of npoints\n(not necessarily the training set) {\\{}x(1),...,x(n){\\}}, and let a square, n-by-n\nmatrixKbe defined so that its ( i,j)-entry is given by Kij=K(x(i),x(j)).\nThis matrix is called the kernel matrix . Note that we`ve overloaded the\nnotation and used Kto denote both the kernel function K(x,z) and the\nkernel matrix K, due to their obvious close relationship.\nNow, ifKis a valid kernel, then Kij=K(x(i),x(j)) ={\\varphi}(x(i))T{\\varphi}(x(j)) =\n{\\varphi}(x(j))T{\\varphi}(x(i)) =K(x(j),x(i)) =Kji, and hence Kmust be symmetric. More-\nover, letting {\\varphi}k(x) denote the k-th coordinate of the vector {\\varphi}(x), we find that\nfor any vector z, we have\nzTKz ={\\sum}\ni{\\sum}\njziKijzj\n={\\sum}\ni{\\sum}\njzi{\\varphi}(x(i))T{\\varphi}(x(j))zj\n={\\sum}\ni{\\sum}\njzi{\\sum}\nk{\\varphi}k(x(i)){\\varphi}k(x(j))zj\n={\\sum}\nk{\\sum}\ni{\\sum}\njzi{\\varphi}k(x(i)){\\varphi}k(x(j))zj\n={\\sum}\nk({\\",
                    "sum}\nizi{\\varphi}k(x(i)))2\n{\\geq}0.\nThe second-to-last step uses the fact that{\\sum}\ni,jaiaj= ({\\sum}\niai)2forai=\nzi{\\varphi}k(x(i)). Sincezwas arbitrary, this shows that Kis positive semi-definite\n(K{\\geq}0).\nHence, we`ve shown that if Kis a valid kernel (i.e., if it corresponds to\nsome feature mapping {\\varphi}), then the corresponding kernel matrix K{\\in}Rn{\\texttimes}n\nis symmetric positive semidefinite.\n57\nSufficient conditions for valid kernels. More generally, the condition\nabove turns out to be not only a necessary, but also a sufficient, condition\nforKto be a valid kernel (also called a Mercer kernel). The following result\nis due to Mercer.3\nTheorem (Mercer). LetK:Rd{\\texttimes}Rd{\\mapsto}{\\textrightarrow}Rbe given. Then for K\nto be a valid (Mercer) kernel, it is necessary and sufficient that for any\n{\\{}x(1),...,x(n){\\}}, (n{<}{\\infty}), the corresponding kernel matrix is symmetric pos-\nitive semi-definite.\nGiven a function K, apart from trying to find a feature mapping {\\varphi}that\ncorresponds to it, this theorem therefore gives another way of testing if it is\na valid kernel. You`ll also have a chance to play with these ideas more in\nproblem set 2.\nIn class, we also brie{fl}y talked about a couple of other examples of ker-\nnels. For instance, consider the digit recognition problem, in which given\nan image (16x16 pixels) of a handwritten digit (0-9), we have to figure out\nwhich digit it was. Using either a simple polynomial kernel K(x,z) = (xTz)k\nor the Gaussian kernel, SVMs were able to obtain extremely good perfor-\nmance on this problem. This was particularly surprising since the input\nattributesxwere just 256-dimensional vectors of the image pixel intensity\nvalues, and the system had no prior knowledge about vision, or even about\nwhich pixels are adjacent to",
                    " which other ones. Another example that we\nbrie{fl}y talked about in lecture was that if the objects xthat we are trying\nto classify are strings (say, xis a list of amino acids, which strung together\nform a protein), then it seems hard to construct a reasonable, {\\textquotedblleft}small{\\textquotedblright} set of\nfeatures for most learning algorithms, especially if different strings have dif-\nferent lengths. However, consider letting {\\varphi}(x) be a feature vector that counts\nthe number of occurrences of each length- ksubstring in x. If we`re consid-\nering strings of English letters, then there are 26ksuch strings. Hence, {\\varphi}(x)\nis a 26kdimensional vector; even for moderate values of k, this is probably\ntoo big for us to efficiently work with. (e.g., 264{\\approx}460000.) However, using\n(dynamic programming-ish) string matching algorithms, it is possible to ef-\nficiently compute K(x,z) ={\\varphi}(x)T{\\varphi}(z), so that we can now implicitly work\nin this 26k-dimensional feature space, but without ever explicitly computing\nfeature vectors in this space.\n3Many texts present Mercer`s theorem in a slightly more complicated form involving\nL2functions, but when the input attributes take values in Rd, the version given here is\nequivalent.\n58\nApplication of kernel methods: We`ve seen the application of kernels\nto linear regression. In the next part, we will introduce the support vector\nmachines to which kernels can be directly applied. dwell too much longer on\nit here. In fact, the idea of kernels has significantly broader applicability than\nlinear regression and SVMs. Specifically, if you have any learning algorithm\nthat you can write in terms of only inner products {\\langle}x,z{\\rangle}between input\nattribute vectors, then by replacing this with K(x,z) whereKis a kernel,\nyou can {\\textquotedblleft}magically{\\textquotedblright} allow your algorithm to work efficiently in the high\ndimensional feature space corresponding to K. For instance, this kernel trick\ncan be applied with the perceptron to derive a kernel perceptron algorithm.\nMany of the algorithms that we`ll see later in this class",
                    " will also be amenable\nto this method, which has come to be known as the {\\textquotedblleft}kernel trick.{\\textquotedblright}"
                ]
            },
            {
                "6": [
                    "Chapter 6\nSupport vector machines\nThis set of notes presents the Support Vector Machine (SVM) learning al-\ngorithm. SVMs are among the best (and many believe are indeed the best)\n{\\textquotedblleft}off-the-shelf{\\textquotedblright} supervised learning algorithms. To tell the SVM story, we`ll\nneed to first talk about margins and the idea of separating data with a large\n{\\textquotedblleft}gap.{\\textquotedblright} Next, we`ll talk about the optimal margin classifier, which will lead\nus into a digression on Lagrange duality. We`ll also see kernels, which give\na way to apply SVMs efficiently in very high dimensional (such as infinite-\ndimensional) feature spaces, and finally, we`ll close off the story with the\nSMO algorithm, which gives an efficient implementation of SVMs.\n6.1 Margins: intuition\nWe`ll start our story on SVMs by talking about margins. This section will\ngive the intuitions about margins and about the {\\textquotedblleft}confidence{\\textquotedblright} of our predic-\ntions; these ideas will be made formal in Section 6.3.\nConsider logistic regression, where the probability p(y= 1|x;{\\theta}) is mod-\neled byh{\\theta}(x) =g({\\theta}Tx). We then predict {\\textquotedblleft}1{\\textquotedblright} on an input xif and only if\nh{\\theta}(x){\\geq}0.5, or equivalently, if and only if {\\theta}Tx{\\geq}0. Consider a positive\ntraining example ( y= 1). The larger {\\theta}Txis, the larger also is h{\\theta}(x) =p(y=\n1|x;{\\theta}), and thus also the higher our degree of {\\textquotedblleft}confidence{\\textquotedblright} that the label is 1.\nThus, informally we can think of our prediction as being very confident that\ny= 1 if{\\theta}Tx{\\gg}0. Similarly, we think of logistic regression as confidently\npredictingy= 0, if{\\theta}Tx{\\ll}0. Given a training set, again informally it seems\nthat we`",
                    "d have found a good fit to the training data if we can find {\\theta}so that\n{\\theta}Tx(i){\\gg}0 whenever y(i)= 1, and{\\theta}Tx(i){\\ll}0 whenever y(i)= 0, since this\nwould re{fl}ect a very confident (and correct) set of classifications for all the\n59\n60\ntraining examples. This seems to be a nice goal to aim for, and we`ll soon\nformalize this idea using the notion of functional margins.\nFor a different type of intuition, consider the following figure, in which x`s\nrepresent positive training examples, o`s denote negative training examples,\na decision boundary (this is the line given by the equation {\\theta}Tx= 0, and\nis also called the separating hyperplane ) is also shown, and three points\nhave also been labeled A, B and C.\n/0 /1\n/0 /1\n/0 /1BA\nC\nNotice that the point A is very far from the decision boundary. If we are\nasked to make a prediction for the value of yat A, it seems we should be\nquite confident that y= 1 there. Conversely, the point C is very close to\nthe decision boundary, and while it`s on the side of the decision boundary\non which we would predict y= 1, it seems likely that just a small change to\nthe decision boundary could easily have caused out prediction to be y= 0.\nHence, we`re much more confident about our prediction at A than at C. The\npoint B lies in-between these two cases, and more broadly, we see that if\na point is far from the separating hyperplane, then we may be significantly\nmore confident in our predictions. Again, informally we think it would be\nnice if, given a training set, we manage to find a decision boundary that\nallows us to make all correct and confident (meaning far from the decision\nboundary) predictions on the training examples. We`ll formalize this later\nusing the notion of geometric margins.\n61\n6.2 Notation (option reading)\nTo make our discussion of SVMs easier, we`ll first need to introduce a new\nnotation for talking about classification. We will be considering a linear\nclassifier for a binary classification problem with labels yand features x.\nFrom now, we`ll use",
                    " y{\\in}{\\{}{-} 1,1{\\}}(instead of{\\{}0,1{\\}}) to denote the class labels.\nAlso, rather than parameterizing our linear classifier with the vector {\\theta}, we\nwill use parameters w,b, and write our classifier as\nhw,b(x) =g(wTx+b).\nHere,g(z) = 1 ifz{\\geq}0, andg(z) ={-}1 otherwise. This {\\textquotedblleft} w,b{\\textquotedblright} notation\nallows us to explicitly treat the intercept term bseparately from the other\nparameters. (We also drop the convention we had previously of letting x0= 1\nbe an extra coordinate in the input feature vector.) Thus, btakes the role of\nwhat was previously {\\theta}0, andwtakes the role of [ {\\theta}1...{\\theta}d]T.\nNote also that, from our definition of gabove, our classifier will directly\npredict either 1 or {-}1 (cf. the perceptron algorithm), without first going\nthrough the intermediate step of estimating p(y= 1) (which is what logistic\nregression does).\n6.3 Functional and geometric margins (op-\ntion reading)\nLet`s formalize the notions of the functional and geometric margins. Given a\ntraining example ( x(i),y(i)), we define the functional margin of (w,b) with\nrespect to the training example as\n{\\textasciicircum}{\\gamma}(i)=y(i)(wTx(i)+b).\nNote that if y(i)= 1, then for the functional margin to be large (i.e., for\nour prediction to be confident and correct), we need wTx(i)+bto be a large\npositive number. Conversely, if y(i)={-}1, then for the functional margin\nto be large, we need wTx(i)+bto be a large negative number. Moreover, if\ny(i)(wTx(i)+b){>}0, then our prediction on this example is correct. (Check\nthis yourself.) Hence, a large functional margin represents a confident and a\ncorrect prediction.\nFor a linear classifier with the choice of ggiven above (taking values in\n{\\{}{-",
                    "}1,1{\\}}), there`s one property of the functional margin that makes it not a\nvery good measure of confidence, however. Given our choice of g, we note that\n62\nif we replace wwith 2wandbwith 2b, then since g(wTx+b) =g(2wTx+2b),\nthis would not change hw,b(x) at all. I.e., g, and hence also hw,b(x), depends\nonly on the sign, but not on the magnitude, of wTx+b. However, replacing\n(w,b) with (2w,2b) also results in multiplying our functional margin by a\nfactor of 2. Thus, it seems that by exploiting our freedom to scale wandb,\nwe can make the functional margin arbitrarily large without really changing\nanything meaningful. Intuitively, it might therefore make sense to impose\nsome sort of normalization condition such as that ||w||2= 1; i.e., we might\nreplace (w,b) with (w/||w||2,b/||w||2), and instead consider the functional\nmargin of ( w/||w||2,b/||w||2). We`ll come back to this later.\nGiven a training set S={\\{}(x(i),y(i));i= 1,...,n{\\}}, we also define the\nfunction margin of ( w,b) with respect to Sas the smallest of the functional\nmargins of the individual training examples. Denoted by {\\textasciicircum} {\\gamma}, this can therefore\nbe written:\n{\\textasciicircum}{\\gamma}= min\ni=1,...,n{\\textasciicircum}{\\gamma}(i).\nNext, let`s talk about geometric margins . Consider the picture below:\nw A\n{\\gamma}\nB(i)\nThe decision boundary corresponding to ( w,b) is shown, along with the\nvectorw. Note that wis orthogonal (at 90{\\textopenbullet}) to the separating hyperplane.\n(You should convince yourself that this must be the case.) Consider the\npoint at A, which represents the input x(i)of some training example with\nlabely(i)= 1. Its distance to the decision boundary, {\\gamma}(i), is",
                    " given by the line\nsegment AB.\nHow can we find the value of {\\gamma}(i)? Well,w/||w||is a unit-length vector\npointing in the same direction as w. SinceArepresentsx(i), we therefore\n63\nfind that the point Bis given by x(i){-}{\\gamma}(i){\\textperiodcentered}w/||w||. But this point lies on\nthe decision boundary, and all points xon the decision boundary satisfy the\nequationwTx+b= 0. Hence,\nwT(\nx(i){-}{\\gamma}(i)w\n||w||)\n+b= 0.\nSolving for {\\gamma}(i)yields\n{\\gamma}(i)=wTx(i)+b\n||w||=(w\n||w||)T\nx(i)+b\n||w||.\nThis was worked out for the case of a positive training example at A in the\nfigure, where being on the {\\textquotedblleft}positive{\\textquotedblright} side of the decision boundary is good.\nMore generally, we define the geometric margin of ( w,b) with respect to a\ntraining example ( x(i),y(i)) to be\n{\\gamma}(i)=y(i)((w\n||w||)T\nx(i)+b\n||w||)\n.\nNote that if||w||= 1, then the functional margin equals the geometric\nmargin{\\textemdash}this thus gives us a way of relating these two different notions of\nmargin. Also, the geometric margin is invariant to rescaling of the parame-\nters; i.e., if we replace wwith 2wandbwith 2b, then the geometric margin\ndoes not change. This will in fact come in handy later. Specifically, because\nof this invariance to the scaling of the parameters, when trying to fit wandb\nto training data, we can impose an arbitrary scaling constraint on wwithout\nchanging anything important; for instance, we can demand that ||w||= 1, or\n|w1|= 5, or|w1+b|+|w2|= 2, and any of these can be satisfied simply by\nrescalingwandb.\nFinally, given a training set S={\\{}(x(",
                    "i),y(i));i= 1,...,n{\\}}, we also define\nthe geometric margin of ( w,b) with respect to Sto be the smallest of the\ngeometric margins on the individual training examples:\n{\\gamma}= min\ni=1,...,n{\\gamma}(i).\n6.4 The optimal margin classifier (option read-\ning)\nGiven a training set, it seems from our previous discussion that a natural\ndesideratum is to try to find a decision boundary that maximizes the (ge-\nometric) margin, since this would re{fl}ect a very confident set of predictions\n64\non the training set and a good {\\textquotedblleft}fit{\\textquotedblright} to the training data. Specifically, this\nwill result in a classifier that separates the positive and the negative training\nexamples with a {\\textquotedblleft}gap{\\textquotedblright} (geometric margin).\nFor now, we will assume that we are given a training set that is linearly\nseparable; i.e., that it is possible to separate the positive and negative ex-\namples using some separating hyperplane. How will we find the one that\nachieves the maximum geometric margin? We can pose the following opti-\nmization problem:\nmax{\\gamma},w,b{\\gamma}\ns.t.y(i)(wTx(i)+b){\\geq}{\\gamma}, i = 1,...,n\n||w||= 1.\nI.e., we want to maximize {\\gamma}, subject to each training example having func-\ntional margin at least {\\gamma}. The||w||= 1 constraint moreover ensures that the\nfunctional margin equals to the geometric margin, so we are also guaranteed\nthat all the geometric margins are at least {\\gamma}. Thus, solving this problem will\nresult in (w,b) with the largest possible geometric margin with respect to the\ntraining set.\nIf we could solve the optimization problem above, we`d be done. But the\n{\\textquotedblleft}||w||= 1{\\textquotedblright} constraint is a nasty (non-convex) one, and this problem certainly\nisn`t in any format that we can plug into standard optimization software to\nsolve. So, let`s try transforming the problem into a nicer one.",
                    " Consider:\nmax {\\textasciicircum}{\\gamma},w,b{\\textasciicircum}{\\gamma}\n||w||\ns.t.y(i)(wTx(i)+b){\\geq}{\\textasciicircum}{\\gamma}, i = 1,...,n\nHere, we`re going to maximize {\\textasciicircum} {\\gamma}/||w||, subject to the functional margins all\nbeing at least {\\textasciicircum} {\\gamma}. Since the geometric and functional margins are related by\n{\\gamma}= {\\textasciicircum}{\\gamma}/||w|, this will give us the answer we want. Moreover, we`ve gotten rid\nof the constraint||w||= 1 that we didn`t like. The downside is that we now\nhave a nasty (again, non-convex) objective{\\textasciicircum}{\\gamma}\n||w||function; and, we still don`t\nhave any off-the-shelf software that can solve this form of an optimization\nproblem.\nLet`s keep going. Recall our earlier discussion that we can add an arbi-\ntrary scaling constraint on wandbwithout changing anything. This is the\nkey idea we`ll use now. We will introduce the scaling constraint that the\nfunctional margin of w,bwith respect to the training set must be 1:\n{\\textasciicircum}{\\gamma}= 1.\n65\nSince multiplying wandbby some constant results in the functional margin\nbeing multiplied by that same constant, this is indeed a scaling constraint,\nand can be satisfied by rescaling w,b. Plugging this into our problem above,\nand noting that maximizing {\\textasciicircum} {\\gamma}/||w||= 1/||w||is the same thing as minimizing\n||w||2, we now have the following optimization problem:\nminw,b1\n2||w||2\ns.t.y(i)(wTx(i)+b){\\geq}1, i= 1,...,n\nWe`ve now transformed the problem into a form that can be efficiently\nsolved. The above is an optimization problem with a convex quadratic ob-\njective and only linear constraints. Its solution gives us the optimal mar",
                    "-\ngin classifier . This optimization problem can be solved using commercial\nquadratic programming (QP) code.1\nWhile we could call the problem solved here, what we will instead do is\nmake a digression to talk about Lagrange duality. This will lead us to our\noptimization problem`s dual form, which will play a key role in allowing us to\nuse kernels to get optimal margin classifiers to work efficiently in very high\ndimensional spaces. The dual form will also allow us to derive an efficient\nalgorithm for solving the above optimization problem that will typically do\nmuch better than generic QP software.\n6.5 Lagrange duality (optional reading)\nLet`s temporarily put aside SVMs and maximum margin classifiers, and talk\nabout solving constrained optimization problems.\nConsider a problem of the following form:\nminwf(w)\ns.t.hi(w) = 0, i= 1,...,l.\nSome of you may recall how the method of Lagrange multipliers can be used\nto solve it. (Don`t worry if you haven`t seen it before.) In this method, we\ndefine the Lagrangian to be\nL(w,{\\beta}) =f(w) +l{\\sum}\ni=1{\\beta}ihi(w)\n1You may be familiar with linear programming, which solves optimization problems\nthat have linear objectives and linear constraints. QP software is also widely available,\nwhich allows convex quadratic objectives and linear constraints.\n66\nHere, the{\\beta}i`s are called the Lagrange multipliers . We would then find\nand setL`s partial derivatives to zero:\n{\\partial}L\n{\\partial}wi= 0;{\\partial}L\n{\\partial}{\\beta}i= 0,\nand solve for wand{\\beta}.\nIn this section, we will generalize this to constrained optimization prob-\nlems in which we may have inequality as well as equality constraints. Due to\ntime constraints, we won`t really be able to do the theory of Lagrange duality\njustice in this class,2but we will give the main ideas and results, which we\nwill then apply to our optimal margin classifier`s optimization problem.\nConsider the following, which we`ll call the primal optimization problem:\nminwf(w)\ns.t.gi(w){\\leq}0, i=",
                    " 1,...,k\nhi(w) = 0, i= 1,...,l.\nTo solve it, we start by defining the generalized Lagrangian\nL(w,{\\alpha},{\\beta} ) =f(w) +k{\\sum}\ni=1{\\alpha}igi(w) +l{\\sum}\ni=1{\\beta}ihi(w).\nHere, the{\\alpha}i`s and{\\beta}i`s are the Lagrange multipliers. Consider the quantity\n{\\theta}P(w) = max\n{\\alpha},{\\beta}:{\\alpha}i{\\geq}0L(w,{\\alpha},{\\beta} ).\nHere, the {\\textquotedblleft}P{\\textquotedblright} subscript stands for {\\textquotedblleft}primal.{\\textquotedblright} Let some wbe given. If w\nviolates any of the primal constraints (i.e., if either gi(w){>}0 orhi(w)= 0\nfor somei), then you should be able to verify that\n{\\theta}P(w) = max\n{\\alpha},{\\beta}:{\\alpha}i{\\geq}0f(w) +k{\\sum}\ni=1{\\alpha}igi(w) +l{\\sum}\ni=1{\\beta}ihi(w) (6.1)\n={\\infty}. (6.2)\nConversely, if the constraints are indeed satisfied for a particular value of w,\nthen{\\theta}P(w) =f(w). Hence,\n{\\theta}P(w) ={\\{}f(w) ifwsatisfies primal constraints\n{\\infty} otherwise.\n2Readers interested in learning more about this topic are encouraged to read, e.g., R.\nT. Rockarfeller (1970), Convex Analysis, Princeton University Press.\n67\nThus,{\\theta}Ptakes the same value as the objective in our problem for all val-\nues ofwthat satisfies the primal constraints, and is positive infinity if the\nconstraints are violated. Hence, if we consider the minimization problem\nmin\nw{\\theta}P(w) = min\nwmax\n{\\alpha},{\\beta}:{\\alpha}i{\\geq}0L(w,{\\alpha},",
                    "{\\beta} ),\nwe see that it is the same problem (i.e., and has the same solutions as) our\noriginal, primal problem. For later use, we also define the optimal value of\nthe objective to be p{*}= minw{\\theta}P(w); we call this the value of the primal\nproblem.\nNow, let`s look at a slightly different problem. We define\n{\\theta}D({\\alpha},{\\beta}) = min\nwL(w,{\\alpha},{\\beta} ).\nHere, the {\\textquotedblleft}D{\\textquotedblright} subscript stands for {\\textquotedblleft}dual.{\\textquotedblright} Note also that whereas in the\ndefinition of {\\theta}Pwe were optimizing (maximizing) with respect to {\\alpha},{\\beta}, here\nwe are minimizing with respect to w.\nWe can now pose the dual optimization problem:\nmax\n{\\alpha},{\\beta}:{\\alpha}i{\\geq}0{\\theta}D({\\alpha},{\\beta}) = max\n{\\alpha},{\\beta}:{\\alpha}i{\\geq}0min\nwL(w,{\\alpha},{\\beta} ).\nThis is exactly the same as our primal problem shown above, except that the\norder of the {\\textquotedblleft}max{\\textquotedblright} and the {\\textquotedblleft}min{\\textquotedblright} are now exchanged. We also define the\noptimal value of the dual problem`s objective to be d{*}= max{\\alpha},{\\beta}:{\\alpha}i{\\geq}0{\\theta}D(w).\nHow are the primal and the dual problems related? It can easily be shown\nthat\nd{*}= max\n{\\alpha},{\\beta}:{\\alpha}i{\\geq}0min\nwL(w,{\\alpha},{\\beta} ){\\leq}min\nwmax\n{\\alpha},{\\beta}:{\\alpha}i{\\geq}0L(w,{\\alpha},{\\beta} ) =p{*}.\n(You should convince yourself of this; this follows from the {\\textquotedblleft}max min{\\textquotedblright} of a\nfunction always being less than or equal to the {\\textquotedblleft}min max",
                    ".{\\textquotedblright}) However, under\ncertain conditions, we will have\nd{*}=p{*},\nso that we can solve the dual problem in lieu of the primal problem. Let`s\nsee what these conditions are.\nSupposefand thegi`s are convex,3and thehi`s are affine.4Suppose\nfurther that the constraints giare (strictly) feasible; this means that there\nexists some wso thatgi(w){<}0 for alli.\n3Whenfhas a Hessian, then it is convex if and only if the Hessian is positive semi-\ndefinite. For instance, f(w) =wTwis convex; similarly, all linear (and affine) functions\nare also convex. (A function fcan also be convex without being differentiable, but we\nwon`t need those more general definitions of convexity here.)\n4I.e., there exists ai,bi, so thathi(w) =aT\niw+bi. {\\textquotedblleft}Affine{\\textquotedblright} means the same thing as\nlinear, except that we also allow the extra intercept term bi.\n68\nUnder our above assumptions, there must exist w{*},{\\alpha}{*},{\\beta}{*}so thatw{*}is the\nsolution to the primal problem, {\\alpha}{*},{\\beta}{*}are the solution to the dual problem,\nand moreover p{*}=d{*}=L(w{*},{\\alpha}{*},{\\beta}{*}). Moreover, w{*},{\\alpha}{*}and{\\beta}{*}satisfy the\nKarush-Kuhn-Tucker (KKT) conditions , which are as follows:\n{\\partial}\n{\\partial}wiL(w{*},{\\alpha}{*},{\\beta}{*}) = 0, i= 1,...,d (6.3)\n{\\partial}\n{\\partial}{\\beta}iL(w{*},{\\alpha}{*},{\\beta}{*}) = 0, i= 1,...,l (6.4)\n{\\alpha}{*}\nigi(w{*}) = 0, i= 1,...,k (6.5)\ngi(w{*}){\\leq}",
                    "0, i= 1,...,k (6.6)\n{\\alpha}{*}{\\geq}0, i= 1,...,k (6.7)\nMoreover, if some w{*},{\\alpha}{*},{\\beta}{*}satisfy the KKT conditions, then it is also a solution to t he primal and dual\nproblems.\nWe draw attention to Equation (6.5), which is called the KKT dual\ncomplementarity condition. Specifically, it implies that if {\\alpha}{*}\ni{>}0, then\ngi(w{*}) = 0. (I.e., the {\\textquotedblleft} gi(w){\\leq}0{\\textquotedblright} constraint is active , meaning it holds with\nequality rather than with inequality.) Later on, this will be key for showing\nthat the SVM has only a small number of {\\textquotedblleft}support vectors{\\textquotedblright}; the KKT dual\ncomplementarity condition will also give us our convergence test when we\ntalk about the SMO algorithm.\n6.6 Optimal margin classifiers: the dual form\n(option reading)\nNote: The equivalence of optimization problem (6.8) and the optimization\nproblem (6.12) , and the relationship between the primary and dual variables\nin equation (6.10) are the most important take home messages of this section.\nPreviously, we posed the following (primal) optimization problem for find-\ning the optimal margin classifier:\nminw,b1\n2||w||2(6.8)\ns.t.y(i)(wTx(i)+b){\\geq}1, i= 1,...,n\nWe can write the constraints as\ngi(w) ={-}y(i)(wTx(i)+b) + 1{\\leq}0.\n69\nWe have one such constraint for each training example. Note that from the\nKKT dual complementarity condition, we will have {\\alpha}i{>}0 only for the train-\ning examples that have functional margin exactly equal to one (i.e., the ones\ncorresponding to constraints that hold with equality, gi(w) = 0). Consider\nthe figure below, in which a maximum margin separating hyperplane is shown\nby the solid line.\nThe points with the smallest",
                    " margins are exactly the ones closest to the\ndecision boundary; here, these are the three points (one negative and two pos-\nitive examples) that lie on the dashed lines parallel to the decision boundary.\nThus, only three of the {\\alpha}i`s{\\textemdash}namely, the ones corresponding to these three\ntraining examples{\\textemdash}will be non-zero at the optimal solution to our optimiza-\ntion problem. These three points are called the support vectors in this\nproblem. The fact that the number of support vectors can be much smaller\nthan the size the training set will be useful later.\nLet`s move on. Looking ahead, as we develop the dual form of the prob-\nlem, one key idea to watch out for is that we`ll try to write our algorithm\nin terms of only the inner product {\\langle}x(i),x(j){\\rangle}(think of this as ( x(i))Tx(j))\nbetween points in the input feature space. The fact that we can express our\nalgorithm in terms of these inner products will be key when we apply the\nkernel trick.\nWhen we construct the Lagrangian for our optimization problem we have:\nL(w,b,{\\alpha} ) =1\n2||w||2{-}n{\\sum}\ni=1{\\alpha}i[\ny(i)(wTx(i)+b){-}1]\n. (6.9)\nNote that there`re only {\\textquotedblleft} {\\alpha}i{\\textquotedblright} but no {\\textquotedblleft} {\\beta}i{\\textquotedblright} Lagrange multipliers, since the\nproblem has only inequality constraints.\n70\nLet`s find the dual form of the problem. To do so, we need to first\nminimizeL(w,b,{\\alpha} ) with respect to wandb(for fixed{\\alpha}), to get{\\theta}D, which\nwe`ll do by setting the derivatives of Lwith respect to wandbto zero. We\nhave:\n{\\nabla}wL(w,b,{\\alpha} ) =w{-}n{\\sum}\ni=1{\\alpha}iy(i)x(i)= 0\nThis implies that\nw=n{\\sum}\ni=1{\\alpha}iy(i",
                    ")x(i). (6.10)\nAs for the derivative with respect to b, we obtain\n{\\partial}\n{\\partial}bL(w,b,{\\alpha} ) =n{\\sum}\ni=1{\\alpha}iy(i)= 0. (6.11)\nIf we take the definition of win Equation (6.10) and plug that back into\nthe Lagrangian (Equation 6.9), and simplify, we get\nL(w,b,{\\alpha} ) =n{\\sum}\ni=1{\\alpha}i{-}1\n2n{\\sum}\ni,j=1y(i)y(j){\\alpha}i{\\alpha}j(x(i))Tx(j){-}bn{\\sum}\ni=1{\\alpha}iy(i).\nBut from Equation (6.11), the last term must be zero, so we obtain\nL(w,b,{\\alpha} ) =n{\\sum}\ni=1{\\alpha}i{-}1\n2n{\\sum}\ni,j=1y(i)y(j){\\alpha}i{\\alpha}j(x(i))Tx(j).\nRecall that we got to the equation above by minimizing Lwith respect to\nwandb. Putting this together with the constraints {\\alpha}i{\\geq}0 (that we always\nhad) and the constraint (6.11), we obtain the following dual optimization\nproblem:\nmax{\\alpha}W({\\alpha}) =n{\\sum}\ni=1{\\alpha}i{-}1\n2n{\\sum}\ni,j=1y(i)y(j){\\alpha}i{\\alpha}j{\\langle}x(i),x(j){\\rangle}. (6.12)\ns.t.{\\alpha}i{\\geq}0, i= 1,...,n\nn{\\sum}\ni=1{\\alpha}iy(i)= 0,\nYou should also be able to verify that the conditions required for p{*}=d{*}\nand the KKT conditions (Equations 6.3{\\textendash}6.7) to hold are indeed satisfied in\n71\nour optimization problem. Hence, we can solve the dual in lieu of solving\nthe primal problem. Specifically, in the dual problem",
                    " above, we have a\nmaximization problem in which the parameters are the {\\alpha}i`s. We`ll talk later\nabout the specific algorithm that we`re going to use to solve the dual problem,\nbut if we are indeed able to solve it (i.e., find the {\\alpha}`s that maximize W({\\alpha})\nsubject to the constraints), then we can use Equation (6.10) to go back and\nfind the optimal w`s as a function of the {\\alpha}`s. Having found w{*}, by considering\nthe primal problem, it is also straightforward to find the optimal value for\nthe intercept term bas\nb{*}={-}maxi:y(i)={-}1w{*}Tx(i)+ mini:y(i)=1w{*}Tx(i)\n2. (6.13)\n(Check for yourself that this is correct.)\nBefore moving on, let`s also take a more careful look at Equation (6.10),\nwhich gives the optimal value of win terms of (the optimal value of) {\\alpha}.\nSuppose we`ve fit our model`s parameters to a training set, and now wish to\nmake a prediction at a new point input x. We would then calculate wTx+b,\nand predict y= 1 if and only if this quantity is bigger than zero. But\nusing (6.10), this quantity can also be written:\nwTx+b=(n{\\sum}\ni=1{\\alpha}iy(i)x(i))T\nx+b (6.14)\n=n{\\sum}\ni=1{\\alpha}iy(i){\\langle}x(i),x{\\rangle}+b. (6.15)\nHence, if we`ve found the {\\alpha}i`s, in order to make a prediction, we have to\ncalculate a quantity that depends only on the inner product between xand\nthe points in the training set. Moreover, we saw earlier that the {\\alpha}i`s will all\nbe zero except for the support vectors. Thus, many of the terms in the sum\nabove will be zero, and we really need to find only the inner products between\nxand the support vectors (of which there is often only a small number) in\norder calculate (6.15) and make our prediction.",
                    "\nBy examining the dual form of the optimization problem, we gained sig-\nnificant insight into the structure of the problem, and were also able to write\nthe entire algorithm in terms of only inner products between input feature\nvectors. In the next section, we will exploit this property to apply the ker-\nnels to our classification problem. The resulting algorithm, support vector\nmachines , will be able to efficiently learn in very high dimensional spaces.\n72\n6.7 Regularization and the non-separable case\n(optional reading)\nThe derivation of the SVM as presented so far assumed that the data is\nlinearly separable. While mapping data to a high dimensional feature space\nvia{\\varphi}does generally increase the likelihood that the data is separable, we\ncan`t guarantee that it always will be so. Also, in some cases it is not clear\nthat finding a separating hyperplane is exactly what we`d want to do, since\nthat might be susceptible to outliers. For instance, the left figure below\nshows an optimal margin classifier, and when a single outlier is added in the\nupper-left region (right figure), it causes the decision boundary to make a\ndramatic swing, and the resulting classifier has a much smaller margin.\nTo make the algorithm work for non-linearly separable datasets as well\nas be less sensitive to outliers, we reformulate our optimization (using {\\ell}1\nregularization ) as follows:\nmin{\\gamma},w,b1\n2||w||2+Cn{\\sum}\ni=1{\\xi}i\ns.t.y(i)(wTx(i)+b){\\geq}1{-}{\\xi}i, i= 1,...,n\n{\\xi}i{\\geq}0, i= 1,...,n.\nThus, examples are now permitted to have (functional) margin less than 1,\nand if an example has functional margin 1 {-}{\\xi}i(with{\\xi} {>}0), we would pay\na cost of the objective function being increased by C{\\xi}i. The parameter C\ncontrols the relative weighting between the twin goals of making the ||w||2\nsmall (which we saw earlier makes the margin large) and of ensuring that\nmost examples have functional margin at least 1.\n73\nAs before, we can form the Lagrangian:",
                    "\nL(w,b,{\\xi},{\\alpha},r ) =1\n2wTw+Cn{\\sum}\ni=1{\\xi}i{-}n{\\sum}\ni=1{\\alpha}i[\ny(i)(xTw+b){-}1 +{\\xi}i]\n{-}n{\\sum}\ni=1ri{\\xi}i.\nHere, the{\\alpha}i`s andri`s are our Lagrange multipliers (constrained to be {\\geq}0).\nWe won`t go through the derivation of the dual again in detail, but after\nsetting the derivatives with respect to wandbto zero as before, substituting\nthem back in, and simplifying, we obtain the following dual form of the\nproblem:\nmax{\\alpha}W({\\alpha}) =n{\\sum}\ni=1{\\alpha}i{-}1\n2n{\\sum}\ni,j=1y(i)y(j){\\alpha}i{\\alpha}j{\\langle}x(i),x(j){\\rangle}\ns.t. 0{\\leq}{\\alpha}i{\\leq}C, i = 1,...,n\nn{\\sum}\ni=1{\\alpha}iy(i)= 0,\nAs before, we also have that wcan be expressed in terms of the {\\alpha}i`s as\ngiven in Equation (6.10), so that after solving the dual problem, we can con-\ntinue to use Equation (6.15) to make our predictions. Note that, somewhat\nsurprisingly, in adding {\\ell}1regularization, the only change to the dual prob-\nlem is that what was originally a constraint that 0 {\\leq}{\\alpha}ihas now become\n0{\\leq}{\\alpha}i{\\leq}C. The calculation for b{*}also has to be modified (Equation 6.13 is\nno longer valid); see the comments in the next section/Platt`s paper.\nAlso, the KKT dual-complementarity conditions (which in the next sec-\ntion will be useful for testing for the convergence of the SMO algorithm)\nare:\n{\\alpha}i= 0{\\Rightarrow}y(i)(wTx(i)+b){\\geq}1 (6.16",
                    ")\n{\\alpha}i=C{\\Rightarrow}y(i)(wTx(i)+b){\\leq}1 (6.17)\n0{<}{\\alpha}i{<}C{\\Rightarrow}y(i)(wTx(i)+b) = 1. (6.18)\nNow, all that remains is to give an algorithm for actually solving the dual\nproblem, which we will do in the next section.\n6.8 The SMO algorithm (optional reading)\nThe SMO (sequential minimal optimization) algorithm, due to John Platt,\ngives an efficient way of solving the dual problem arising from the derivation\n74\nof the SVM. Partly to motivate the SMO algorithm, and partly because it`s\ninteresting in its own right, let`s first take another digression to talk about\nthe coordinate ascent algorithm.\n6.8.1 Coordinate ascent\nConsider trying to solve the unconstrained optimization problem\nmax\n{\\alpha}W({\\alpha}1,{\\alpha}2,...,{\\alpha}n).\nHere, we think of Was just some function of the parameters {\\alpha}i`s, and for now\nignore any relationship between this problem and SVMs. We`ve already seen\ntwo optimization algorithms, gradient ascent and Newton`s method. The\nnew algorithm we`re going to consider here is called coordinate ascent :\nLoop until convergence: {\\{}\nFori= 1,...,n ,{\\{}\n{\\alpha}i:= arg max {\\textasciicircum}{\\alpha}iW({\\alpha}1,...,{\\alpha}i{-}1,{\\textasciicircum}{\\alpha}i,{\\alpha}i+1,...,{\\alpha}n).\n{\\}}\n{\\}}\nThus, in the innermost loop of this algorithm, we will hold all the variables\nexcept for some {\\alpha}ifixed, and reoptimize Wwith respect to just the parameter\n{\\alpha}i. In the version of this method presented here, the inner-loop reoptimizes\nthe variables in order {\\alpha}1,{\\alpha}2,...,{\\alpha}n,{\\alpha}1,{\\alpha}2,.... (A more sophisticated version\nmight choose other orderings; for instance, we may choose the next variable\nto update according to which one we expect to allow us to make the",
                    " largest\nincrease in W({\\alpha}).)\nWhen the function Whappens to be of such a form that the {\\textquotedblleft}arg max{\\textquotedblright}\nin the inner loop can be performed efficiently, then coordinate ascent can be\na fairly efficient algorithm. Here`s a picture of coordinate ascent in action:\n75\n{-}2 {-}1.5 {-}1 {-}0.5 0 0.5 1 1.5 2 2.5{-}2{-}1.5{-}1{-}0.500.511.522.5\nThe ellipses in the figure are the contours of a quadratic function that\nwe want to optimize. Coordinate ascent was initialized at (2 ,{-}2), and also\nplotted in the figure is the path that it took on its way to the global maximum.\nNotice that on each step, coordinate ascent takes a step that`s parallel to one\nof the axes, since only one variable is being optimized at a time.\n6.8.2 SMO\nWe close off the discussion of SVMs by sketching the derivation of the SMO\nalgorithm.\nHere`s the (dual) optimization problem that we want to solve:\nmax{\\alpha}W({\\alpha}) =n{\\sum}\ni=1{\\alpha}i{-}1\n2n{\\sum}\ni,j=1y(i)y(j){\\alpha}i{\\alpha}j{\\langle}x(i),x(j){\\rangle}. (6.19)\ns.t. 0{\\leq}{\\alpha}i{\\leq}C, i = 1,...,n (6.20)\nn{\\sum}\ni=1{\\alpha}iy(i)= 0. (6.21)\nLet`s say we have set of {\\alpha}i`s that satisfy the constraints (6.20-6.21). Now,\nsuppose we want to hold {\\alpha}2,...,{\\alpha}nfixed, and take a coordinate ascent step\nand reoptimize the objective with respect to {\\alpha}1. Can we make any progress?\nThe answer is no, because the constraint (6.21) ensures that\n{\\alpha}1y(1)={-}n{\\sum}\ni=2",
                    "{\\alpha}iy(i).\n76\nOr, by multiplying both sides by y(1), we equivalently have\n{\\alpha}1={-}y(1)n{\\sum}\ni=2{\\alpha}iy(i).\n(This step used the fact that y(1){\\in}{\\{}{-} 1,1{\\}}, and hence ( y(1))2= 1.) Hence,\n{\\alpha}1is exactly determined by the other {\\alpha}i`s, and if we were to hold {\\alpha}2,...,{\\alpha}n\nfixed, then we can`t make any change to {\\alpha}1without violating the con-\nstraint (6.21) in the optimization problem.\nThus, if we want to update some subject of the {\\alpha}i`s, we must update at\nleast two of them simultaneously in order to keep satisfying the constraints.\nThis motivates the SMO algorithm, which simply does the following:\nRepeat till convergence {\\{}\n1. Select some pair {\\alpha}iand{\\alpha}jto update next (using a heuristic that\ntries to pick the two that will allow us to make the biggest progress\ntowards the global maximum).\n2. Reoptimize W({\\alpha}) with respect to {\\alpha}iand{\\alpha}j, while holding all the\nother{\\alpha}k`s (k=i,j) fixed.\n{\\}}\nTo test for convergence of this algorithm, we can check whether the KKT\nconditions (Equations 6.16-6.18) are satisfied to within some tol. Here, tolis\nthe convergence tolerance parameter, and is typically set to around 0.01 to\n0.001. (See the paper and pseudocode for details.)\nThe key reason that SMO is an efficient algorithm is that the update to\n{\\alpha}i,{\\alpha}jcan be computed very efficiently. Let`s now brie{fl}y sketch the main\nideas for deriving the efficient update.\nLet`s say we currently have some setting of the {\\alpha}i`s that satisfy the con-\nstraints (6.20-6.21), and suppose we`ve decided to hold {\\alpha}3,...,{\\alpha}nfixed, and\nwant to reoptimize W({\\alpha}1,{\\alpha}2,...,{\\alpha",
                    "}n) with respect to {\\alpha}1and{\\alpha}2(subject to\nthe constraints). From (6.21), we require that\n{\\alpha}1y(1)+{\\alpha}2y(2)={-}n{\\sum}\ni=3{\\alpha}iy(i).\nSince the right hand side is fixed (as we`ve fixed {\\alpha}3,...{\\alpha}n), we can just let\nit be denoted by some constant {\\zeta}:\n{\\alpha}1y(1)+{\\alpha}2y(2)={\\zeta}. (6.22)\nWe can thus picture the constraints on {\\alpha}1and{\\alpha}2as follows:\n77\n{\\alpha}2\n{\\alpha}1{\\alpha}1 {\\alpha}2\nCC\n(1)+(2)y y={\\zeta}H\nL\nFrom the constraints (6.20), we know that {\\alpha}1and{\\alpha}2must lie within the box\n[0,C]{\\texttimes}[0,C] shown. Also plotted is the line {\\alpha}1y(1)+{\\alpha}2y(2)={\\zeta}, on which we\nknow{\\alpha}1and{\\alpha}2must lie. Note also that, from these constraints, we know\nL{\\leq}{\\alpha}2{\\leq}H; otherwise, ( {\\alpha}1,{\\alpha}2) can`t simultaneously satisfy both the box\nand the straight line constraint. In this example, L= 0. But depending on\nwhat the line {\\alpha}1y(1)+{\\alpha}2y(2)={\\zeta}looks like, this won`t always necessarily be\nthe case; but more generally, there will be some lower-bound Land some\nupper-bound Hon the permissible values for {\\alpha}2that will ensure that {\\alpha}1,{\\alpha}2\nlie within the box [0 ,C]{\\texttimes}[0,C].\nUsing Equation (6.22), we can also write {\\alpha}1as a function of {\\alpha}2:\n{\\alpha}1= ({\\zeta}{-}{\\alpha}2y(2))y(1).\n(Check this derivation yourself; we again used the fact that y(1){\\in}{\\{}{-} 1,1{\\}}so",
                    "\nthat (y(1))2= 1.) Hence, the objective W({\\alpha}) can be written\nW({\\alpha}1,{\\alpha}2,...,{\\alpha}n) =W(({\\zeta}{-}{\\alpha}2y(2))y(1),{\\alpha}2,...,{\\alpha}n).\nTreating{\\alpha}3,...,{\\alpha}nas constants, you should be able to verify that this is\njust some quadratic function in {\\alpha}2. I.e., this can also be expressed in the\nforma{\\alpha}2\n2+b{\\alpha}2+cfor some appropriate a,b, andc. If we ignore the {\\textquotedblleft}box{\\textquotedblright}\nconstraints (6.20) (or, equivalently, that L{\\leq}{\\alpha}2{\\leq}H), then we can easily\nmaximize this quadratic function by setting its derivative to zero and solving.\nWe`ll let{\\alpha}new,unclipped\n2 denote the resulting value of {\\alpha}2. You should also be\nable to convince yourself that if we had instead wanted to maximize Wwith\nrespect to{\\alpha}2but subject to the box constraint, then we can find the resulting\nvalue optimal simply by taking {\\alpha}new,unclipped\n2 and {\\textquotedblleft}clipping{\\textquotedblright} it to lie in the\n78\n[L,H] interval, to get\n{\\alpha}new\n2 =\n\nH if{\\alpha}new,unclipped\n2 {>}H\n{\\alpha}new,unclipped\n2 ifL{\\leq}{\\alpha}new,unclipped\n2{\\leq}H\nL if{\\alpha}new,unclipped\n2 {<}L\nFinally, having found the {\\alpha}new\n2, we can use Equation (6.22) to go back and\nfind the optimal value of {\\alpha}new\n1.\nThere`re a couple more details that are quite easy but that we`ll leave you\nto read about yourself in Platt`s paper: One is the choice of the heuristics\nused to select the next {\\alpha}i,{\\alpha}jto update; the other is how to update bas the\nSMO algorithm is",
                    " run."
                ]
            },
            {
                "7": [
                    "Chapter 7\nDeep learning\nWe now begin our study of deep learning. In this set of notes, we give an\noverview of neural networks, discuss vectorization and discuss training neural\nnetworks with backpropagation.\n7.1 Supervised learning with non-linear mod-\nels\nIn the supervised learning setting (predicting yfrom the input x), suppose\nour model/hypothesis is h{\\theta}(x). In the past lectures, we have considered the\ncases when h{\\theta}(x) ={\\theta}{\\top}x(in linear regression) or h{\\theta}(x) ={\\theta}{\\top}{\\varphi}(x) (where{\\varphi}(x)\nis the feature map). A commonality of these two models is that they are\nlinear in the parameters {\\theta}. Next we will consider learning general family of\nmodels that are non-linear in both the parameters {\\theta}and the inputs x. The\nmost common non-linear models are neural networks, which we will define\nstaring from the next section. For this section, it suffices to think h{\\theta}(x) as\nan abstract non-linear model.1\nSuppose{\\{}(x(i),y(i)){\\}}n\ni=1are the training examples. We will define the\nnonlinear model and the loss/cost function for learning it.\nRegression problems. For simplicity, we start with the case where the\noutput is a real number, that is, y(i){\\in}R, and thus the model h{\\theta}also outputs\na real number h{\\theta}(x){\\in}R. We define the least square cost function for the\n1If a concrete example is helpful, perhaps think about the model h{\\theta}(x) ={\\theta}2\n1x2\n1+{\\theta}2\n2x2\n2+\n{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\theta}2\ndx2\ndin this subsection, even though it`s not a neural network.\n80\n81\ni-th example ( x(i),y(i)) as\nJ(i)({\\theta}) =1\n2(h{\\theta}(x(i)){-}y(i))2",
                    ", (7.1)\nand define the mean-square cost function for the dataset as\nJ({\\theta}) =1\nnn{\\sum}\ni=1J(i)({\\theta}), (7.2)\nwhich is same as in linear regression except that we introduce a constant\n1/nin front of the cost function to be consistent with the convention. Note\nthat multiplying the cost function with a scalar will not change the local\nminima or global minima of the cost function. Also note that the underlying\nparameterization for h{\\theta}(x) is different from the case of linear regression,\neven though the form of the cost function is the same mean-squared loss.\nThroughout the notes, we use the words {\\textquotedblleft}loss{\\textquotedblright} and {\\textquotedblleft}cost{\\textquotedblright} interchangeably.\nBinary classification. Next we define the model and loss function for\nbinary classification. Suppose the inputs x{\\in}Rd. Let {\\textasciimacron}h{\\theta}:Rd{\\textrightarrow}Rbe a\nparameterized model (the analog of {\\theta}{\\top}xin logistic linear regression). We\ncall the output {\\textasciimacron}h{\\theta}(x){\\in}Rthe logit. Analogous to Section 2.1, we use the\nlogistic function g({\\textperiodcentered}) to turn the logit {\\textasciimacron}h{\\theta}(x) to a probability h{\\theta}(x){\\in}[0,1]:\nh{\\theta}(x) =g({\\textasciimacron}h{\\theta}(x)) = 1/(1 + exp({-}{\\textasciimacron}h{\\theta}(x)). (7.3)\nWe model the conditional distribution of ygivenxand{\\theta}by\nP(y= 1|x;{\\theta}) =h{\\theta}(x)\nP(y= 0|x;{\\theta}) = 1{-}h{\\theta}(x)\nFollowing the same derivation in Section 2.1 and using the derivation in\nRemark 2.1.1, the negative likelihood loss function is equal to:\n",
                    "J(i)({\\theta}) ={-}logp(y(i)|x(i);{\\theta}) ={\\ell}logistic ({\\textasciimacron}h{\\theta}(x(i)),y(i)) (7.4)\nAs done in equation (7.2), the total loss function is also defined as the average\nof the loss function over individual training examples, J({\\theta}) =1\nn{\\sum}n\ni=1J(i)({\\theta}).\n82\nMulti-class classification. Following Section 2.3, we consider a classifica-\ntion problem where the response variable ycan take on any one of kvalues,\ni.e.y{\\in}{\\{}1,2,...,k{\\}}. Let {\\textasciimacron}h{\\theta}:Rd{\\textrightarrow}Rkbe a parameterized model. We\ncall the outputs {\\textasciimacron}h{\\theta}(x){\\in}Rkthe logits. Each logit corresponds to the predic-\ntion for one of the kclasses. Analogous to Section 2.3, we use the softmax\nfunction to turn the logits {\\textasciimacron}h{\\theta}(x) into a probability vector with non-negative\nentries that sum up to 1:\nP(y=j|x;{\\theta}) =exp({\\textasciimacron}h{\\theta}(x)j)\n{\\sum}k\ns=1exp({\\textasciimacron}h{\\theta}(x)s), (7.5)\nwhere {\\textasciimacron}h{\\theta}(x)sdenotes the s-th coordinate of {\\textasciimacron}h{\\theta}(x).\nSimilarly to Section 2.3, the loss function for a single training example\n(x(i),y(i)) is its negative log-likelihood:\nJ(i)({\\theta}) ={-}logp(y(i)|x(i);{\\theta}) ={-}log(\nexp({\\textasciimacron}h{\\theta}(x(i))y(i))\n{\\sum}k\ns=1exp({",
                    "\\textasciimacron}h{\\theta}(x(i))s))\n. (7.6)\nUsing the notations of Section 2.3, we can simply write in an abstract way:\nJ(i)({\\theta}) ={\\ell}ce({\\textasciimacron}h{\\theta}(x(i)),y(i)). (7.7)\nThe loss function is also defined as the average of the loss function of indi-\nvidual training examples, J({\\theta}) =1\nn{\\sum}n\ni=1J(i)({\\theta}).\nWe also note that the approach above can also be generated to any con-\nditional probabilistic model where we have an exponential distribution for\ny, Exponential-family( y;{\\eta}), where{\\eta}={\\textasciimacron}h{\\theta}(x) is a parameterized nonlinear\nfunction of x. However, the most widely used situations are the three cases\ndiscussed above.\nOptimizers (SGD). Commonly, people use gradient descent (GD), stochas-\ntic gradient (SGD), or their variants to optimize the loss function J({\\theta}). GD`s\nupdate rule can be written as2\n{\\theta}:={\\theta}{-}{\\alpha}{\\nabla}{\\theta}J({\\theta}) (7.8)\nwhere{\\alpha} {>} 0 is often referred to as the learning rate or step size. Next, we\nintroduce a version of the SGD (Algorithm 1), which is lightly different from\nthat in the first lecture notes.\n2Recall that, as defined in the previous lecture notes, we use the notation {\\textquotedblleft} a:=b{\\textquotedblright} to\ndenote an operation (in a computer program) in which we setthe value of a variable ato\nbe equal to the value of b. In other words, this operation overwrites awith the value of\nb. In contrast, we will write {\\textquotedblleft} a=b{\\textquotedblright} when we are asserting a statement of fact, that the\nvalue ofais equal to the value of b.\n83\nAlgorithm 1 Stochastic Gradient Descent\n1:Hyperparameter",
                    ": learning rate {\\alpha}, number of total iteration niter.\n2:Initialize{\\theta}randomly.\n3:fori= 1 toniterdo\n4: Samplejuniformly from{\\{}1,...,n{\\}}, and update {\\theta}by\n{\\theta}:={\\theta}{-}{\\alpha}{\\nabla}{\\theta}J(j)({\\theta}) (7.9)\nOftentimes computing the gradient of Bexamples simultaneously for the\nparameter {\\theta}can be faster than computing Bgradients separately due to\nhardware parallelization. Therefore, a mini-batch version of SGD is most\ncommonly used in deep learning, as shown in Algorithm 2. There are also\nother variants of the SGD or mini-batch SGD with slightly different sampling\nschemes.\nAlgorithm 2 Mini-batch Stochastic Gradient Descent\n1:Hyperparameters: learning rate {\\alpha}, batch size B, {\\#} iterations niter.\n2:Initialize{\\theta}randomly\n3:fori= 1 toniterdo\n4: SampleBexamplesj1,...,jB(without replacement) uniformly from\n{\\{}1,...,n{\\}}, and update {\\theta}by\n{\\theta}:={\\theta}{-}{\\alpha}\nBB{\\sum}\nk=1{\\nabla}{\\theta}J(jk)({\\theta}) (7.10)\nWith these generic algorithms, a typical deep learning model is learned\nwith the following steps. 1. Define a neural network parametrization h{\\theta}(x),\nwhich we will introduce in Section 7.2, and 2. write the backpropagation\nalgorithm to compute the gradient of the loss function J(j)({\\theta}) efficiently,\nwhich will be covered in Section 7.4, and 3. run SGD or mini-batch SGD (or\nother gradient-based optimizers) with the loss function J({\\theta}).\n84\n7.2 Neural networks\nNeural networks refer to a broad type of non-linear models/parametrizations\n{\\textasciimacron}h{\\theta}(x) that involve combinations of matrix multiplications and other entry-\nwise non-linear operations. To have a unified treatment for regression",
                    " prob-\nlem and classification problem, here we consider {\\textasciimacron}h{\\theta}(x) as the output of the\nneural network. For regression problem, the final prediction h{\\theta}(x) ={\\textasciimacron}h{\\theta}(x),\nand for classification problem, {\\textasciimacron}h{\\theta}(x) is the logits and the predicted probability\nwill beh{\\theta}(x) = 1/(1+exp({-}{\\textasciimacron}h{\\theta}(x)) (see equation 7.3) for binary classification\norh{\\theta}(x) = softmax( {\\textasciimacron}h{\\theta}(x)) for multi-class classification (see equation 7.5).\nWe will start small and slowly build up a neural network, step by step.\nA Neural Network with a Single Neuron. Recall the housing price\nprediction problem from before: given the size of the house, we want to\npredict the price. We will use it as a running example in this subsection.\nPreviously, we fit a straight line to the graph of size vs. housing price.\nNow, instead of fitting a straight line, we wish to prevent negative housing\nprices by setting the absolute minimum price as zero. This produces a {\\textquotedblleft}kink{\\textquotedblright}\nin the graph as shown in Figure 7.1. How do we represent such a function\nwith a single kink as {\\textasciimacron}h{\\theta}(x) with unknown parameter? (After doing so, we\ncan invoke the machinery in Section 7.1.)\nWe define a parameterized function {\\textasciimacron}h{\\theta}(x) with input x, parameterized by\n{\\theta}, which outputs the price of the house y. Formally, {\\textasciimacron}h{\\theta}:x{\\textrightarrow}y. Perhaps\none of the simplest parametrization would be\n{\\textasciimacron}h{\\theta}(x) = max(wx+b,0),where{\\theta}= (w,b){\\in}R2(7.11)\nHere {\\textasciimacron}h{\\theta",
                    "}(x) returns a single value: ( wx+b) or zero, whichever is greater. In\nthe context of neural networks, the function max {\\{}t,0{\\}}is called a ReLU (pro-\nnounced {\\textquotedblleft}ray-lu{\\textquotedblright}), or rectified linear unit, and often denoted by ReLU( t){\\triangleq}\nmax{\\{}t,0{\\}}.\nGenerally, a one-dimensional non-linear function that maps RtoRsuch as\nReLU is often referred to as an activation function . The model {\\textasciimacron}h{\\theta}(x) is said\nto have a single neuron partly because it has a single non-linear activation\nfunction. (We will discuss more about why a non-linear activation is called\nneuron.)\nWhen the input x{\\in}Rdhas multiple dimensions, a neural network with\na single neuron can be written as\n{\\textasciimacron}h{\\theta}(x) = ReLU(w{\\top}x+b),wherew{\\in}Rd,b{\\in}R, and{\\theta}= (w,b) (7.12)\n85\n500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing prices \nsquare feet price (in {\\$}1000) \nFigure 7.1: Housing prices with a {\\textquotedblleft}kink{\\textquotedblright} in the graph.\nThe termbis often referred to as the {\\textquotedblleft}bias{\\textquotedblright}, and the vector wis referred\nto as the weight vector. Such a neural network has 1 layer. (We will define\nwhat multiple layers mean in the sequel.)\nStacking Neurons. A more complex neural network may take the single\nneuron described above and {\\textquotedblleft}stack{\\textquotedblright} them together such that one neuron\npasses its output as input into the next neuron, resulting in a more complex\nfunction.\nLet us now deepen the housing prediction example. In addition to the size\nof the house, suppose that you know the number of bedrooms, the zip code\nand the wealth of the neighborhood. Building neural networks is analogous\nto Lego bricks: you take",
                    " individual bricks and stack them together to build\ncomplex structures. The same applies to neural networks: we take individual\nneurons and stack them together to create complex neural networks.\nGiven these features (size, number of bedrooms, zip code, and wealth),\nwe might then decide that the price of the house depends on the maximum\nfamily size it can accommodate. Suppose the family size is a function of the\nsize of the house and number of bedrooms (see Figure 7.2). The zip code\nmay provide additional information such as how walkable the neighborhood\nis (i.e., can you walk to the grocery store or do you need to drive everywhere).\nCombining the zip code with the wealth of the neighborhood may predict\nthe quality of the local elementary school. Given these three derived features\n(family size, walkable, school quality), we may conclude that the price of the\n86\nhome ultimately depends on these three features.\nFamily Size \nSchool Quality Walkable Size \n{\\#} Bedrooms \nZip Code \nWealth Price \ny\nFigure 7.2: Diagram of a small neural network for predicting housing prices.\nFormally, the input to a neural network is a set of input features\nx1,x2,x3,x4. We denote the intermediate variables for {\\textquotedblleft}family size{\\textquotedblright}, {\\textquotedblleft}walk-\nable{\\textquotedblright}, and {\\textquotedblleft}school quality{\\textquotedblright} by a1,a2,a3(theseai`s are often referred to as\n{\\textquotedblleft}hidden units{\\textquotedblright} or {\\textquotedblleft}hidden neurons{\\textquotedblright}). We represent each of the ai`s as a neu-\nral network with a single neuron with a subset of x1,...,x 4as inputs. Then\nas in Figure 7.1, we will have the parameterization:\na1= ReLU({\\theta}1x1+{\\theta}2x2+{\\theta}3)\na2= ReLU({\\theta}4x3+{\\theta}5)\na3= ReLU({\\theta}6x3+{\\theta}7x4+{\\theta}8)\nwhere ({\\theta}1,{\\",
                    "textperiodcentered}{\\textperiodcentered}{\\textperiodcentered},{\\theta}8) are parameters. Now we represent the final output {\\textasciimacron}h{\\theta}(x)\nas another linear function with a1,a2,a3as inputs, and we get3\n{\\textasciimacron}h{\\theta}(x) ={\\theta}9a1+{\\theta}10a2+{\\theta}11a3+{\\theta}12 (7.13)\nwhere{\\theta}contains all the parameters ( {\\theta}1,{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered},{\\theta}12).\nNow we represent the output as a quite complex function of xwith pa-\nrameters{\\theta}. Then you can use this parametrization {\\textasciimacron}h{\\theta}with the machinery of\nSection 7.1 to learn the parameters {\\theta}.\nInspiration from Biological Neural Networks. As the name suggests,\nartificial neural networks were inspired by biological neural networks. The\nhidden units a1,...,amcorrespond to the neurons in a biological neural net-\nwork, and the parameters {\\theta}i`s correspond to the synapses. However, it`s\nunclear how similar the modern deep artificial neural networks are to the bi-\nological ones. For example, perhaps not many neuroscientists think biological\n3Typically, for multi-layer neural network, at the end, near the output, we don`t apply\nReLU, especially when the output is not necessarily a positive number.\n87\nneural networks could have 1000 layers, while some modern artificial neural\nnetworks do (we will elaborate more on the notion of layers.) Moreover, it`s\nan open question whether human brains update their neural networks in a\nway similar to the way that computer scientists learn artificial neural net-\nworks (using backpropagation, which we will introduce in the next section.).\nTwo-layer Fully-Connected Neural Networks. We constructed the\nneural network in equation (7.13) using a significant amount of prior knowl-\nedge/belief about how the {\\textquotedblleft}family size{\\textquotedblright}, {\\textquotedblleft}walkable{\\textquotedblright}, and {\\textquotedblleft}",
                    "school quality{\\textquotedblright} are\ndetermined by the inputs. We implicitly assumed that we know the family\nsize is an important quantity to look at and that it can be determined by\nonly the {\\textquotedblleft}size{\\textquotedblright} and {\\textquotedblleft}{\\#} bedrooms{\\textquotedblright}. Such a prior knowledge might not be\navailable for other applications. It would be more {fl}exible and general to have\na generic parameterization. A simple way would be to write the intermediate\nvariablea1as a function of all x1,...,x 4:\na1= ReLU(w{\\top}\n1x+b1),wherew1{\\in}R4andb1{\\in}R (7.14)\na2= ReLU(w{\\top}\n2x+b2),wherew2{\\in}R4andb2{\\in}R\na3= ReLU(w{\\top}\n3x+b3),wherew3{\\in}R4andb3{\\in}R\nWe still define {\\textasciimacron}h{\\theta}(x) using equation (7.13) with a1,a2,a3being defined as\nabove. Thus we have a so-called fully-connected neural network because\nall the intermediate variables ai`s depend on all the inputs xi`s.\nFor full generality, a two-layer fully-connected neural network with m\nhidden units and ddimensional input x{\\in}Rdis defined as\n{\\forall}j{\\in}[1,...,m ], zj=w[1]\nj{\\top}x+b[1]\njwherew[1]\nj{\\in}Rd,b[1]\nj{\\in}R (7.15)\naj= ReLU(zj),\na= [a1,...,am]{\\top}{\\in}Rm\n{\\textasciimacron}h{\\theta}(x) =w[2]{\\top}a+b[2]wherew[2]{\\in}Rm,b[2]{\\in}R, (7.16)\nNote that by default the vectors in Rdare viewed as column vectors, and\nin particular ais a column vector with components a1,a",
                    "2,...,am. The indices\n[1]and[2]are used to distinguish two sets of parameters: the w[1]\nj`s (each of\nwhich is a vector in Rd) andw[2](which is a vector in Rm). We will have\nmore of these later.\nVectorization. Before we introduce neural networks with more layers and\nmore complex structures, we will simplify the expressions for neural networks\n88\nwith more matrix and vector notations. Another important motivation of\nvectorization is the speed perspective in the implementation. In order to\nimplement a neural network efficiently, one must be careful when using for\nloops. The most natural way to implement equation (7.15) in code is perhaps\nto use a for loop. In practice, the dimensionalities of the inputs and hidden\nunits are high. As a result, code will run very slowly if you use for loops.\nLeveraging the parallelism in GPUs is/was crucial for the progress of deep\nlearning.\nThis gave rise to vectorization . Instead of using for loops, vectorization\ntakes advantage of matrix algebra and highly optimized numerical linear\nalgebra packages (e.g., BLAS) to make neural network computations run\nquickly. Before the deep learning era, a for loop may have been sufficient\non smaller datasets, but modern deep networks and state-of-the-art datasets\nwill be infeasible to run with for loops.\nWe vectorize the two-layer fully-connected neural network as below. We\ndefine a weight matrix W[1]inRm{\\texttimes}das the concatenation of all the vectors\nw[1]\nj`s in the following way:\nW[1]=\n{\\textemdash}w[1]\n1{\\top}{\\textemdash}\n{\\textemdash}w[1]\n2{\\top}{\\textemdash}\n...\n{\\textemdash}w[1]\nm{\\top}{\\textemdash}\n{\\in}Rm{\\texttimes}d(7.17)\nNow by the definition of matrix vector multiplication, we can write z=\n[z1,...,zm]{\\top}{\\in}Rmas\n\nz1\n...\n...\nzm\n\n\nz{\\in}Rm{\\texttimes}1=\n{\\textemdash}w[1]\n1{\\top}{",
                    "\\textemdash}\n{\\textemdash}w[1]\n2{\\top}{\\textemdash}\n...\n{\\textemdash}w[1]\nm{\\top}{\\textemdash}\n\n\nW[1]{\\in}Rm{\\texttimes}d\nx1\nx2\n...\nxd\n\n\nx{\\in}Rd{\\texttimes}1+\nb[1]\n1\nb[1]\n2...\nb[1]\nm\n\n\nb[1]{\\in}Rm{\\texttimes}1(7.18)\nOr succinctly,\nz=W[1]x+b[1](7.19)\nWe remark again that a vector in Rdin this notes, following the conventions\npreviously established, is automatically viewed as a column vector, and can\n89\nalso be viewed as a d{\\texttimes}1 dimensional matrix. (Note that this is different\nfrom numpy where a vector is viewed as a row vector in broadcasting.)\nComputing the activations a{\\in}Rmfromz{\\in}Rminvolves an element-\nwise non-linear application of the ReLU function, which can be computed in\nparallel efficiently. Overloading ReLU for element-wise application of ReLU\n(meaning, for a vector t{\\in}Rd, ReLU(t) is a vector such that ReLU( t)i=\nReLU(ti)), we have\na= ReLU(z) (7.20)\nDefineW[2]= [w[2]{\\top}]{\\in}R1{\\texttimes}msimilarly. Then, the model in equa-\ntion (7.16) can be summarized as\na= ReLU(W[1]x+b[1])\n{\\textasciimacron}h{\\theta}(x) =W[2]a+b[2](7.21)\nHere{\\theta}consists of W[1],W[2](often referred to as the weight matrices) and\nb[1],b[2](referred to as the biases). The collection of W[1],b[1]is referred to as\nthe first layer, and W[2],b[2]the second layer. The activation ais referred to as\nthe hidden layer. A two-layer neural",
                    " network is also called one-hidden-layer\nneural network.\nMulti-layer fully-connected neural networks. With this succinct no-\ntations, we can stack more layers to get a deeper fully-connected neu-\nral network. Let rbe the number of layers (weight matrices). Let\nW[1],...,W[r],b[1],...,b[r]be the weight matrices and biases of all the layers.\nThen a multi-layer neural network can be written as\na[1]= ReLU(W[1]x+b[1])\na[2]= ReLU(W[2]a[1]+b[2])\n{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}\na[r{-}1]= ReLU(W[r{-}1]a[r{-}2]+b[r{-}1])\n{\\textasciimacron}h{\\theta}(x) =W[r]a[r{-}1]+b[r](7.22)\nWe note that the weight matrices and biases need to have compatible\ndimensions for the equations above to make sense. If a[k]has dimension mk,\nthen the weight matrix W[k]should be of dimension mk{\\texttimes}mk{-}1, and the bias\nb[k]{\\in}Rmk. Moreover, W[1]{\\in}Rm1{\\texttimes}dandW[r]{\\in}R1{\\texttimes}mr{-}1.\n90\nThe total number of neurons in the network is m1+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+mr, and the\ntotal number of parameters in this network is ( d+ 1)m1+ (m1+ 1)m2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+\n(mr{-}1+ 1)mr.\nSometimes for notational consistency we also write a[0]=x, anda[r]=\nh{\\theta}(x). Then we have simple recursion that\na[k]= ReLU(W[k]a[k{-}1]+b[k]),{\\forall}k= 1,...,r{-}1 (7.23)\nNote that this would have",
                    " be true for k=rif there were an additional\nReLU in equation (7.22), but often people like to make the last layer linear\n(aka without a ReLU) so that negative outputs are possible and it`s easier\nto interpret the last layer as a linear model. (More on the interpretability at\nthe {\\textquotedblleft}connection to kernel method{\\textquotedblright} paragraph of this section.)\nOther activation functions. The activation function ReLU can be re-\nplaced by many other non-linear function {\\sigma}({\\textperiodcentered}) that maps RtoRsuch as\n{\\sigma}(z) =1\n1 +e{-}z(sigmoid) (7.24)\n{\\sigma}(z) =ez{-}e{-}z\nez+e{-}z(tanh) (7.25)\n{\\sigma}(z) = max{\\{}z,{\\gamma}z{\\}},{\\gamma}{\\in}(0,1) (leaky ReLU) (7.26)\n{\\sigma}(z) =z\n2[\n1 + erf(z{\\sqrt{}}\n2)]\n(GELU) (7.27)\n{\\sigma}(z) =1\n{\\beta}log(1 + exp( {\\beta}z)),{\\beta} {>} 0 (Softplus) (7.28)\nThe activation functions are plotted in Figure 7.3. Sigmoid and tanh are\nless and less used these days partly because their are bounded from both sides\nand the gradient of them vanishes as zgoes to both positive and negative\ninfinity (whereas all the other activation functions still have gradients as the\ninput goes to positive infinity.) Softplus is not used very often either in\npractice and can be viewed as a smoothing of the ReLU so that it has a\nproper second order derivative. GELU and leaky ReLU are both variants of\nReLU but they have some non-zero gradient even when the input is negative.\nGELU (or its slight variant) is used in NLP models such as BERT and GPT\n(which we will discuss in Chapter 14.)\nWhy do we not use the identity function for {\\sigma}(z)?That is, why\nnot use{\\s",
                    "igma}(z) =z? Assume for sake of argument that b[1]andb[2]are zeros.\n91\nFigure 7.3: Activation functions in deep learning.\nSuppose{\\sigma}(z) =z, then for two-layer neural network, we have that\n{\\textasciimacron}h{\\theta}(x) =W[2]a[1](7.29)\n=W[2]{\\sigma}(z[1]) by definition (7.30)\n=W[2]z[1]since{\\sigma}(z) =z (7.31)\n=W[2]W[1]x from Equation (7.18) (7.32)\n={\\textasciitilde}Wx where {\\textasciitilde}W=W[2]W[1](7.33)\nNotice how W[2]W[1]collapsed into {\\textasciitilde}W.\nThis is because applying a linear function to another linear function will\nresult in a linear function over the original input (i.e., you can construct a {\\textasciitilde}W\nsuch that {\\textasciitilde}Wx=W[2]W[1]x). This loses much of the representational power\nof the neural network as often times the output we are trying to predict\nhas a non-linear relationship with the inputs. Without non-linear activation\nfunctions, the neural network will simply perform linear regression.\nConnection to the Kernel Method. In the previous lectures, we covered\nthe concept of feature maps. Recall that the main motivation for feature\nmaps is to represent functions that are non-linear in the input xby{\\theta}{\\top}{\\varphi}(x),\nwhere{\\theta}are the parameters and {\\varphi}(x), the feature map, is a handcrafted\nfunction non-linear in the raw input x. The performance of the learning\nalgorithms can significantly depends on the choice of the feature map {\\varphi}(x).\nOftentimes people use domain knowledge to design the feature map {\\varphi}(x) that\n92\nsuits the particular applications. The process of choosing the feature maps\nis often referred to as feature engineering .\nWe can view deep learning as a way to automatically learn the right\nfeature map",
                    " (sometimes also referred to as {\\textquotedblleft}the representation{\\textquotedblright}) as follows.\nSuppose we denote by {\\beta}the collection of the parameters in a fully-connected\nneural networks (equation (7.22)) except those in the last layer. Then we\ncan abstract right a[r{-}1]as a function of the input xand the parameters in\n{\\beta}:a[r{-}1]={\\varphi}{\\beta}(x). Now we can write the model as\n{\\textasciimacron}h{\\theta}(x) =W[r]{\\varphi}{\\beta}(x) +b[r](7.34)\nWhen{\\beta}is fixed, then {\\varphi}{\\beta}({\\textperiodcentered}) can viewed as a feature map, and therefore {\\textasciimacron}h{\\theta}(x)\nis just a linear model over the features {\\varphi}{\\beta}(x). However, we will train the\nneural networks, both the parameters in {\\beta}and the parameters W[r],b[r]are\noptimized, and therefore we are not learning a linear model in the feature\nspace, but also learning a good feature map {\\varphi}{\\beta}({\\textperiodcentered}) itself so that it`s possi-\nble to predict accurately with a linear model on top of the feature map.\nTherefore, deep learning tends to depend less on the domain knowledge of\nthe particular applications and requires often less feature engineering. The\npenultimate layer a[r]is often (informally) referred to as the learned features\nor representations in the context of deep learning.\nIn the example of house price prediction, a fully-connected neural network\ndoes not need us to specify the intermediate quantity such {\\textquotedblleft}family size{\\textquotedblright}, and\nmay automatically discover some useful features in the last penultimate layer\n(the activation a[r{-}1]), and use them to linearly predict the housing price.\nOften the feature map / representation obtained from one datasets (that is,\nthe function {\\varphi}{\\beta}({\\textperiodcentered}) can be also useful for other datasets, which indicates they\ncontain essential information about the data. However, oftentimes, the neural\nnetwork will discover complex features which are very",
                    " useful for predicting\nthe output but may be difficult for a human to understand or interpret. This\nis why some people refer to neural networks as a black box , as it can be\ndifficult to understand the features it has discovered.\n7.3 Modules in Modern Neural Networks\nThe multi-layer neural network introduced in equation (7.22) of Section 7.2\nis often called multi-layer perceptron (MLP) these days. Modern neural net-\nworks used in practice are often much more complex and consist of multiple\nbuilding blocks or multiple layers of building blocks. In this section, we will\n93\nintroduce some of the other building blocks and discuss possible ways to\ncombine them.\nFirst, each matrix multiplication can be viewed as a building block. Con-\nsider a matrix multiplication operation with parameters ( W,b) whereWis\nthe weight matrix and bis the bias vector, operating on an input z,\nMMW,b(z) =Wz+b. (7.35)\nNote that we implicitly assume all the dimensions are chosen to be compat-\nible. We will also drop the subscripts under MM when they are clear in the\ncontext or just for convenience when they are not essential to the discussion.\nThen, the MLP can be written as as a composition of multiple matrix\nmultiplication modules and nonlinear activation modules (which can also be\nviewed as a building block):\nMLP(x) = MMW[r],b[r]({\\sigma}(MMW[r{-}1],b[r{-}1]({\\sigma}({\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}MMW[1],b[1](x)))).(7.36)\nAlternatively, when we drop the subscripts that indicate the parameters for\nconvenience, we can write\nMLP(x) = MM({\\sigma}(MM{\\sigma}({\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}MM(x)))). (7.37)\nNote that in this lecture notes, by default, all the modules have different\nsets of parameters, and the dimensions of the parameters are chosen such\nthat the composition is meaningful.\nLarger modules can be defined via smaller modules as well, e.g., one\nactivation layer {\\sigma}and a matrix multiplication layer MM are often combined\nand called",
                    " a {\\textquotedblleft}layer{\\textquotedblright} in many papers. People often draw the architecture\nwith the basic modules in a figure by indicating the dependency between\nthese modules. E.g., see an illustration of an MLP in Figure 7.4, Left.\nResidual connections. One of the very in{fl}uential neural network archi-\ntecture for vision application is ResNet, which uses the residual connections\nthat are essentially used in almost all large-scale deep learning architectures\nthese days. Using our notation above, a very much simplified residual block\ncan be defined as\nRes(z) =z+{\\sigma}(MM({\\sigma}(MM(z)))). (7.38)\nA much simplified ResNet is a composition of many residual blocks followed\nby a matrix multiplication,\nResNet-S(x) = MM(Res(Res( {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}Res(x)))). (7.39)\n94\n{\\mathit{x}}Layer {\\mathit{r}}{-}1Layer {\\mathit{i}}...Layer 1MLP({\\mathit{x}})...Layer{\\mathit{i}}MM![{''}],{\\#}[{''}]MM![{\\$}],{\\#}[{\\$}]\n{\\mathit{x}}ResRes...ResResNet-S({\\mathit{x}})...Res\nMMMM\nFigure 7.4: Illustrative Figures for Architecture. Left: An MLP with r\nlayers. Right : A residual network.\nWe also draw the dependency of these modules in Figure 7.4, Right.\nWe note that the ResNet-S is still not the same as the ResNet architec-\nture introduced in the seminal paper [He et al., 2016] because ResNet uses\nconvolution layers instead of vanilla matrix multiplication, and adds batch\nnormalization between convolutions and activations. We will introduce con-\nvolutional layers and some variants of batch normalization below. ResNet-S\nand layer normalization are part of the Transformer architecture that are\nwidely used in modern large language models.\nLayer normalization. Layer normalization, denoted by LN in this text,\nis a module that maps a vector z{\\in}Rmto a more normalized vector L",
                    "N( z){\\in}\nRm. It is oftentimes used after the nonlinear activations.\nWe first define a sub-module of the layer normalization, denoted by LN-S.\nLN-S(z) =\nz1{-}{\\textasciicircum}{\\textmu}\n{\\textasciicircum}{\\sigma}z2{-}{\\textasciicircum}{\\textmu}\n{\\textasciicircum}{\\sigma}...\nzm{-}{\\textasciicircum}{\\textmu}\n{\\textasciicircum}{\\sigma}\n, (7.40)\nwhere {\\textasciicircum}{\\textmu}={\\sum}m\ni=1zi\nmis the empirical mean of the vector zand {\\textasciicircum}{\\sigma}={\\sqrt{}}{\\sum}m\ni=1(zi{-}{\\textasciicircum}{\\textmu}2)\nm\nis the empirical standard deviation of the entries of z.4Intuitively, LN-S( z)\nis a vector that is normalized to having empirical mean zero and empirical\nstandard deviation 1.\n4Note that we divide by minstead ofm{-}1 in the empirical standard deviation here\nbecause we are interested in making the output of LN-S( z) have sum of squares equal to\n1 (as opposed to estimating the standard deviation in statistics.)\n95\nOftentimes zero mean and standard deviation 1 is not the most desired\nnormalization scheme, and thus layernorm introduces to parameters learnable\nscalars{\\beta}and{\\gamma}as the desired mean and standard deviation, and use an affine\ntransformation to turn the output of LN-S( z) into a vector with mean {\\beta}and\nstandard deviation {\\gamma}.\nLN(z) ={\\beta}+{\\gamma}{\\textperiodcentered}LN-S(z) =\n{\\beta}+{\\gamma}(z1{-}{\\textasciicircum}{\\textmu}\n{\\textasciicircum}{\\sigma})\n{\\beta}+{\\gamma}(z2{-}{\\textasciicircum}{\\textmu}\n{\\text",
                    "asciicircum}{\\sigma})\n...\n{\\beta}+{\\gamma}(zm{-}{\\textasciicircum}{\\textmu}\n{\\textasciicircum}{\\sigma})\n. (7.41)\nHere the first occurrence of {\\beta}should be technically interpreted as a vector\nwith all the entries being {\\beta}. in We also note that {\\textasciicircum} {\\textmu}and {\\textasciicircum}{\\sigma}are also functions\nofzand shouldn`t be treated as constants when computing the derivatives of\nlayernorm. Moreover, {\\beta}and{\\gamma}are learnable parameters and thus layernorm\nis a parameterized module (as opposed to the activation layer which doesn`t\nhave any parameters.)\nScaling-invariant property. One important property of layer normalization\nis that it will make the model invariant to scaling of the parameters in the\nfollowing sense. Suppose we consider composing LN with MM W,band get\na subnetwork LN(MM W,b(z)). Then, we have that the output of this sub-\nnetwork does not change when the parameter in MM W,bis scaled:\nLN(MM {\\alpha}W,{\\alpha}b (z)) = LN(MM W,b(z)),{\\forall}{\\alpha}{>}0. (7.42)\nTo see this, we first know that LN-S( {\\textperiodcentered}) is scale-invariant\nLN-S({\\alpha}z) =\n{\\alpha}z1{-}{\\alpha}{\\textasciicircum}{\\textmu}\n{\\alpha}{\\textasciicircum}{\\sigma}{\\alpha}z2{-}{\\alpha}{\\textasciicircum}{\\textmu}\n{\\alpha}{\\textasciicircum}{\\sigma}...\n{\\alpha}zm{-}{\\alpha}{\\textasciicircum}{\\textmu}\n{\\alpha}{\\textasciicircum}{\\sigma}\n=\nz1{-}{\\textasciicircum}{\\textmu}\n{\\textasciicircum}{\\sigma}z2{-}{\\textasciicircum}{\\textmu}\n",
                    "{\\textasciicircum}{\\sigma}...\nzm{-}{\\textasciicircum}{\\textmu}\n{\\textasciicircum}{\\sigma}\n= LN-S(z). (7.43)\nThen we have\nLN(MM {\\alpha}W,{\\alpha}b (z)) ={\\beta}+{\\gamma}LN-S(MM {\\alpha}W,{\\alpha}b (z)) (7.44)\n={\\beta}+{\\gamma}LN-S({\\alpha}MMW,b(z)) (7.45)\n={\\beta}+{\\gamma}LN-S(MM W,b(z)) (7.46)\n= LN(MM W,b(z)). (7.47)\nDue to this property, most of the modern DL architectures for large-scale\ncomputer vision and language applications have the following scale-invariant\n96\nproperty w.r.t all the weights that are not at the last layer. Suppose the\nnetworkfhas last layer` weights Wlast, and all the rest of the weights are\ndenote byW. Then, we have fWlast,{\\alpha}W(x) =fWlast,W(x) for all{\\alpha}{>}0. Here,\nthe last layers weights are special because there are typically no layernorm\nor batchnorm after the last layer`s weights.\nOther normalization layers. There are several other normalization layers that\naim to normalize the intermediate layers of the neural networks to a more\nfixed and controllable scaling, such as batch-normalization [ ?], and group\nnormalization [ ?]. Batch normalization and group normalization are more\noften used in computer vision applications whereas layer norm is used more\noften in language applications.\nConvolutional Layers. Convolutional Neural Networks are neural net-\nworks that consist of convolution layers (and many other modules), and are\nparticularly useful for computer vision applications. For the simplicity of\nexposition, we focus on 1-D convolution in this text and only brie{fl}y mention\n2-D convolution informally at the end of this subsection. (2-D convolution\nis more suitable for images which have two dimensions. 1-D convolution is\nalso used in natural language processing",
                    ".)\nWe start by introducing a simplified version of the 1-D convolution layer,\ndenoted by Conv1D-S( {\\textperiodcentered}) which is a type of matrix multiplication layer with\na special structure. The parameters of Conv1D-S are a filter vector w{\\in}Rk\nwherekis called the filter size (oftentimes k{\\ll}m), and a bias scalar b.\nOftentimes the filter is also called a kernel (but it does not have much to do\nwith the kernel in kernel method.) For simplicity, we assume k= 2{\\ell}+ 1 is\nan odd number. We first pad zeros to the input vector zin the sense that we\nletz1{-}{\\ell}=z1{-}{\\ell}+1=..=z0= 0 andzm+1=zm+2=..=zm+{\\ell}= 0, and treat\nzas an (m+ 2{\\ell})-dimension vector. Conv1D-S outputs a vector of dimension\nRmwhere each output dimension is a linear combination of subsets of zj`s\nwith coefficients from w,\nConv1D-S(z)i=w1zi{-}{\\ell}+w2zi{-}{\\ell}+1+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+w2{\\ell}+1zi+{\\ell}=2{\\ell}+1{\\sum}\nj=1wjzi{-}{\\ell}+(j{-}1).\n(7.48)\nTherefore, one can view Conv1D-S as a matrix multiplication with shared\n97\nparameters: Conv1D-S( z) =Qz, where\nQ=\nw{\\ell}+1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}w2{\\ell}+1 0 0{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} 0\nw{\\ell}{\\textperiodcentered",
                    "}{\\textperiodcentered}{\\textperiodcentered}w2{\\ell}w2{\\ell}+1 0{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} 0\n...\nw1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}w{\\ell}+1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} w2{\\ell}+1 0{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} 0\n0w1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} w2{\\ell}w2{\\ell}+1 0{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} 0\n...\n...\n0{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} 0w1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} w2{\\ell}+1\n...\n0{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiod",
                    "centered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} {\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} 0w1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}w{\\ell}+1\n.(7.49)\nNote thatQi,j=Qi{-}1,j{-}1for alli,j{\\in}{\\{}2,...,m{\\}}, and thus convoluation is a\nmatrix multiplication with parameter sharing. We also note that computing\nthe convolution only takes O(km) times but computing a generic matrix\nmultiplication takes O(m2) time. Convolution has kparameters but generic\nmatrix multiplication will have m2parameters. Thus convolution is supposed\nto be much more efficient than a generic matrix multiplication (as long as\nthe additional structure imposed does not hurt the {fl}exibility of the model\nto fit the data).\nWe also note that in practice there are many variants of the convolutional\nlayers that we define here, e.g., there are other ways to pad zeros or sometimes\nthe dimension of the output of the convolutional layers could be different from\nthe input. We omit some of this subtleties here for simplicity.\nThe convolutional layers used in practice have also many {\\textquotedblleft}channels{\\textquotedblright} and\nthe simplified version above corresponds to the 1-channel version. Formally,\nConv1D takes in Cvectorsz1,...,zC{\\in}Rmas inputs, where Cis referred\nto as the number of channels. In other words, the more general version,\ndenoted by Conv1D, takes in a matrix as input, which is the concatenation\nofz1,...,zCand has dimension m{\\texttimes}C. It can output C{'}vectors of dimension\nm, denoted by Conv1D( z)1,..., Conv1D(z)C{'}, whereC{'}is referred to as the\noutput channel, or equivalently a matrix of dimension m{\\texttimes}C{'}. Each of the\noutput is a sum of the simplified convolutions applied on various channels.\n{\\forall}i{\\",
                    "in}[C{'}],Conv1D(z)i=C{\\sum}\nj=1Conv1D-S i,j(zj). (7.50)\nNote that each Conv1D-S i,jare modules with different parameters, and\nthus the total number of parameters is k(the number of parameters in a\nConv1D-S){\\texttimes}CC{'}(the number of Conv1D-S i.j`s) =kCC{'}. In contrast, a\ngeneric linear mapping from Rm{\\texttimes}CandRm{\\texttimes}C{'}hasm2CC{'}parameters. The\n98\nparameters can also be represented as a three-dimensional tensor of dimen-\nsionk{\\texttimes}C{\\texttimes}C{'}.\n2-D convolution (brief). A 2-D convolution with one channel, denoted by\nConv2D-S, is analogous to the Conv1D-S, but takes a 2-dimensional input\nz{\\in}Rm{\\texttimes}mand applies a filter of size k{\\texttimes}k, and outputs Conv2D-S( z){\\in}\nRm{\\texttimes}m. The full 2-D convolutional layer, denoted by Conv2D, takes in\na sequence of matrices z1,...,zC{\\in}Rm{\\texttimes}m, or equivalently a 3-D ten-\nsorz= (z1,...,zC){\\in}Rm{\\texttimes}m{\\texttimes}Cand outputs a sequence of matrices,\nConv2D(z)1,..., Conv2D(z)C{'}{\\in}Rm{\\texttimes}m, which can also be viewed as a 3D\ntensor in Rm{\\texttimes}m{\\texttimes}C{'}. Each channel of the output is sum of the outcomes of\napplying Conv2D-S layers on all the input channels.\n{\\forall}i{\\in}[C{'}],Conv2D(z)i=C{\\sum}\nj=1Conv2D-S i,j(zj). (7.51)\nBecause there are CC{'}number of Conv2D-S modules",
                    " and each of the\nConv2D-S module has k2parameters, the total number of parameters is\nCC{'}k2. The parameters can also be viewed as a 4D tensor of dimension\nC{\\texttimes}C{'}{\\texttimes}k{\\texttimes}k.\n7.4 Backpropagation\nIn this section, we introduce backpropgation or auto-differentiation, which\ncomputes the gradient of the loss {\\nabla}J({\\theta}) efficiently. We will start with an\ninformal theorem that states that as long as a real-valued function fcan be\nefficiently computed/evaluated by a differentiable network or circuit, then its\ngradient can be efficiently computed in a similar time. We will then show\nhow to do this concretely for neural networks.\nBecause the formality of the general theorem is not the main focus here,\nwe will introduce the terms with informal definitions. By a differentiable\ncircuit or a differentiable network, we mean a composition of a sequence of\ndifferentiable arithmetic operations (additions, subtraction, multiplication,\ndivisions, etc) and elementary differentiable functions (ReLU, exp, log, sin,\ncos, etc.). Let the size of the circuit be the total number of such operations\nand elementary functions. We assume that each of the operations and func-\ntions, and their derivatives or partial derivatives ecan be computed in O(1)\ntime.\nTheorem 7.4.1: [backpropagation or auto-differentiation, informally stated]\nSuppose a differentiable circuit of size Ncomputes a real-valued function"
                ]
            },
            {
                "8": [
                    "Chapter 8\nGeneralization\nThis chapter discusses tools to analyze and understand the generaliza-\ntion of machine learning models, i.e, their performances on unseen test\nexamples. Recall that for supervised learning problems, given a train-\ning dataset{\\{}(x(i),y(i)){\\}}n\ni=1, we typically learn a model h{\\theta}by minimizing a\nloss/cost function J({\\theta}), which encourages h{\\theta}to fit the data. E.g., when\nthe loss function is the least square loss (aka mean squared error), we have\nJ({\\theta}) =1\nn{\\sum}n\ni=1(y(i){-}h{\\theta}(x(i)))2. This loss function for training purposes is\noftentimes referred to as the training loss/error/cost.\nHowever, minimizing the training loss is not our ultimate goal{\\textemdash}it is\nmerely our approach towards the goal of learning a predictive model. The\nmost important evaluation metric of a model is the loss on unseen test exam-\nples, which is oftentimes referred to as the test error. Formally, we sample a\ntest example ( x,y) from the so-called test distribution D, and measure the\nmodel`s error on it, by, e.g., the mean squared error, ( h{\\theta}(x){-}y)2. The ex-\npected loss/error over the randomness of the test example is called the test\nloss/error,1\nL({\\theta}) =E(x,y){\\sim}D[(y{-}h{\\theta}(x))2] (8.1)\nNote that the measurement of the error involves computing the expectation,\nand in practice, it can be approximated by the average error on many sampled\ntest examples, which are referred to as the test dataset. Note that the key\ndifference here between training and test datasets is that the test examples\n1In theoretical and statistical literature, we oftentimes call the uniform distribution\nover the training set {\\{}(x(i),y(i)){\\}}n\ni=1, denoted by {\\textasciicircum}D, an empirical distribution, and call\nDthe population distribution. Partly because of this, the training loss is also referred\nto as the",
                    " empirical loss/risk/error, and the test loss is also referred to as the population\nloss/risk/error.\n113\n114\nareunseen , in the sense that the training procedure has not used the test\nexamples. In classical statistical learning settings, the training examples are\nalso drawn from the same distribution as the test distribution D, but still\nthe test examples are unseen by the learning procedure whereas the training\nexamples are seen.2\nBecause of this key difference between training and test datasets, even\nif they are both drawn from the same distribution D, the test error is not\nnecessarily always close to the training error.3As a result, successfully min-\nimizing the training error may not always lead to a small test error. We\ntypically say the model overfits the data if the model predicts accurately on\nthe training dataset but doesn`t generalize well to other test examples, that\nis, if the training error is small but the test error is large. We say the model\nunderfits the data if the training error is relatively large4(and in this case,\ntypically the test error is also relatively large.)\nThis chapter studies how the test error is in{fl}uenced by the learning pro-\ncedure, especially the choice of model parameterizations. We will decompose\nthe test error into {\\textquotedblleft}bias{\\textquotedblright} and {\\textquotedblleft}variance{\\textquotedblright} terms and study how each of them is\naffected by the choice of model parameterizations and their tradeoffs. Using\nthe bias-variance tradeoff, we will discuss when overfitting and underfitting\nwill occur and be avoided. We will also discuss the double descent phe-\nnomenon in Section 8.2 and some classical theoretical results in Section 8.3.\n2These days, researchers have increasingly been more interested in the setting with\n{\\textquotedblleft}domain shift{\\textquotedblright}, that is, the training distribution and test distribution are different.\n3the difference between test error and training error is often referred to as the gener-\nalization gap. The term generalization error in some literature means the test error, and\nin some other literature means the generalization gap.\n4e.g., larger than the intrinsic noise level of the data in regression problems.\n115\n8.1 Bias-variance tradeoff\n0.0",
                    " 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytraining dataset\ntraining data\nground truth h*\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytest dataset\ntest data\nground truth h*\nFigure 8.1: A running example of training and test dataset for this section.\nAs an illustrating example, we consider the following training dataset and\ntest dataset, which are also shown in Figure 8.1. The training inputs x(i)`s are\nrandomly chosen and the outputs y(i)are generated by y(i)=h{\\star}(x(i)) +{\\xi}(i)\nwhere the function h{\\star}({\\textperiodcentered}) is a quadratic function and is shown in Figure 8.1\nas the solid line, and {\\xi}(i)is the a observation noise assumed to be generated\nfrom{\\sim}N(0,{\\sigma}2). A test example ( x,y) also has the same input-output\nrelationship y=h{\\star}(x) +{\\xi}where{\\xi}{\\sim}N(0,{\\sigma}2). It`s impossible to predict the\nnoise{\\xi}, and therefore essentially our goal is to recover the function h{\\star}({\\textperiodcentered}).\nWe will consider the test error of learning various types of models. When\ntalking about linear regression, we discussed the problem of whether to fit\na {\\textquotedblleft}simple{\\textquotedblright} model such as the linear {\\textquotedblleft} y={\\theta}0+{\\theta}1x,{\\textquotedblright} or a more {\\textquotedblleft}complex{\\textquotedblright}\nmodel such as the polynomial {\\textquotedblleft} y={\\theta}0+{\\theta}1x+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}{\\theta}5x5.{\\textquotedblright}\nWe start with fitting a linear model, as shown in Figure 8.2. The best\nfitted linear model cannot predict yfromxaccurately even on the training\ndataset, let alone on the test dataset. This is",
                    " because the true relationship\nbetweenyandxis not linear{\\textemdash}any linear model is far away from the true\nfunctionh{\\star}({\\textperiodcentered}). As a result, the training error is large and this is a typical\nsituation of underfitting .\n116\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytraining data\nbest fit linear model\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytest data\nbest fit linear model\nFigure 8.2: The best fit linear model has large training and test errors.\nThe issue cannot be mitigated with more training examples{\\textemdash}even with\na very large amount of, or even infinite training examples, the best fitted\nlinear model is still inaccurate and fails to capture the structure of the data\n(Figure 8.3). Even if the noise is not present in the training data, the issue\nstill occurs (Figure 8.4). Therefore, the fundamental bottleneck here is the\nlinear model family`s inability to capture the structure in the data{\\textemdash}linear\nmodels cannot represent the true quadratic function h{\\star}{\\textemdash}, but not the lack of\nthe data. Informally, we define the bias of a model to be the test error even\nif we were to fit it to a very (say, infinitely) large training dataset. Thus, in\nthis case, the linear model suffers from large bias, and underfits (i.e., fails to\ncapture structure exhibited by) the data.\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5yfitting linear models on a large dataset\ntraining data\nground truth h*\nbest fit linear model\nFigure 8.3: The best fit linear\nmodel on a much larger dataset\nstill has a large training error.\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5yfitting linear models on a noiseless dataset\ntraining data\nground truth h*\nbest fit linear modelFigure 8.4: The best fit linear\nmodel on a noiseless dataset also\nhas a large training/test error.\nNext, we",
                    " fit a 5th-degree polynomial to the data. Figure 8.5 shows that\nit fails to learn a good model either. However, the failure pattern is different\nfrom the linear model case. Specifically, even though the learnt 5th-degree\n117\npolynomial did a very good job predicting y(i)`s fromx(i)`s for training ex-\namples, it does not work well on test examples (Figure 8.5). In other words,\nthe model learnt from the training set does not generalize well to other test\nexamples{\\textemdash}the test error is high. Contrary to the behavior of linear models,\nthe bias of the 5-th degree polynomials is small{\\textemdash}if we were to fit a 5-th de-\ngree polynomial to an extremely large dataset, the resulting model would be\nclose to a quadratic function and be accurate (Figure 8.6). This is because\nthe family of 5-th degree polynomials contains all the quadratic functions\n(setting{\\theta}5={\\theta}4={\\theta}3= 0 results in a quadratic function), and, therefore,\n5-th degree polynomials are in principle capable of capturing the structure\nof the data.\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytraining data\nbest fit 5-th degree model\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytest data\nground truth h*\nbest fit 5-th degree model\nFigure 8.5: Best fit 5-th degree polynomial has zero training error, but still\nhas a large test error and does not recover the the ground truth. This is a\nclassic situation of overfitting.\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytraining data\nbest fit 5-th degree model\nground truth h*fitting 5-th degree model on large dataset\nFigure 8.6: The best fit 5-th degree polynomial on a huge dataset nearly\nrecovers the ground-truth{\\textemdash}suggesting that the culprit in Figure 8.5 is the\nvari",
                    "ance (or lack of data) but not bias.\nThe failure of fitting 5-th degree polynomials can be captured by another\n118\ncomponent of the test error, called variance of a model fitting procedure.\nSpecifically, when fitting a 5-th degree polynomial as in Figure 8.7, there is a\nlarge risk that we`re fitting patterns in the data that happened to be present\nin our small, finite training set, but that do not re{fl}ect the wider pattern of\nthe relationship between xandy. These {\\textquotedblleft}spurious{\\textquotedblright} patterns in the training\nset are (mostly) due to the observation noise {\\xi}(i), and fitting these spurious\npatters results in a model with large test error. In this case, we say the model\nhas a large variance.\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytraining data\nbest fit 5-th degree model\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytraining data\nbest fit 5-th degree model\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytraining data\nbest fit 5-th degree modelfitting 5-th degree model on different datasets\nFigure 8.7: The best fit 5-th degree models on three different datasets gen-\nerated from the same distribution behave quite differently, suggesting the\nexistence of a large variance.\nThe variance can be intuitively (and mathematically, as shown in Sec-\ntion 8.1.1) characterized by the amount of variations across models learnt\non multiple different training datasets (drawn from the same underlying dis-\ntribution). The {\\textquotedblleft}spurious patterns{\\textquotedblright} are specific to the randomness of the\nnoise (and inputs) in a particular dataset, and thus are different across mul-\ntiple training datasets. Therefore, overfitting to the {\\textquotedblleft}spurious patterns{\\textquotedblright} of\nmultiple datasets should result in very different models. Indeed, as shown\nin Figure 8.7, the models learned on the three different training datasets are\nquite different, overfitting to the",
                    " {\\textquotedblleft}spurious patterns{\\textquotedblright} of each datasets.\nOften, there is a tradeoff between bias and variance. If our model is too\n{\\textquotedblleft}simple{\\textquotedblright} and has very few parameters, then it may have large bias (but small\nvariance), and it typically may suffer from underfittng. If it is too {\\textquotedblleft}complex{\\textquotedblright}\nand has very many parameters, then it may suffer from large variance (but\nhave smaller bias), and thus overfitting. See Figure 8.8 for a typical tradeoff\nbetween bias and variance.\n119\nModel ComplexityError\nBias2VarianceTest Error (= Bias2+Variance) Optimal Tradeoff\nFigure 8.8: An illustration of the typical bias-variance tradeoff.\nAs we will see formally in Section 8.1.1, the test error can be decomposed\nas a summation of bias and variance. This means that the test error will\nhave a convex curve as the model complexity increases, and in practice we\nshould tune the model complexity to achieve the best tradeoff. For instance,\nin the example above, fitting a quadratic function does better than either of\nthe extremes of a first or a 5-th degree polynomial, as shown in Figure 8.9.\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytraining data\nbest fit quadratic model\n0.0 0.2 0.4 0.6 0.8 1.0\nx0.00.51.01.5ytest data\nbest fit quadratic model\nground truth h*\nFigure 8.9: Best fit quadratic model has small training and test error because\nquadratic model achieves a better tradeoff.\nInterestingly, the bias-variance tradeoff curves or the test error curves\ndo not universally follow the shape in Figure 8.8, at least not universally\nwhen the model complexity is simply measured by the number of parameters.\n(We will discuss the so-called double descent phenomenon in Section 8.2.)\nNevertheless, the principle of bias-variance tradeoff is perhaps still the first\nresort when analyzing and predicting the behavior of test errors.\n120\n8.1.1 A",
                    " mathematical decomposition (for regression)\nTo formally state the bias-variance tradeoff for regression problems, we con-\nsider the following setup (which is an extension of the beginning paragraph\nof Section 8.1).\nDraw a training dataset S={\\{}x(i),y(i){\\}}n\ni=1such thaty(i)=h{\\star}(x(i)) +{\\xi}(i)\nwhere{\\xi}(i){\\in}N(0,{\\sigma}2).\nTrain a model on the dataset S, denoted by {\\textasciicircum}hS.\nTake a test example ( x,y) such that y=h{\\star}(x) +{\\xi}where{\\xi}{\\sim}N(0,{\\sigma}2),\nand measure the expected test error (averaged over the random draw of\nthe training set Sand the randomness of {\\xi})56\nMSE(x) =ES,{\\xi}[(y{-}hS(x))2] (8.2)\nWe will decompose the MSE into a bias and variance term. We start by\nstating a following simple mathematical tool that will be used twice below.\nClaim 8.1.1: SupposeAandBare two independent real random variables\nandE[A] = 0. Then, E[(A+B)2] =E[A2] +E[B2].\nAs a corollary, because a random variable Ais independent with a con-\nstantc, when E[A] = 0, we have E[(A+c)2] =E[A2] +c2.\nThe proof of the claim follows from expanding the square: E[(A+B)2] =\nE[A2] +E[B2] + 2E[AB] =E[A2] +E[B2]. Here we used the independence to\nshow that E[AB] =E[A]E[B] = 0.\nUsing Claim 8.1.1 with A={\\xi}andB=h{\\star}(x){-}{\\textasciicircum}hS(x), we have\nMSE(x) =E[(y{-}hS(x))2] =E[({\\xi}+ (h",
                    "{\\star}(x){-}hS(x)))2] (8.3)\n=E[{\\xi}2] +E[(h{\\star}(x){-}hS(x))2] (by Claim 8.1.1)\n={\\sigma}2+E[(h{\\star}(x){-}hS(x))2] (8.4)\nThen, let`s define havg(x) =ES[hS(x)] as the {\\textquotedblleft}average model{\\textquotedblright}{\\textemdash}the model\nobtained by drawing an infinite number of datasets, training on them, and\naveraging their predictions on x. Note that havgis a hypothetical model for\nanalytical purposes that can not be obtained in reality (because we don`t\n5For simplicity, the test input xis considered to be fixed here, but the same conceptual\nmessage holds when we average over the choice of x`s.\n6The subscript under the expectation symbol is to emphasize the variables that are\nconsidered as random by the expectation operation.\n121\nhave infinite number of datasets). It turns out that for many cases, havg\nis (approximately) equal to the the model obtained by training on a single\ndataset with infinite samples. Thus, we can also intuitively interpret havgthis\nway, which is consistent with our intuitive definition of bias in the previous\nsubsection.\nWe can further decompose MSE( x) by letting c=h{\\star}(x){-}havg(x) (which is\na constant that does not depend on the choice of S!) andA=havg(x){-}hS(x)\nin the corollary part of Claim 8.1.1:\nMSE(x) ={\\sigma}2+E[(h{\\star}(x){-}hS(x))2] (8.5)\n={\\sigma}2+ (h{\\star}(x){-}havg(x))2+E[(havg{-}hS(x))2] (8.6)\n={\\sigma}2\n\nunavoidable+ (h{\\star}(x){-}havg(x))2\n\n{\\triangleq}bias2+ var(",
                    "hS(x))\n{\\triangleq}variance(8.7)\nWe call the second term the bias (square) and the third term the variance. As\ndiscussed before, the bias captures the part of the error that are introduced\ndue to the lack of expressivity of the model. Recall that havgcan be thought\nof as the best possible model learned even with infinite data. Thus, the bias is\nnot due to the lack of data, but is rather caused by that the family of models\nfundamentally cannot approximate the h{\\star}. For example, in the illustrating\nexample in Figure 8.2, because any linear model cannot approximate the\ntrue quadratic function h{\\star}, neither can havg, and thus the bias term has to\nbe large.\nThe variance term captures how the random nature of the finite dataset\nintroduces errors in the learned model. It measures the sensitivity of the\nlearned model to the randomness in the dataset. It often decreases as the\nsize of the dataset increases.\nThere is nothing we can do about the first term {\\sigma}2as we can not predict\nthe noise{\\xi}by definition.\nFinally, we note that the bias-variance decomposition for classification\nis much less clear than for regression problems. There have been several\nproposals, but there is as yet no agreement on what is the {\\textquotedblleft}right{\\textquotedblright} and/or\nthe most useful formalism.\n8.2 The double descent phenomenon\nModel-wise double descent. Recent works have demonstrated that the\ntest error can present a {\\textquotedblleft}double descent{\\textquotedblright} phenomenon in a range of machine"
                ]
            },
            {
                "9": [
                    "Chapter 9\nRegularization and model\nselection\n9.1 Regularization\nRecall that as discussed in Section 8.1, overftting is typically a result of using\ntoo complex models, and we need to choose a proper model complexity to\nachieve the optimal bias-variance tradeoff. When the model complexity is\nmeasured by the number of parameters, we can vary the size of the model\n(e.g., the width of a neural net). However, the correct, informative complex-\nity measure of the models can be a function of the parameters (e.g., {\\ell}2norm\nof the parameters), which may not necessarily depend on the number of pa-\nrameters. In such cases, we will use regularization, an important technique\nin machine learning, control the model complexity and prevent overfitting.\nRegularization typically involves adding an additional term, called a reg-\nularizer and denoted by R({\\theta}) here, to the training loss/cost function:\nJ{\\lambda}({\\theta}) =J({\\theta}) +{\\lambda}R({\\theta}) (9.1)\nHereJ{\\lambda}is often called the regularized loss, and {\\lambda}{\\geq}0 is called the regular-\nization parameter. The regularizer R({\\theta}) is a nonnegative function (in almost\nall cases). In classical methods, R({\\theta}) is purely a function of the parameter {\\theta},\nbut some modern approach allows R({\\theta}) to depend on the training dataset.1\nThe regularizer R({\\theta}) is typically chosen to be some measure of the com-\nplexity of the model {\\theta}. Thus, when using the regularized loss, we aim to\nfind a model that both fit the data (a small loss J({\\theta})) and have a small\n1Here our notations generally omit the dependency on the training dataset for\nsimplicity{\\textemdash}we write J({\\theta}) even though it obviously needs to depend on the training dataset.\n135\n136\nmodel complexity (a small R({\\theta})). The balance between the two objectives is\ncontrolled by the regularization parameter {\\lambda}. When{\\lambda}= 0, the regularized\nloss is equivalent to the original loss. When {\\lambda}is a sufficiently small positive\nnumber, minimizing the regularized loss is",
                    " effectively minimizing the original\nloss with the regularizer as the tie-breaker. When the regularizer is extremely\nlarge, then the original loss is not effective (and likely the model will have a\nlarge bias.)\nThe most commonly used regularization is perhaps {\\ell}2regularization,\nwhereR({\\theta}) =1\n2{\\parallel}{\\theta}{\\parallel}2\n2. It encourages the optimizer to find a model with\nsmall{\\ell}2norm. In deep learning, it`s oftentimes referred to as weight de-\ncay, because gradient descent with learning rate {\\eta}on the regularized loss\nR{\\lambda}({\\theta}) is equivalent to shrinking/decaying {\\theta}by a scalar factor of 1 {-}{\\eta}{\\lambda}and\nthen applying the standard gradient\n{\\theta}{\\textleftarrow}{\\theta}{-}{\\eta}{\\nabla}J{\\lambda}({\\theta}) ={\\theta}{-}{\\eta}{\\lambda}{\\theta}{-}{\\eta}{\\nabla}J({\\theta})\n= (1{-}{\\lambda}{\\eta}){\\theta}\ndecaying weights{-}{\\eta}{\\nabla}J({\\theta}) (9.2)\nBesides encouraging simpler models, regularization can also impose in-\nductive biases or structures on the model parameters. For example, suppose\nwe had a prior belief that the number of non-zeros in the ground-truth model\nparameters is small,2{\\textemdash}which is oftentimes called sparsity of the model{\\textemdash}, we\ncan impose a regularization on the number of non-zeros in {\\theta}, denoted by\n{\\parallel}{\\theta}{\\parallel}0, to leverage such a prior belief. Imposing additional structure of the\nparameters narrows our search space and makes the complexity of the model\nfamily smaller,{\\textemdash}e.g., the family of sparse models can be thought of as having\nlower complexity than the family of all models{\\textemdash}, and thus tends to lead to a\nbetter generalization. On the other hand, imposing additional structure may\nrisk increasing the bias. For example, if we regularize the sparsity strongly\nbut no sparse models can predict the label accurately,",
                    " we will suffer from\nlarge bias (analogously to the situation when we use linear models to learn\ndata than can only be represented by quadratic functions in Section 8.1.)\nThe sparsity of the parameters is not a continuous function of the param-\neters, and thus we cannot optimize it with (stochastic) gradient descent. A\ncommon relaxation is to use R({\\theta}) ={\\parallel}{\\theta}{\\parallel}1as a continuous surrogate.3\n2For linear models, this means the model just uses a few coordinates of the inputs to\nmake an accurate prediction.\n3There has been a rich line of theoretical work that explains why {\\parallel}{\\theta}{\\parallel}1is a good sur-\nrogate for encouraging sparsity, but it`s beyond the scope of this course. An intuition is:\nassuming the parameter is on the unit sphere, the parameter with smallest {\\ell}1norm also\n137\nTheR({\\theta}) ={\\parallel}{\\theta}{\\parallel}1(also called LASSO) and R({\\theta}) =1\n2{\\parallel}{\\theta}{\\parallel}2\n2are perhaps\namong the most commonly used regularizers for linear models. Other norm\nand powers of norms are sometimes also used. The {\\ell}2norm regularization is\nmuch more commonly used with kernel methods because {\\ell}1regularization is\ntypically not compatible with the kernel trick (the optimal solution cannot\nbe written as functions of inner products of features.)\nIn deep learning, the most commonly used regularizer is {\\ell}2regularization\nor weight decay. Other common ones include dropout, data augmentation,\nregularizing the spectral norm of the weight matrices, and regularizing the\nLipschitzness of the model, etc. Regularization in deep learning is an ac-\ntive research area, and it`s known that there is another implicit source of\nregularization, as discussed in the next section.\n9.2 Implicit regularization effect (optional\nreading)\nThe implicit regularization effect of optimizers, or implicit bias or algorithmic\nregularization, is a new concept/phenomenon observed in the deep learning\nera. It largely refers to that the optimizers can implicitly impose structures\non parameters beyond what has been imposed by the regularized loss.\nIn most classical settings, the",
                    " loss or regularized loss has a unique global\nminimum, and thus any reasonable optimizer should converge to that global\nminimum and cannot impose any additional preferences. However, in deep\nlearning, oftentimes the loss or regularized loss has more than one (approx-\nimate) global minima, and difference optimizers may converge to different\nglobal minima. Though these global minima have the same or similar train-\ning losses, they may be of different nature and have dramatically different\ngeneralization performance. See Figures 9.1 and 9.2 and its caption for an\nillustration and some experiment results. For example, it`s possible that one\nglobal minimum gives a much more Lipschitz or sparse model than others\nand thus has a better test error. It turns out that many commonly-used op-\ntimizers (or their components) prefer or bias towards finding global minima\nof certain properties, leading to a better test performance.\nhappen to be the sparsest parameter with only 1 non-zero coordinate. Thus, sparsity and\n{\\ell}1norm gives the same extremal points to some extent.\n138\n{\\theta}loss\nFigure 9.1: An Illustration that different global minima of the training loss\ncan have different test performance.\nFigure 9.2: Left: Performance of neural networks trained by two different\nlearning rates schedules on the CIFAR-10 dataset. Although both exper-\niments used exactly the same regularized losses and the optimizers fit the\ntraining data perfectly, the models` generalization performance differ much.\nRight: On a different synthetic dataset, optimizers with different initializa-\ntions have the same training error but different generalization performance.4\nIn summary, the takehome message here is that the choice of optimizer\ndoes not only affect minimizing the training loss, but also imposes implicit\nregularization and affects the generalization of the model. Even if your cur-\nrent optimizer already converges to a small training error perfectly, you may\nstill need to tune your optimizer for a better generalization, .\n4The setting is the same as in Woodworth et al. [2020], HaoChen et al. [2020]\n139\nOne may wonder which components of the optimizers bias towards what\ntype of global minima and what type of global minima may generalize bet-\nter. These are open questions that researchers are actively investigating.\nEmpirical",
                    " and theoretical research have offered some clues and heuristics.\nIn many (but definitely far from all) situations, among those setting where\noptimization can succeed in minimizing the training loss, the use of larger\ninitial learning rate, smaller initialization, smaller batch size, and momen-\ntum appears to help with biasing towards more generalizable solutions. A\nconjecture (that can be proven in certain simplified case) is that stochas-\nticity in the optimization process help the optimizer to find {fl}atter global\nminima (global minima where the curvature of the loss is small), and {fl}at\nglobal minima tend to give more Lipschitz models and better generalization.\nCharacterizing the implicit regularization effect formally is still a challenging\nopen research question.\n9.3 Model selection via cross validation\nSuppose we are trying select among several different models for a learning\nproblem. For instance, we might be using a polynomial regression model\nh{\\theta}(x) =g({\\theta}0+{\\theta}1x+{\\theta}2x2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\theta}kxk), and wish to decide if kshould be\n0, 1, . . . , or 10. How can we automatically select a model that represents\na good tradeoff between the twin evils of bias and variance5? Alternatively,\nsuppose we want to automatically choose the bandwidth parameter {\\tau}for\nlocally weighted regression, or the parameter Cfor our{\\ell}1-regularized SVM.\nHow can we do that?\nFor the sake of concreteness, in these notes we assume we have some\nfinite set of models M={\\{}M1,...,Md{\\}}that we`re trying to select among.\nFor instance, in our first example above, the model Miwould be an i-th\ndegree polynomial regression model. (The generalization to infinite Mis\nnot hard.6) Alternatively, if we are trying to decide between using an SVM,\na neural network or logistic regression, then Mmay contain these models.\n5Given that we said in the previous set of notes that bias and variance are two very\ndifferent beasts, some readers may be wondering if we should be calling them {\\textquotedblleft}twin{\\textquotedbl",
                    "right} evils\nhere. Perhaps it`d be better to think of them as non-identical twins. The phrase {\\textquotedblleft}the\nfraternal twin evils of bias and variance{\\textquotedblright} doesn`t have the same ring to it, though.\n6If we are trying to choose from an infinite set of models, say corresponding to the\npossible values of the bandwidth {\\tau}{\\in}R+, we may discretize {\\tau}and consider only a finite\nnumber of possible values for it. More generally, most of the algorithms described here\ncan all be viewed as performing optimization search in the space of models, and we can\nperform this search over infinite model classes as well.\n140\nCross validation. Lets suppose we are, as usual, given a training set S.\nGiven what we know about empirical risk minimization, here`s what might\ninitially seem like a algorithm, resulting from using empirical risk minimiza-\ntion for model selection:\n1. Train each model MionS, to get some hypothesis hi.\n2. Pick the hypotheses with the smallest training error.\nThis algorithm does notwork. Consider choosing the degree of a poly-\nnomial. The higher the degree of the polynomial, the better it will fit the\ntraining set S, and thus the lower the training error. Hence, this method will\nalways select a high-variance, high-degree polynomial model, which we saw\npreviously is often poor choice.\nHere`s an algorithm that works better. In hold-out cross validation\n(also called simple cross validation ), we do the following:\n1. Randomly split SintoStrain(say, 70{\\%} of the data) and Scv(the remain-\ning 30{\\%}). Here, Scvis called the hold-out cross validation set.\n2. Train each model MionStrainonly, to get some hypothesis hi.\n3. Select and output the hypothesis hithat had the smallest error {\\textasciicircum} {\\varepsilon}Scv(hi)\non the hold out cross validation set. (Here {\\textasciicircum} {\\varepsilon}Scv(h) denotes the average\nerror ofhon the set of examples in Scv.) The error on the hold out\nvalidation set is also referred to as the validation error.\nBy testing/",
                    "validating on a set of examples Scvthat the models were not\ntrained on, we obtain a better estimate of each hypothesis hi`s true general-\nization/test error. Thus, this approach is essentially picking the model with\nthe smallest estimated generalization/test error. The size of the validation\nset depends on the total number of available examples. Usually, somewhere\nbetween 1/4{-}1/3 of the data is used in the hold out cross validation set, and\n30{\\%} is a typical choice. However, when the total dataset is huge, validation\nset can be a smaller fraction of the total examples as long as the absolute\nnumber of validation examples is decent. For example, for the ImageNet\ndataset that has about 1M training images, the validation set is sometimes\nset to be 50K images, which is only about 5{\\%} of the total examples.\nOptionally, step 3 in the algorithm may also be replaced with selecting\nthe modelMiaccording to arg min i{\\textasciicircum}{\\varepsilon}Scv(hi), and then retraining Mion the\nentire training set S. (This is often a good idea, with one exception being\nlearning algorithms that are be very sensitive to perturbations of the initial\n141\nconditions and/or data. For these methods, Midoing well on Straindoes not\nnecessarily mean it will also do well on Scv, and it might be better to forgo\nthis retraining step.)\nThe disadvantage of using hold out cross validation is that it {\\textquotedblleft}wastes{\\textquotedblright}\nabout 30{\\%} of the data. Even if we were to take the optional step of retraining\nthe model on the entire training set, it`s still as if we`re trying to find a good\nmodel for a learning problem in which we had 0 .7ntraining examples, rather\nthanntraining examples, since we`re testing models that were trained on\nonly 0.7nexamples each time. While this is fine if data is abundant and/or\ncheap, in learning problems in which data is scarce (consider a problem with\nn= 20, say), we`d like to do something better.\nHere is a method, called k-fold cross validation , that holds out less\ndata each time:\n1. Randomly split Sintokdisjoint subs",
                    "ets of m/k training examples each.\nLets call these subsets S1,...,Sk.\n2. For each model Mi, we evaluate it as follows:\nForj= 1,...,k\nTrain the model MionS1{\\cup}{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}{\\cup}Sj{-}1{\\cup}Sj+1{\\cup}{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}Sk(i.e., train\non all the data except Sj) to get some hypothesis hij.\nTest the hypothesis hijonSj, to get {\\textasciicircum}{\\varepsilon}Sj(hij).\nThe estimated generalization error of model Miis then calculated\nas the average of the {\\textasciicircum} {\\varepsilon}Sj(hij)`s (averaged over j).\n3. Pick the model Miwith the lowest estimated generalization error, and\nretrain that model on the entire training set S. The resulting hypothesis\nis then output as our final answer.\nA typical choice for the number of folds to use here would be k= 10.\nWhile the fraction of data held out each time is now 1 /k{\\textemdash}much smaller\nthan before{\\textemdash}this procedure may also be more computationally expensive\nthan hold-out cross validation, since we now need train to each model k\ntimes.\nWhilek= 10 is a commonly used choice, in problems in which data is\nreally scarce, sometimes we will use the extreme choice of k=min order\nto leave out as little data as possible each time. In this setting, we would\nrepeatedly train on all but one of the training examples in S, and test on that\nheld-out example. The resulting m=kerrors are then averaged together to\nobtain our estimate of the generalization error of a model. This method has\n142\nits own name; since we`re holding out one training example at a time, this\nmethod is called leave-one-out cross validation.\nFinally, even though we have described the different versions of cross vali-\ndation as methods for selecting a model, they can also be used more simply to\nevaluate a single model or algorithm. For example, if you have implemented\nsome learning algorithm and want to estimate how well it performs for your",
                    "\napplication (or if you have invented a novel learning algorithm and want to\nreport in a technical paper how well it performs on various test sets), cross\nvalidation would give a reasonable way of doing so.\n9.4 Bayesian statistics and regularization\nIn this section, we will talk about one more tool in our arsenal for our battle\nagainst overfitting.\nAt the beginning of the quarter, we talked about parameter fitting using\nmaximum likelihood estimation (MLE), and chose our parameters according\nto\n{\\theta}MLE= arg max\n{\\theta}n{\\prod}\ni=1p(y(i)|x(i);{\\theta}).\nThroughout our subsequent discussions, we viewed {\\theta}as an unknown param-\neter of the world. This view of the {\\theta}as being constant-valued but unknown\nis taken in frequentist statistics. In the frequentist this view of the world, {\\theta}\nis not random{\\textemdash}it just happens to be unknown{\\textemdash}and it`s our job to come up\nwith statistical procedures (such as maximum likelihood) to try to estimate\nthis parameter.\nAn alternative way to approach our parameter estimation problems is to\ntake the Bayesian view of the world, and think of {\\theta}as being a random\nvariable whose value is unknown. In this approach, we would specify a\nprior distribution p({\\theta}) on{\\theta}that expresses our {\\textquotedblleft}prior beliefs{\\textquotedblright} about the\nparameters. Given a training set S={\\{}(x(i),y(i)){\\}}n\ni=1, when we are asked to\nmake a prediction on a new value of x, we can then compute the posterior\ndistribution on the parameters\np({\\theta}|S) =p(S|{\\theta})p({\\theta})\np(S)\n=({\\prod}n\ni=1p(y(i)|x(i),{\\theta}))\np({\\theta}){\\int}\n{\\theta}({\\prod}n\ni=1p(y(i)|x(i),{\\theta})p({\\theta}))d{\\theta}(9.3)\nIn the equation above, p(y(i)|x",
                    "(i),{\\theta}) comes from whatever model you`re using\n143\nfor your learning problem. For example, if you are using Bayesian logistic re-\ngression, then you might choose p(y(i)|x(i),{\\theta}) =h{\\theta}(x(i))y(i)(1{-}h{\\theta}(x(i)))(1{-}y(i)),\nwhereh{\\theta}(x(i)) = 1/(1 + exp({-}{\\theta}Tx(i))).7\nWhen we are given a new test example xand asked to make it prediction\non it, we can compute our posterior distribution on the class label using the\nposterior distribution on {\\theta}:\np(y|x,S) ={\\int}\n{\\theta}p(y|x,{\\theta})p({\\theta}|S)d{\\theta} (9.4)\nIn the equation above, p({\\theta}|S) comes from Equation (9.3). Thus, for example,\nif the goal is to the predict the expected value of ygivenx, then we would\noutput8\nE[y|x,S] ={\\int}\nyyp(y|x,S)dy\nThe procedure that we`ve outlined here can be thought of as doing {\\textquotedblleft}fully\nBayesian{\\textquotedblright} prediction, where our prediction is computed by taking an average\nwith respect to the posterior p({\\theta}|S) over{\\theta}. Unfortunately, in general it is\ncomputationally very difficult to compute this posterior distribution. This is\nbecause it requires taking integrals over the (usually high-dimensional) {\\theta}as\nin Equation (9.3), and this typically cannot be done in closed-form.\nThus, in practice we will instead approximate the posterior distribution\nfor{\\theta}. One common approximation is to replace our posterior distribution for\n{\\theta}(as in Equation 9.4) with a single point estimate. The MAP (maximum\na posteriori) estimate for {\\theta}is given by\n{\\theta}MAP= arg max\n{\\theta}n{\\prod}\ni=1p(y(i)|x(i),{\\theta})p({\\the",
                    "ta}). (9.5)\nNote that this is the same formulas as for the MLE (maximum likelihood)\nestimate for {\\theta}, except for the prior p({\\theta}) term at the end.\nIn practical applications, a common choice for the prior p({\\theta}) is to assume\nthat{\\theta}{\\sim}N(0,{\\tau}2I). Using this choice of prior, the fitted parameters {\\theta}MAPwill\nhave smaller norm than that selected by maximum likelihood. In practice,\nthis causes the Bayesian MAP estimate to be less susceptible to overfitting\nthan the ML estimate of the parameters. For example, Bayesian logistic\nregression turns out to be an effective algorithm for text classification, even\nthough in text classification we usually have d{\\gg}n.\n7Since we are now viewing {\\theta}as a random variable, it is okay to condition on it value,\nand write {\\textquotedblleft} p(y|x,{\\theta}){\\textquotedblright} instead of {\\textquotedblleft} p(y|x;{\\theta}).{\\textquotedblright}\n8The integral below would be replaced by a summation if yis discrete-valued."
                ]
            },
            {
                "11": [
                    "151\nthat will allow us to easily apply it to other estimation problems in which\nthere are also latent variables, and which will allow us to give a convergence\nguarantee.\n11.2 Jensen`s inequality\nWe begin our discussion with a very useful result called Jensen`s inequality\nLetfbe a function whose domain is the set of real numbers. Recall that\nfis a convex function if f{'}{'}(x){\\geq}0 (for allx{\\in}R). In the case of ftaking\nvector-valued inputs, this is generalized to the condition that its hessian H\nis positive semi-definite ( H{\\geq}0). Iff{'}{'}(x){>}0 for allx, then we say fis\nstrictly convex (in the vector-valued case, the corresponding statement is\nthatHmust be positive definite, written H {>} 0). Jensen`s inequality can\nthen be stated as follows:\nTheorem. Letfbe a convex function, and let Xbe a random variable.\nThen:\nE[f(X)]{\\geq}f(EX).\nMoreover, if fis strictly convex, then E[ f(X)] =f(EX) holds true if and\nonly ifX= E[X] with probability 1 (i.e., if Xis a constant).\nRecall our convention of occasionally dropping the parentheses when writ-\ning expectations, so in the theorem above, f(EX) =f(E[X]).\nFor an interpretation of the theorem, consider the figure below.\na E[X] bf(a)\nf(b)\nf(EX)E[f(X)]f\nHere,fis a convex function shown by the solid line. Also, Xis a random\nvariable that has a 0.5 chance of taking the value a, and a 0.5 chance of\n152\ntaking the value b(indicated on the x-axis). Thus, the expected value of X\nis given by the midpoint between aandb.\nWe also see the values f(a),f(b) andf(E[X]) indicated on the y-axis.\nMoreover, the value E[ f(X)] is now the midpoint on the y-axis between f(a)\nandf(b). From our example, we see that",
                    " because fis convex, it must be the\ncase that E[ f(X)]{\\geq}f(EX).\nIncidentally, quite a lot of people have trouble remembering which way\nthe inequality goes, and remembering a picture like this is a good way to\nquickly figure out the answer.\nRemark. Recall that fis [strictly] concave if and only if {-}fis [strictly]\nconvex (i.e., f{'}{'}(x){\\leq}0 orH{\\leq}0). Jensen`s inequality also holds for concave\nfunctionsf, but with the direction of all the inequalities reversed (E[ f(X)]{\\leq}\nf(EX), etc.).\n11.3 General EM algorithms\nSuppose we have an estimation problem in which we have a training set\n{\\{}x(1),...,x(n){\\}}consisting of nindependent examples. We have a latent vari-\nable model p(x,z;{\\theta}) withzbeing the latent variable (which for simplicity is\nassumed to take finite number of values). The density for xcan be obtained\nby marginalized over the latent variable z:\np(x;{\\theta}) ={\\sum}\nzp(x,z;{\\theta}) (11.1)\nWe wish to fit the parameters {\\theta}by maximizing the log-likelihood of the\ndata, defined by\n{\\ell}({\\theta}) =n{\\sum}\ni=1logp(x(i);{\\theta}) (11.2)\nWe can rewrite the objective in terms of the joint density p(x,z;{\\theta}) by\n{\\ell}({\\theta}) =n{\\sum}\ni=1logp(x(i);{\\theta}) (11.3)\n=n{\\sum}\ni=1log{\\sum}\nz(i)p(x(i),z(i);{\\theta}). (11.4)\nBut, explicitly finding the maximum likelihood estimates of the parameters\n{\\theta}may be hard since it will result in difficult non-convex optimization prob-"
                ]
            }
        ]
    },
    "Math for Machine Learning": {
        "authors": [
            "Marc Peter Deisenroth",
            "A Aldo Faisal",
            "Cheng Soon Ong"
        ],
        "year": 2019,
        "chapters": [
            {
                "3": [
                    "3.1 Norms 71\nFigure 3.3 For\ndifferent norms, the\nred lines indicate\nthe set of vectors\nwith norm 1. Left:\nManhattan norm;\nRight: Euclidean\ndistance.\n1\n1\n1\n1\n{\\parallel}x{\\parallel}1= 1\n{\\parallel}x{\\parallel}2= 1\n3.1 Norms\nWhen we think of geometric vectors, i.e., directed line segments that start\nat the origin, then intuitively the length of a vector is the distance of the\n{\\textquotedblleft}end{\\textquotedblright} of this directed line segment from the origin. In the following, we\nwill discuss the notion of the length of vectors using the concept of a norm.\nDefinition 3.1 (Norm) .Anorm on a vector space Vis a function norm\n{\\parallel} {\\textperiodcentered} {\\parallel}:V{\\textrightarrow}R, (3.1)\nx7{\\textrightarrow} {\\parallel}x{\\parallel}, (3.2)\nwhich assigns each vector xitslength {\\parallel}x{\\parallel} {\\in}R, such that for all {\\lambda}{\\in}R length\nandx,y{\\in}Vthe following hold:\nabsolutely\nhomogeneous Absolutely homogeneous: {\\parallel}{\\lambda}x{\\parallel}=|{\\lambda}|{\\parallel}x{\\parallel}\ntriangle inequality Triangle inequality: {\\parallel}x+y{\\parallel}{\\leqslant}{\\parallel}x{\\parallel}+{\\parallel}y{\\parallel}\npositive definite Positive definite: {\\parallel}x{\\parallel}{\\geqslant}0and{\\parallel}x{\\parallel}= 0{\\Leftarrow}{\\Rightarrow}x=0\nFigure 3.2 Triangle\ninequality.\na\nb\nc{\\leq}a+b In geometric terms, the triangle inequality states that for any triangle,\nthe sum of the lengths of any two sides must be greater than or equal\nto the length of the remaining side; see Figure 3.2 for an illustration.\nDefinition 3.1 is in terms of a general vector space V(Section 2.4), but\nin this book we will only consider a finite-dimensional vector space Rn.\nRecall that for",
                    " a vector x{\\in}Rnwe denote the elements of the vector using\na subscript, that is, xiis the ithelement of the vector x.\nExample 3.1 (Manhattan Norm)\nThe Manhattan norm onRnis defined for x{\\in}Rnas Manhattan norm\n{\\parallel}x{\\parallel}1:=nX\ni=1|xi|, (3.3)\nwhere | {\\textperiodcentered} |is the absolute value. The left panel of Figure 3.3 shows all\nvectors x{\\in}R2with{\\parallel}x{\\parallel}1= 1. The Manhattan norm is also called {\\ell}1{\\ell}1norm\nnorm .\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n72 Analytic Geometry\nExample 3.2 (Euclidean Norm)\nThe Euclidean norm ofx{\\in}Rnis defined as Euclidean norm\n{\\parallel}x{\\parallel}2:=vuutnX\ni=1x2\ni={\\sqrt{}}\nx{\\top}x (3.4)\nand computes the Euclidean distance ofxfrom the origin. The right panel Euclidean distance\nof Figure 3.3 shows all vectors x{\\in}R2with{\\parallel}x{\\parallel}2= 1. The Euclidean\nnorm is also called {\\ell}2norm . {\\ell}2norm\nRemark. Throughout this book, we will use the Euclidean norm (3.4) by\ndefault if not stated otherwise. {\\diamond}\n3.2 Inner Products\nInner products allow for the introduction of intuitive geometrical con-\ncepts, such as the length of a vector and the angle or distance between\ntwo vectors. A major purpose of inner products is to determine whether\nvectors are orthogonal to each other.\n3.2.1 Dot Product\nWe may already be familiar with a particular type of inner product, the\nscalar product /dot product inRn, which is given by scalar product\ndot product\nx{\\top}y=nX\ni=1xiyi. (3.5)\nWe will refer to this particular inner product as the dot product in this",
                    "\nbook. However, inner products are more general concepts with specific\nproperties, which we will now introduce.\n3.2.2 General Inner Products\nRecall the linear mapping from Section 2.7, where we can rearrange the\nmapping with respect to addition and multiplication with a scalar. A bi- bilinear mapping\nlinear mapping {\\Omega}is a mapping with two arguments, and it is linear in\neach argument, i.e., when we look at a vector space Vthen it holds that\nfor all x,y,z{\\in}V, {\\lambda}, {\\psi} {\\in}Rthat\n{\\Omega}({\\lambda}x+{\\psi}y,z) ={\\lambda}{\\Omega}(x,z) +{\\psi}{\\Omega}(y,z) (3.6)\n{\\Omega}(x, {\\lambda}y+{\\psi}z) ={\\lambda}{\\Omega}(x,y) +{\\psi}{\\Omega}(x,z). (3.7)\nHere, (3.6) asserts that {\\Omega}is linear in the first argument, and (3.7) asserts\nthat{\\Omega}is linear in the second argument (see also (2.87)).\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n3.2 Inner Products 73\nDefinition 3.2. LetVbe a vector space and {\\Omega} :V{\\texttimes}V{\\textrightarrow}Rbe a bilinear\nmapping that takes two vectors and maps them onto a real number. Then\n{\\Omega}is called symmetric if{\\Omega}(x,y) = {\\Omega}( y,x)for all x,y{\\in}V, i.e., the symmetric\norder of the arguments does not matter.\n{\\Omega}is called positive definite if positive definite\n{\\forall}x{\\in}V{\\textbackslash}{\\{}0{\\}}: {\\Omega}(x,x){>}0,{\\Omega}(0,0) = 0 . (3.8)\nDefinition 3.3. LetVbe a vector space and {\\Omega}",
                    " :V{\\texttimes}V{\\textrightarrow}Rbe a bilinear\nmapping that takes two vectors and maps them onto a real number. Then\nA positive definite, symmetric bilinear mapping {\\Omega} :V{\\texttimes}V{\\textrightarrow}Ris called\naninner product onV. We typically write {\\langle}x,y{\\rangle}instead of {\\Omega}(x,y). inner product\nThe pair (V,{\\langle}{\\textperiodcentered},{\\textperiodcentered}{\\rangle})is called an inner product space or (real) vector space inner product space\nvector space with\ninner productwith inner product . If we use the dot product defined in (3.5), we call\n(V,{\\langle}{\\textperiodcentered},{\\textperiodcentered}{\\rangle})aEuclidean vector space .\nEuclidean vector\nspace We will refer to these spaces as inner product spaces in this book.\nExample 3.3 (Inner Product That Is Not the Dot Product)\nConsider V=R2. If we define\n{\\langle}x,y{\\rangle}:=x1y1{-}(x1y2+x2y1) + 2x2y2 (3.9)\nthen{\\langle}{\\textperiodcentered},{\\textperiodcentered}{\\rangle}is an inner product but different from the dot product. The proof\nwill be an exercise.\n3.2.3 Symmetric, Positive Definite Matrices\nSymmetric, positive definite matrices play an important role in machine\nlearning, and they are defined via the inner product. In Section 4.3, we\nwill return to symmetric, positive definite matrices in the context of matrix\ndecompositions. The idea of symmetric positive semidefinite matrices is\nkey in the definition of kernels (Section 12.4).\nConsider an n-dimensional vector space Vwith an inner product {\\langle}{\\textperiodcentered},{\\textperiodcentered}{\\rangle}:\nV{\\texttimes}V{\\textrightarrow}R(see Definition 3.3) and an ordered basis B= (b1, . . . ,bn)of\nV. Recall from Section 2.6.1 that any vectors x,y{\\in}Vcan be written as\nlinear combinations of the basis vectors so that",
                    " x=Pn\ni=1{\\psi}ibi{\\in}Vand\ny=Pn\nj=1{\\lambda}jbj{\\in}Vfor suitable {\\psi}i, {\\lambda}j{\\in}R. Due to the bilinearity of the\ninner product, it holds for all x,y{\\in}Vthat\n{\\langle}x,y{\\rangle}=*nX\ni=1{\\psi}ibi,nX\nj=1{\\lambda}jbj+\n=nX\ni=1nX\nj=1{\\psi}i{\\langle}bi,bj{\\rangle}{\\lambda}j={\\textasciicircum}x{\\top}A{\\textasciicircum}y,(3.10)\nwhere Aij:={\\langle}bi,bj{\\rangle}and{\\textasciicircum}x,{\\textasciicircum}yare the coordinates of xandywith respect\nto the basis B. This implies that the inner product {\\langle}{\\textperiodcentered},{\\textperiodcentered}{\\rangle}is uniquely deter-\nmined through A. The symmetry of the inner product also means that A\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n74 Analytic Geometry\nis symmetric. Furthermore, the positive definiteness of the inner product\nimplies that\n{\\forall}x{\\in}V{\\textbackslash}{\\{}0{\\}}:x{\\top}Ax{>}0. (3.11)\nDefinition 3.4 (Symmetric, Positive Definite Matrix) .A symmetric matrix\nA{\\in}Rn{\\texttimes}nthat satisfies (3.11) is called symmetric, positive definite , or symmetric, positive\ndefinite justpositive definite . If only {\\geqslant}holds in (3.11), then Ais called symmetric,\npositive definite\nsymmetric, positive\nsemidefinitepositive semidefinite .\nExample 3.4 (Symmetric, Positive Definite Matrices)\nConsider the matrices\nA1=9 6\n6 5\n,A2=9 6\n6 3\n",
                    ". (3.12)\nA1is positive definite because it is symmetric and\nx{\\top}A1x=x1x29 6\n6 5x1\nx2\n(3.13a)\n= 9x2\n1+ 12x1x2+ 5x2\n2= (3x1+ 2x2)2+x2\n2{>}0 (3.13b)\nfor all x{\\in}V{\\textbackslash}{\\{}0{\\}}. In contrast, A2is symmetric but not positive definite\nbecause x{\\top}A2x= 9x2\n1+ 12x1x2+ 3x2\n2= (3x1+ 2x2)2{-}x2\n2can be less\nthan 0, e.g., for x= [2,{-}3]{\\top}.\nIfA{\\in}Rn{\\texttimes}nis symmetric, positive definite, then\n{\\langle}x,y{\\rangle}={\\textasciicircum}x{\\top}A{\\textasciicircum}y (3.14)\ndefines an inner product with respect to an ordered basis B, where {\\textasciicircum}xand\n{\\textasciicircum}yare the coordinate representations of x,y{\\in}Vwith respect to B.\nTheorem 3.5. For a real-valued, finite-dimensional vector space Vand an\nordered basis BofV, it holds that {\\langle}{\\textperiodcentered},{\\textperiodcentered}{\\rangle}:V{\\texttimes}V{\\textrightarrow}Ris an inner product if\nand only if there exists a symmetric, positive definite matrix A{\\in}Rn{\\texttimes}nwith\n{\\langle}x,y{\\rangle}={\\textasciicircum}x{\\top}A{\\textasciicircum}y. (3.15)\nThe following properties hold if A{\\in}Rn{\\texttimes}nis symmetric and positive\ndefinite:\nThe null space (kernel) of Aconsists only of 0because x{\\top}Ax{>}0for\nallx=0. This implies that Ax=0ifx=0.\nThe diagonal elements aiiofAare positive because aii=e",
                    "{\\top}\niAei{>}0,\nwhere eiis the ith vector of the standard basis in Rn.\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n3.3 Lengths and Distances 75\n3.3 Lengths and Distances\nIn Section 3.1, we already discussed norms that we can use to compute\nthe length of a vector. Inner products and norms are closely related in the\nsense that any inner product induces a norm Inner products\ninduce norms.\n{\\parallel}x{\\parallel}:=q\n{\\langle}x,x{\\rangle} (3.16)\nin a natural way, such that we can compute lengths of vectors using the in-\nner product. However, not every norm is induced by an inner product. The\nManhattan norm (3.3) is an example of a norm without a corresponding\ninner product. In the following, we will focus on norms that are induced\nby inner products and introduce geometric concepts, such as lengths, dis-\ntances, and angles.\nRemark (Cauchy-Schwarz Inequality) .For an inner product vector space\n(V,{\\langle}{\\textperiodcentered},{\\textperiodcentered}{\\rangle})the induced norm {\\parallel} {\\textperiodcentered} {\\parallel}satisfies the Cauchy-Schwarz inequality Cauchy-Schwarz\ninequality\n|{\\langle}x,y{\\rangle}|{\\leqslant}{\\parallel}x{\\parallel}{\\parallel}y{\\parallel}. (3.17)\n{\\diamond}\nExample 3.5 (Lengths of Vectors Using Inner Products)\nIn geometry, we are often interested in lengths of vectors. We can now use\nan inner product to compute them using (3.16). Let us take x= [1,1]{\\top}{\\in}\nR2. If we use the dot product as the inner product, with (3.16) we obtain\n{\\parallel}x{\\parallel}={\\sqrt{}}\nx{\\top}x={\\sqrt{}}\n12+ 12={\\sqrt{}}\n2 (3.18)\nas the",
                    " length of x. Let us now choose a different inner product:\n{\\langle}x,y{\\rangle}:=x{\\top}1{-}1\n2\n{-}1\n21\ny=x1y1{-}1\n2(x1y2+x2y1) +x2y2.(3.19)\nIf we compute the norm of a vector, then this inner product returns smaller\nvalues than the dot product if x1andx2have the same sign (and x1x2{>}\n0); otherwise, it returns greater values than the dot product. With this\ninner product, we obtain\n{\\langle}x,x{\\rangle}=x2\n1{-}x1x2+x2\n2= 1{-}1 + 1 = 1 = {\\Rightarrow} {\\parallel}x{\\parallel}={\\sqrt{}}\n1 = 1 ,(3.20)\nsuch that xis {\\textquotedblleft}shorter{\\textquotedblright} with this inner product than with the dot product.\nDefinition 3.6 (Distance and Metric) .Consider an inner product space\n(V,{\\langle}{\\textperiodcentered},{\\textperiodcentered}{\\rangle}). Then\nd(x,y) :={\\parallel}x{-}y{\\parallel}=q\n{\\langle}x{-}y,x{-}y{\\rangle} (3.21)\nis called the distance between xandyforx,y{\\in}V. If we use the dot distance\nproduct as the inner product, then the distance is called Euclidean distance .Euclidean distance\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n76 Analytic Geometry\nThe mapping\nd:V{\\texttimes}V{\\textrightarrow}R (3.22)\n(x,y)7{\\textrightarrow}d(x,y) (3.23)\nis called a metric . metric\nRemark. Similar to the length of a vector, the distance between vectors\ndoes not require an inner product: a norm is sufficient. If we have a norm\ninduced by an inner product, the distance may vary depending on the\n",
                    "choice of the inner product. {\\diamond}\nA metric dsatisfies the following:\n1.dispositive definite , i.e., d(x,y){\\geqslant}0for all x,y{\\in}Vandd(x,y) = positive definite\n0{\\Leftarrow}{\\Rightarrow}x=y.\n2.dissymmetric , i.e., d(x,y) =d(y,x)for all x,y{\\in}V. symmetric\ntriangle inequality 3.Triangle inequality: d(x,z){\\leqslant}d(x,y) +d(y,z)for all x,y,z{\\in}V.\nRemark. At first glance, the lists of properties of inner products and met-\nrics look very similar. However, by comparing Definition 3.3 with Defini-\ntion 3.6 we observe that {\\langle}x,y{\\rangle}andd(x,y)behave in opposite directions.\nVery similar xandywill result in a large value for the inner product and\na small value for the metric. {\\diamond}\n3.4 Angles and Orthogonality\nFigure 3.4 When\nrestricted to [0, {\\pi}]\nthenf({\\omega}) = cos( {\\omega})\nreturns a unique\nnumber in the\ninterval [{-}1,1].\n0{\\pi}/2{\\pi}\n{\\omega}{-}101cos({\\omega})In addition to enabling the definition of lengths of vectors, as well as the\ndistance between two vectors, inner products also capture the geometry\nof a vector space by defining the angle {\\omega}between two vectors. We use\nthe Cauchy-Schwarz inequality (3.17) to define angles {\\omega}in inner prod-\nuct spaces between two vectors x,y, and this notion coincides with our\nintuition in R2andR3. Assume that x=0,y=0. Then\n{-}1{\\leqslant}{\\langle}x,y{\\rangle}\n{\\parallel}x{\\parallel}{\\parallel}y{\\parallel}{\\leqslant}1. (3.24)\nTherefore, there exists a unique {\\omega}{\\in",
                    "}[0, {\\pi}], illustrated in Figure 3.4, with\ncos{\\omega}={\\langle}x,y{\\rangle}\n{\\parallel}x{\\parallel}{\\parallel}y{\\parallel}. (3.25)\nThe number {\\omega}is the angle between the vectors xandy. Intuitively, the angle\nangle between two vectors tells us how similar their orientations are. For\nexample, using the dot product, the angle between xandy= 4x, i.e.,y\nis a scaled version of x, is0: Their orientation is the same.\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n3.4 Angles and Orthogonality 77\nExample 3.6 (Angle between Vectors)\nLet us compute the angle between x= [1,1]{\\top}{\\in}R2andy= [1,2]{\\top}{\\in}R2;Figure 3.5 The\nangle {\\omega}between\ntwo vectors x,yis\ncomputed using the\ninner product.\ny\nx\n1 01\n{\\omega}see Figure 3.5, where we use the dot product as the inner product. Then\nwe get\ncos{\\omega}={\\langle}x,y{\\rangle}p\n{\\langle}x,x{\\rangle}{\\langle}y,y{\\rangle}=x{\\top}yp\nx{\\top}xy{\\top}y=3{\\sqrt{}}\n10, (3.26)\nand the angle between the two vectors is arccos(3{\\sqrt{}}\n10){\\approx}0.32 rad , which\ncorresponds to about 18{\\textopenbullet}.\nA key feature of the inner product is that it also allows us to characterize\nvectors that are orthogonal.\nDefinition 3.7 (Orthogonality) .Two vectors xandyareorthogonal if and orthogonal\nonly if {\\langle}x,y{\\rangle}= 0, and we write x{\\perp}y. If additionally {\\parallel}x{\\parallel}= 1 = {\\parallel}y{\\parallel},\ni",
                    ".e., the vectors are unit vectors, then xandyareorthonormal . orthonormal\nAn implication of this definition is that the 0-vector is orthogonal to\nevery vector in the vector space.\nRemark. Orthogonality is the generalization of the concept of perpendic-\nularity to bilinear forms that do not have to be the dot product. In our\ncontext, geometrically, we can think of orthogonal vectors as having a\nright angle with respect to a specific inner product. {\\diamond}\nExample 3.7 (Orthogonal Vectors)\nFigure 3.6 The\nangle {\\omega}between\ntwo vectors x,ycan\nchange depending\non the inner\nproduct.y x\n{-}1 1 01\n{\\omega}\nConsider two vectors x= [1,1]{\\top},y= [{-}1,1]{\\top}{\\in}R2; see Figure 3.6.\nWe are interested in determining the angle {\\omega}between them using two\ndifferent inner products. Using the dot product as the inner product yields\nan angle {\\omega}between xandyof90{\\textopenbullet}, such that x{\\perp}y. However, if we\nchoose the inner product\n{\\langle}x,y{\\rangle}=x{\\top}2 0\n0 1\ny, (3.27)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n78 Analytic Geometry\nwe get that the angle {\\omega}between xandyis given by\ncos{\\omega}={\\langle}x,y{\\rangle}\n{\\parallel}x{\\parallel}{\\parallel}y{\\parallel}={-}1\n3={\\Rightarrow}{\\omega}{\\approx}1.91 rad {\\approx}109.5{\\textopenbullet}, (3.28)\nandxandyare not orthogonal. Therefore, vectors that are orthogonal\nwith respect to one inner product do not have to be orthogonal with re-\nspect to a different inner product.\nDefinition 3.8 (Orthogonal Matrix) .A square matrix A{\\in}Rn{\\texttimes}nis an",
                    "\northogonal matrix if and only if its columns are orthonormal so that orthogonal matrix\nAA{\\top}=I=A{\\top}A, (3.29)\nwhich implies that\nA{-}1=A{\\top}, (3.30)\ni.e., the inverse is obtained by simply transposing the matrix. It is convention to\ncall these matrices\n{\\textquotedblleft}orthogonal{\\textquotedblright} but a\nmore precise\ndescription would\nbe {\\textquotedblleft}orthonormal{\\textquotedblright}.Transformations by orthogonal matrices are special because the length\nof a vector xis not changed when transforming it using an orthogonal\nmatrix A. For the dot product, we obtain\nTransformations\nwith orthogonal\nmatrices preserve\ndistances and\nangles.{\\parallel}Ax{\\parallel}2= (Ax){\\top}(Ax) =x{\\top}A{\\top}Ax=x{\\top}Ix=x{\\top}x={\\parallel}x{\\parallel}2.(3.31)\nMoreover, the angle between any two vectors x,y, as measured by their\ninner product, is also unchanged when transforming both of them using\nan orthogonal matrix A. Assuming the dot product as the inner product,\nthe angle of the images AxandAyis given as\ncos{\\omega}=(Ax){\\top}(Ay)\n{\\parallel}Ax{\\parallel}{\\parallel}Ay{\\parallel}=x{\\top}A{\\top}Ayq\nx{\\top}A{\\top}Axy{\\top}A{\\top}Ay=x{\\top}y\n{\\parallel}x{\\parallel}{\\parallel}y{\\parallel},(3.32)\nwhich gives exactly the angle between xandy. This means that orthog-\nonal matrices AwithA{\\top}=A{-}1preserve both angles and distances. It\nturns out that orthogonal matrices define transformations that are rota-\ntions (with the possibility of flips). In Section 3.9, we will discuss more\ndetails about rotations.\n3.5 Orthonormal Basis\nIn Section 2.6.1, we characterized properties of basis vectors and found\nthat in an n-dimensional vector space, we",
                    " need nbasis vectors, i.e., n\nvectors that are linearly independent. In Sections 3.3 and 3.4, we used\ninner products to compute the length of vectors and the angle between\nvectors. In the following, we will discuss the special case where the basis\nvectors are orthogonal to each other and where the length of each basis\nvector is 1. We will call this basis then an orthonormal basis.\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com ."
                ]
            },
            {
                "4": [
                    "4\nMatrix Decompositions\nIn Chapters 2 and 3, we studied ways to manipulate and measure vectors,\nprojections of vectors, and linear mappings. Mappings and transforma-\ntions of vectors can be conveniently described as operations performed by\nmatrices. Moreover, data is often represented in matrix form as well, e.g.,\nwhere the rows of the matrix represent different people and the columns\ndescribe different features of the people, such as weight, height, and socio-\neconomic status. In this chapter, we present three aspects of matrices: how\nto summarize matrices, how matrices can be decomposed, and how these\ndecompositions can be used for matrix approximations.\nWe first consider methods that allow us to describe matrices with just\na few numbers that characterize the overall properties of matrices. We\nwill do this in the sections on determinants (Section 4.1) and eigenval-\nues (Section 4.2) for the important special case of square matrices. These\ncharacteristic numbers have important mathematical consequences and\nallow us to quickly grasp what useful properties a matrix has. From here\nwe will proceed to matrix decomposition methods: An analogy for ma-\ntrix decomposition is the factoring of numbers, such as the factoring of\n21into prime numbers 7{\\textperiodcentered}3. For this reason matrix decomposition is also\noften referred to as matrix factorization . Matrix decompositions are used matrix factorization\nto describe a matrix by means of a different representation using factors\nof interpretable matrices.\nWe will first cover a square-root-like operation for symmetric, positive\ndefinite matrices, the Cholesky decomposition (Section 4.3). From here\nwe will look at two related methods for factorizing matrices into canoni-\ncal forms. The first one is known as matrix diagonalization (Section 4.4),\nwhich allows us to represent the linear mapping using a diagonal trans-\nformation matrix if we choose an appropriate basis. The second method,\nsingular value decomposition (Section 4.5), extends this factorization to\nnon-square matrices, and it is considered one of the fundamental concepts\nin linear algebra. These decompositions are helpful, as matrices represent-\ning numerical data are often very large and hard to analyze. We conclude\nthe chapter with a systematic overview of the types of matrices and the\ncharacteristic properties that",
                    " distinguish them in the form of a matrix tax-\nonomy (Section 4.7).\nThe methods that we cover in this chapter will become important in\n98\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n{\\textcopyright}by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com .\n4.1 Determinant and Trace 99\nFigure 4.1 A mind\nmap of the concepts\nintroduced in this\nchapter, along with\nwhere they are used\nin other parts of the\nbook.Determinant Invertibility Cholesky\nEigenvalues\nEigenvectors Orthogonal matrix Diagonalization\nSVDChapter 6\nProbability\n{\\&} distributions\nChapter 10\nDimensionality\nreductiontests used inused in\nused in determines\nused in\nused in\nused inconstructs used in\nused inused in\nboth subsequent mathematical chapters, such as Chapter 6, but also in\napplied chapters, such as dimensionality reduction in Chapters 10 or den-\nsity estimation in Chapter 11. This chapter`s overall structure is depicted\nin the mind map of Figure 4.1.\n4.1 Determinant and TraceThe determinant\nnotation |A|must\nnot be confused\nwith the absolute\nvalue.Determinants are important concepts in linear algebra. A determinant is\na mathematical object in the analysis and solution of systems of linear\nequations. Determinants are only defined for square matrices A{\\in}Rn{\\texttimes}n,\ni.e., matrices with the same number of rows and columns. In this book,\nwe write the determinant as det(A)or sometimes as |A|so that\ndet(A) =a11a12. . . a 1n\na21a22. . . a 2n\n.........\nan1an2. . . a nn. (4.1)\nThedeterminant of a square matrix A{\\in}Rn{\\texttimes}nis a function that maps A determinant\n{\\textcopyright}",
                    "2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n100 Matrix Decompositions\nonto a real number. Before providing a definition of the determinant for\ngeneral n{\\texttimes}nmatrices, let us have a look at some motivating examples,\nand define determinants for some special matrices.\nExample 4.1 (Testing for Matrix Invertibility)\nLet us begin with exploring if a square matrix Ais invertible (see Sec-\ntion 2.2.2). For the smallest cases, we already know when a matrix\nis invertible. If Ais a1{\\texttimes}1matrix, i.e., it is a scalar number, then\nA=a={\\Rightarrow}A{-}1=1\na. Thus a1\na= 1holds, if and only if a= 0.\nFor2{\\texttimes}2matrices, by the definition of the inverse (Definition 2.3), we\nknow that AA{-}1=I. Then, with (2.24), the inverse of Ais\nA{-}1=1\na11a22{-}a12a21a22{-}a12\n{-}a21a11\n. (4.2)\nHence, Ais invertible if and only if\na11a22{-}a12a21= 0. (4.3)\nThis quantity is the determinant of A{\\in}R2{\\texttimes}2, i.e.,\ndet(A) =a11a12\na21a22=a11a22{-}a12a21. (4.4)\nExample 4.1 points already at the relationship between determinants\nand the existence of inverse matrices. The next theorem states the same\nresult for n{\\texttimes}nmatrices.\nTheorem 4.1. For any square matrix A{\\in}Rn{\\texttimes}nit holds that Ais invertible\nif and only if det(A)= 0.\nWe have explicit (closed-form) expressions for determinants of small\nmatrices in terms of the elements of the matrix. For n= 1,\ndet(A) = det( a11) =a11. (4.5)\nForn=",
                    " 2,\ndet(A) =a11a12\na21a22=a11a22{-}a12a21, (4.6)\nwhich we have observed in the preceding example.\nForn= 3(known as Sarrus` rule),\na11a12a13\na21a22a23\na31a32a33=a11a22a33+a21a32a13+a31a12a23 (4.7)\n{-}a31a22a13{-}a11a32a23{-}a21a12a33.\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.1 Determinant and Trace 101\nFor a memory aid of the product terms in Sarrus` rule, try tracing the\nelements of the triple products in the matrix.\nWe call a square matrix Tanupper-triangular matrix ifTij= 0 for upper-triangular\nmatrix i {>} j , i.e., the matrix is zero below its diagonal. Analogously, we define a\nlower-triangular matrix as a matrix with zeros above its diagonal. For a tri- lower-triangular\nmatrix angular matrix T{\\in}Rn{\\texttimes}n, the determinant is the product of the diagonal\nelements, i.e.,\ndet(T) =nY\ni=1Tii. (4.8)\nThe determinant is\nthe signed volume\nof the parallelepiped\nformed by the\ncolumns of the\nmatrix.\nFigure 4.2 The area\nof the parallelogram\n(shaded region)\nspanned by the\nvectors bandgis\n|det([b,g])|.\nb\ng\nFigure 4.3 The\nvolume of the\nparallelepiped\n(shaded volume)\nspanned by vectors\nr,b,gis\n|det([r,b,g])|.\nb\ngrExample 4.2 (Determinants as Measures of Volume)\nThe notion of a determinant is natural when we consider it as a mapping\nfrom a set of nvectors spanning an object in Rn. It turns out that the de-\nterminant det",
                    "(A)is the signed volume of an n-dimensional parallelepiped\nformed by columns of the matrix A.\nForn= 2, the columns of the matrix form a parallelogram; see Fig-\nure 4.2. As the angle between vectors gets smaller, the area of a parallel-\nogram shrinks, too. Consider two vectors b,gthat form the columns of a\nmatrix A= [b,g]. Then, the absolute value of the determinant of Ais the\narea of the parallelogram with vertices 0,b,g,b+g. In particular, if b,g\nare linearly dependent so that b={\\lambda}gfor some {\\lambda}{\\in}R, they no longer\nform a two-dimensional parallelogram. Therefore, the corresponding area\nis0. On the contrary, if b,gare linearly independent and are multiples of\nthe canonical basis vectors e1,e2then they can be written as b=b\n0\nand\ng=0\ng\n, and the determinant isb0\n0g=bg{-}0 =bg.\nThe sign of the determinant indicates the orientation of the spanning\nvectors b,gwith respect to the standard basis (e1,e2). In our figure, flip-\nping the order to g,bswaps the columns of Aand reverses the orientation\nof the shaded area. This becomes the familiar formula: area =height {\\texttimes}\nlength. This intuition extends to higher dimensions. In R3, we consider\nthree vectors r,b,g{\\in}R3spanning the edges of a parallelepiped, i.e., a\nsolid with faces that are parallel parallelograms (see Figure 4.3). The ab- The sign of the\ndeterminant\nindicates the\norientation of the\nspanning vectors.solute value of the determinant of the 3{\\texttimes}3matrix [r,b,g]is the volume\nof the solid. Thus, the determinant acts as a function that measures the\nsigned volume formed by column vectors composed in a matrix.\nConsider the three linearly independent vectors r,g,b{\\in}R3given as\nr=\n2\n0\n{-}8\n,g=\n6\n1\n0\n,b=\n1\n4\n{-}1",
                    "\n. (4.9)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n102 Matrix Decompositions\nWriting these vectors as the columns of a matrix\nA= [r,g,b] =\n2 6 1\n0 1 4\n{-}8 0 {-}1\n (4.10)\nallows us to compute the desired volume as\nV=|det(A)|= 186 . (4.11)\nComputing the determinant of an n{\\texttimes}nmatrix requires a general algo-\nrithm to solve the cases for n {>}3, which we are going to explore in the fol-\nlowing. Theorem 4.2 below reduces the problem of computing the deter-\nminant of an n{\\texttimes}nmatrix to computing the determinant of (n{-}1){\\texttimes}(n{-}1)\nmatrices. By recursively applying the Laplace expansion (Theorem 4.2),\nwe can therefore compute determinants of n{\\texttimes}nmatrices by ultimately\ncomputing determinants of 2{\\texttimes}2matrices.\nLaplace expansion\nTheorem 4.2 (Laplace Expansion) .Consider a matrix A{\\in}Rn{\\texttimes}n. Then,\nfor all j= 1, . . . , n :\n1. Expansion along column j det(Ak,j)is called\naminor and\n({-}1)k+jdet(Ak,j)\nacofactor .det(A) =nX\nk=1({-}1)k+jakjdet(Ak,j). (4.12)\n2. Expansion along row j\ndet(A) =nX\nk=1({-}1)k+jajkdet(Aj,k). (4.13)\nHereAk,j{\\in}R(n{-}1){\\texttimes}(n{-}1)is the submatrix of Athat we obtain when delet-\ning row kand column j.\nExample 4.3 (Laplace Expansion)\nLet us compute the determinant of\nA=\n1 2 3\n3 1 2\n0 0 1\n (4.14",
                    ")\nusing the Laplace expansion along the first row. Applying (4.13) yields\n1 2 3\n3 1 2\n0 0 1= ({-}1)1+1{\\textperiodcentered}11 2\n0 1\n+ ({-}1)1+2{\\textperiodcentered}23 2\n0 1+ ({-}1)1+3{\\textperiodcentered}33 1\n0 0.(4.15)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.1 Determinant and Trace 103\nWe use (4.6) to compute the determinants of all 2{\\texttimes}2matrices and obtain\ndet(A) = 1(1 {-}0){-}2(3{-}0) + 3(0 {-}0) ={-}5. (4.16)\nFor completeness we can compare this result to computing the determi-\nnant using Sarrus` rule (4.7):\ndet(A) = 1{\\textperiodcentered}1{\\textperiodcentered}1+3{\\textperiodcentered}0{\\textperiodcentered}3+0{\\textperiodcentered}2{\\textperiodcentered}2{-}0{\\textperiodcentered}1{\\textperiodcentered}3{-}1{\\textperiodcentered}0{\\textperiodcentered}2{-}3{\\textperiodcentered}2{\\textperiodcentered}1 = 1{-}6 ={-}5.(4.17)\nForA{\\in}Rn{\\texttimes}nthe determinant exhibits the following properties:\nThe determinant of a matrix product is the product of the corresponding\ndeterminants, det(AB) = det( A)det(B).\nDeterminants are invariant to transposition, i.e., det(A) = det( A{\\top}).\nIfAis regular (invertible), then det(A{-}1) =1\ndet(A).\nSimilar matrices (Definition 2.22) possess the same determinant. There-\nfore, for a linear mapping {\\Phi} :V{\\textrightarrow}Vall transformation matrices A{\\Phi}\nof{\\Phi}have the same determinant. Thus,",
                    " the determinant is invariant to\nthe choice of basis of a linear mapping.\nAdding a multiple of a column/row to another one does not change\ndet(A).\nMultiplication of a column/row with {\\lambda}{\\in}Rscales det(A)by{\\lambda}. In\nparticular, det({\\lambda}A) ={\\lambda}ndet(A).\nSwapping two rows/columns changes the sign of det(A).\nBecause of the last three properties, we can use Gaussian elimination (see\nSection 2.1) to compute det(A)by bringing Ainto row-echelon form.\nWe can stop Gaussian elimination when we have Ain a triangular form\nwhere the elements below the diagonal are all 0. Recall from (4.8) that the\ndeterminant of a triangular matrix is the product of the diagonal elements.\nTheorem 4.3. A square matrix A{\\in}Rn{\\texttimes}nhasdet(A)= 0if and only if\nrk(A) =n. In other words, Ais invertible if and only if it is full rank.\nWhen mathematics was mainly performed by hand, the determinant\ncalculation was considered an essential way to analyze matrix invertibil-\nity. However, contemporary approaches in machine learning use direct\nnumerical methods that superseded the explicit calculation of the deter-\nminant. For example, in Chapter 2, we learned that inverse matrices can\nbe computed by Gaussian elimination. Gaussian elimination can thus be\nused to compute the determinant of a matrix.\nDeterminants will play an important theoretical role for the following\nsections, especially when we learn about eigenvalues and eigenvectors\n(Section 4.2) through the characteristic polynomial.\nDefinition 4.4. Thetrace of a square matrix A{\\in}Rn{\\texttimes}nis defined as trace\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n104 Matrix Decompositions\ntr(A) :=nX\ni=1aii, (4.18)\ni.e. , the trace is the sum of the diagonal elements of A.\nThe trace satisfies the following properties:\ntr(A+B) =tr(A) +tr(B",
                    ")forA,B{\\in}Rn{\\texttimes}n\ntr({\\alpha}A) ={\\alpha}tr(A), {\\alpha}{\\in}RforA{\\in}Rn{\\texttimes}n\ntr(In) =n\ntr(AB) =tr(BA)forA{\\in}Rn{\\texttimes}k,B{\\in}Rk{\\texttimes}n\nIt can be shown that only one function satisfies these four properties to-\ngether {\\textendash} the trace (Gohberg et al., 2012).\nThe properties of the trace of matrix products are more general. Specif-\nically, the trace is invariant under cyclic permutations, i.e., The trace is\ninvariant under\ncyclic permutations. tr(AKL ) =tr(KLA ) (4.19)\nfor matrices A{\\in}Ra{\\texttimes}k,K{\\in}Rk{\\texttimes}l,L{\\in}Rl{\\texttimes}a. This property generalizes to\nproducts of an arbitrary number of matrices. As a special case of (4.19), it\nfollows that for two vectors x,y{\\in}Rn\ntr(xy{\\top}) =tr(y{\\top}x) =y{\\top}x{\\in}R. (4.20)\nGiven a linear mapping {\\Phi} :V{\\textrightarrow}V, where Vis a vector space, we\ndefine the trace of this map by using the trace of matrix representation\nof{\\Phi}. For a given basis of V, we can describe {\\Phi}by means of the transfor-\nmation matrix A. Then the trace of {\\Phi}is the trace of A. For a different\nbasis of V, it holds that the corresponding transformation matrix Bof{\\Phi}\ncan be obtained by a basis change of the form S{-}1ASfor suitable S(see\nSection 2.7.2). For the corresponding trace of {\\Phi}, this means\ntr(B) =tr(S{-}1AS)(4.19)=tr(ASS{-}1) =tr(A). (4.21)\nHence, while matrix representations of linear mappings are basis depen-\ndent the trace of a linear mapping {\\Phi}is independent of the basis",
                    ".\nIn this section, we covered determinants and traces as functions char-\nacterizing a square matrix. Taking together our understanding of determi-\nnants and traces we can now define an important equation describing a\nmatrix Ain terms of a polynomial, which we will use extensively in the\nfollowing sections.\nDefinition 4.5 (Characteristic Polynomial) .For{\\lambda}{\\in}Rand a square ma-\ntrixA{\\in}Rn{\\texttimes}n\npA({\\lambda}) := det( A{-}{\\lambda}I) (4.22a)\n=c0+c1{\\lambda}+c2{\\lambda}2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+cn{-}1{\\lambda}n{-}1+ ({-}1)n{\\lambda}n, (4.22b)\nc0, . . . , c n{-}1{\\in}R, is the characteristic polynomial ofA. In particular, characteristic\npolynomial\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.2 Eigenvalues and Eigenvectors 105\nc0= det( A), (4.23)\ncn{-}1= ({-}1)n{-}1tr(A). (4.24)\nThe characteristic polynomial (4.22a) will allow us to compute eigen-\nvalues and eigenvectors, covered in the next section.\n4.2 Eigenvalues and Eigenvectors\nWe will now get to know a new way to characterize a matrix and its associ-\nated linear mapping. Recall from Section 2.7.1 that every linear mapping\nhas a unique transformation matrix given an ordered basis. We can in-\nterpret linear mappings and their associated transformation matrices by\nperforming an {\\textquotedblleft}eigen{\\textquotedblright} analysis. As we will see, the eigenvalues of a lin- Eigen is a German\nword meaning\n{\\textquotedblleft}characteristic{\\textquotedblright},\n{\\textquotedblleft}self{\\textquotedblright}, or {\\textquotedblleft}own{\\text",
                    "quotedblright}.ear mapping will tell us how a special set of vectors, the eigenvectors, is\ntransformed by the linear mapping.\nDefinition 4.6. LetA{\\in}Rn{\\texttimes}nbe a square matrix. Then {\\lambda}{\\in}Ris an\neigenvalue ofAandx{\\in}Rn{\\textbackslash}{\\{}0{\\}}is the corresponding eigenvector ofAif eigenvalue\neigenvectorAx={\\lambda}x. (4.25)\nWe call (4.25) the eigenvalue equation . eigenvalue equation\nRemark. In the linear algebra literature and software, it is often a conven-\ntion that eigenvalues are sorted in descending order, so that the largest\neigenvalue and associated eigenvector are called the first eigenvalue and\nits associated eigenvector, and the second largest called the second eigen-\nvalue and its associated eigenvector, and so on. However, textbooks and\npublications may have different or no notion of orderings. We do not want\nto presume an ordering in this book if not stated explicitly. {\\diamond}\nThe following statements are equivalent:\n{\\lambda}is an eigenvalue of A{\\in}Rn{\\texttimes}n.\nThere exists an x{\\in}Rn{\\textbackslash}{\\{}0{\\}}withAx={\\lambda}x, or equivalently, (A{-}\n{\\lambda}In)x=0can be solved non-trivially, i.e., x=0.\nrk(A{-}{\\lambda}In){<} n.\ndet(A{-}{\\lambda}In) = 0 .\nDefinition 4.7 (Collinearity and Codirection) .Two vectors that point in\nthe same direction are called codirected . Two vectors are collinear if they codirected\ncollinear point in the same or the opposite direction.\nRemark (Non-uniqueness of eigenvectors) .Ifxis an eigenvector of A\nassociated with eigenvalue {\\lambda}, then for any c{\\in}R{\\textbackslash}{\\{}0{\\}}it holds that cxis\nan eigenvector of Awith the same eigenvalue since\nA(cx) =cAx=c{\\lambda}",
                    "x={\\lambda}(cx). (4.26)\nThus, all vectors that are collinear to xare also eigenvectors of A.\n{\\diamond}\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n106 Matrix Decompositions\nTheorem 4.8. {\\lambda}{\\in}Ris an eigenvalue of A{\\in}Rn{\\texttimes}nif and only if {\\lambda}is a\nroot of the characteristic polynomial pA({\\lambda})ofA.\nDefinition 4.9. Let a square matrix Ahave an eigenvalue {\\lambda}i. The algebraic algebraic\nmultiplicity multiplicity of{\\lambda}iis the number of times the root appears in the character-\nistic polynomial.\nDefinition 4.10 (Eigenspace and Eigenspectrum) .ForA{\\in}Rn{\\texttimes}n, the set\nof all eigenvectors of Aassociated with an eigenvalue {\\lambda}spans a subspace\nofRn, which is called the eigenspace ofAwith respect to {\\lambda}and is denoted eigenspace\nbyE{\\lambda}. The set of all eigenvalues of Ais called the eigenspectrum , or just eigenspectrum\nspectrum , ofA. spectrum\nIf{\\lambda}is an eigenvalue of A{\\in}Rn{\\texttimes}n, then the corresponding eigenspace\nE{\\lambda}is the solution space of the homogeneous system of linear equations\n(A{-}{\\lambda}I)x=0. Geometrically, the eigenvector corresponding to a nonzero\neigenvalue points in a direction that is stretched by the linear mapping.\nThe eigenvalue is the factor by which it is stretched. If the eigenvalue is\nnegative, the direction of the stretching is flipped.\nExample 4.4 (The Case of the Identity Matrix)\nThe identity matrix I{\\in}Rn{\\texttimes}nhas characteristic polynomial pI({\\lambda}) =\ndet(I{-}{\\lambda}I) = (1 {-}{\\lambda})n= 0, which has only one eigenvalue {\\lambda}= 1that",
                    " oc-\ncursntimes. Moreover, Ix={\\lambda}x= 1xholds for all vectors x{\\in}Rn{\\textbackslash}{\\{}0{\\}}.\nBecause of this, the sole eigenspace E1of the identity matrix spans ndi-\nmensions, and all nstandard basis vectors of Rnare eigenvectors of I.\nUseful properties regarding eigenvalues and eigenvectors include the\nfollowing:\nA matrix Aand its transpose A{\\top}possess the same eigenvalues, but not\nnecessarily the same eigenvectors.\nThe eigenspace E{\\lambda}is the null space of A{-}{\\lambda}Isince\nAx={\\lambda}x{\\Leftarrow}{\\Rightarrow}Ax{-}{\\lambda}x=0 (4.27a)\n{\\Leftarrow}{\\Rightarrow} (A{-}{\\lambda}I)x=0{\\Leftarrow}{\\Rightarrow}x{\\in}ker(A{-}{\\lambda}I).(4.27b)\nSimilar matrices (see Definition 2.22) possess the same eigenvalues.\nTherefore, a linear mapping {\\Phi}has eigenvalues that are independent of\nthe choice of basis of its transformation matrix. This makes eigenvalues,\ntogether with the determinant and the trace, key characteristic param-\neters of a linear mapping as they are all invariant under basis change.\nSymmetric, positive definite matrices always have positive, real eigen-\nvalues.\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.2 Eigenvalues and Eigenvectors 107\nExample 4.5 (Computing Eigenvalues, Eigenvectors, and\nEigenspaces)\nLet us find the eigenvalues and eigenvectors of the 2{\\texttimes}2matrix\nA=4 2\n1 3\n. (4.28)\nStep 1: Characteristic Polynomial. From our definition of the eigen-\nvector x=0and eigenvalue {\\lambda}ofA, there will be a vector such that\nAx={\\lambda}x, i.e., (A{-}{",
                    "\\lambda}I)x=0. Since x=0, this requires that the kernel\n(null space) of A{-}{\\lambda}Icontains more elements than just 0. This means\nthatA{-}{\\lambda}Iis not invertible and therefore det(A{-}{\\lambda}I) = 0 . Hence, we\nneed to compute the roots of the characteristic polynomial (4.22a) to find\nthe eigenvalues.\nStep 2: Eigenvalues. The characteristic polynomial is\npA({\\lambda}) = det( A{-}{\\lambda}I) (4.29a)\n= det4 2\n1 3\n{-}{\\lambda}0\n0{\\lambda}\n=4{-}{\\lambda} 2\n1 3 {-}{\\lambda}(4.29b)\n= (4{-}{\\lambda})(3{-}{\\lambda}){-}2{\\textperiodcentered}1. (4.29c)\nWe factorize the characteristic polynomial and obtain\np({\\lambda}) = (4 {-}{\\lambda})(3{-}{\\lambda}){-}2{\\textperiodcentered}1 = 10 {-}7{\\lambda}+{\\lambda}2= (2{-}{\\lambda})(5{-}{\\lambda})(4.30)\ngiving the roots {\\lambda}1= 2and{\\lambda}2= 5.\nStep 3: Eigenvectors and Eigenspaces. We find the eigenvectors that\ncorrespond to these eigenvalues by looking at vectors xsuch that\n4{-}{\\lambda} 2\n1 3 {-}{\\lambda}\nx=0. (4.31)\nFor{\\lambda}= 5we obtain\n4{-}5 2\n1 3 {-}5x1\nx2\n={-}1 2\n1{-}2x1\nx2\n=0. (4.32)\nWe solve this homogeneous system and obtain a solution space\nE5= span[2\n1\n]. (4.33)\nThis eigenspace is one-dimensional as it possesses a single basis vector.\nAnalogously, we find the eigenvector for {\\lambda}= 2by solving the homoge-\nneous system of equations\n4{-}",
                    "2 2\n1 3 {-}2\nx=2 2\n1 1\nx=0. (4.34)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n108 Matrix Decompositions\nThis means any vector x=x1\nx2\n, where x2={-}x1, such as1\n{-}1\n, is an\neigenvector with eigenvalue 2. The corresponding eigenspace is given as\nE2= span[1\n{-}1\n]. (4.35)\nThe two eigenspaces E5andE2in Example 4.5 are one-dimensional\nas they are each spanned by a single vector. However, in other cases\nwe may have multiple identical eigenvalues (see Definition 4.9) and the\neigenspace may have more than one dimension.\nDefinition 4.11. Let{\\lambda}ibe an eigenvalue of a square matrix A. Then the\ngeometric multiplicity of{\\lambda}iis the number of linearly independent eigen- geometric\nmultiplicity vectors associated with {\\lambda}i. In other words, it is the dimensionality of the\neigenspace spanned by the eigenvectors associated with {\\lambda}i.\nRemark. A specific eigenvalue`s geometric multiplicity must be at least\none because every eigenvalue has at least one associated eigenvector. An\neigenvalue`s geometric multiplicity cannot exceed its algebraic multiplic-\nity, but it may be lower. {\\diamond}\nExample 4.6\nThe matrix A=2 1\n0 2\nhas two repeated eigenvalues {\\lambda}1={\\lambda}2= 2and an\nalgebraic multiplicity of 2. The eigenvalue has, however, only one distinct\nunit eigenvector x1=1\n0\nand, thus, geometric multiplicity 1.\nGraphical Intuition in Two Dimensions\nLet us gain some intuition for determinants, eigenvectors, and eigenval-\nues using different linear mappings. Figure 4.4 depicts five transformation\nmatrices A1, . . . ,A5and their impact on a square grid of points, centered\nat the origin: In geometry, the\narea-preserving",
                    "\nproperties of this\ntype of shearing\nparallel to an axis is\nalso known as\nCavalieri`s principle\nof equal areas for\nparallelograms\n(Katz, 2004).A1=1\n20\n0 2\n. The direction of the two eigenvectors correspond to the\ncanonical basis vectors in R2, i.e., to two cardinal axes. The vertical axis\nis extended by a factor of 2(eigenvalue {\\lambda}1= 2), and the horizontal axis\nis compressed by factor1\n2(eigenvalue {\\lambda}2=1\n2). The mapping is area\npreserving ( det(A1) = 1 = 2 {\\textperiodcentered}1\n2).\nA2=11\n2\n0 1\ncorresponds to a shearing mapping , i.e., it shears the\npoints along the horizontal axis to the right if they are on the positive\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.2 Eigenvalues and Eigenvectors 109\nFigure 4.4\nDeterminants and\neigenspaces.\nOverview of five\nlinear mappings and\ntheir associated\ntransformation\nmatrices\nAi{\\in}R2{\\texttimes}2\nprojecting 400\ncolor-coded points\nx{\\in}R2(left\ncolumn) onto target\npoints Aix(right\ncolumn). The\ncentral column\ndepicts the first\neigenvector,\nstretched by its\nassociated\neigenvalue {\\lambda}1, and\nthe second\neigenvector\nstretched by its\neigenvalue {\\lambda}2. Each\nrow depicts the\neffect of one of five\ntransformation\nmatrices Aiwith\nrespect to the\nstandard basis.\ndet(A) = 1.0{\\lambda}1= 2.0\n{\\lambda}2= 0.5\ndet(A) = 1.0{\\lambda}1= 1.0\n{\\lambda}2= 1.0\ndet(A) = 1.0{\\lambda}1= (0.87-0.5j)\n{\\lambda}2= (0.87+0.5j)\ndet(A) = 0.0{\\lambda}1= 0.0\n",
                    "{\\lambda}2= 2.0\ndet(A) = 0.75{\\lambda}1= 0.5\n{\\lambda}2= 1.5\nhalf of the vertical axis, and to the left vice versa. This mapping is area\npreserving ( det(A2) = 1 ). The eigenvalue {\\lambda}1= 1 = {\\lambda}2is repeated\nand the eigenvectors are collinear (drawn here for emphasis in two\nopposite directions). This indicates that the mapping acts only along\none direction (the horizontal axis).\nA3=cos({\\pi}\n6){-}sin({\\pi}\n6)\nsin({\\pi}\n6) cos({\\pi}\n6)\n=1\n2{\\sqrt{}}\n3{-}1\n1{\\sqrt{}}\n3\nThe matrix A3rotates the\npoints by{\\pi}\n6rad = 30{\\textopenbullet}counter-clockwise and has only complex eigen-\nvalues, reflecting that the mapping is a rotation (hence, no eigenvectors\nare drawn). A rotation has to be volume preserving, and so the deter-\nminant is 1. For more details on rotations, we refer to Section 3.9.\nA4=1{-}1\n{-}1 1\nrepresents a mapping in the standard basis that col-\nlapses a two-dimensional domain onto one dimension. Since one eigen-\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n110 Matrix Decompositions\nvalue is 0, the space in direction of the (blue) eigenvector corresponding\nto{\\lambda}1= 0 collapses, while the orthogonal (red) eigenvector stretches\nspace by a factor {\\lambda}2= 2. Therefore, the area of the image is 0.\nA5=11\n21\n21\nis a shear-and-stretch mapping that scales space by 75{\\%}\nsince|det(A5)|=3\n4. It stretches space along the (red) eigenvector\nof{\\lambda}2by a factor 1.5and compresses it along the orthogonal (blue)\neigenvector by a factor 0.5.\nExample 4.7 (",
                    "Eigenspectrum of a Biological Neural Network)\nFigure 4.5\nCaenorhabditis\nelegans neural\nnetwork (Kaiser and\nHilgetag,\n2006).(a) Sym-\nmetrized\nconnectivity matrix;\n(b) Eigenspectrum.\n0 50 100 150 200 250\nneuron index0\n50\n100\n150\n200\n250neuron index\n(a) Connectivity matrix.\n0 100 200\nindex of sorted eigenvalue{-}10{-}50510152025eigenvalue\n (b) Eigenspectrum.\nMethods to analyze and learn from network data are an essential com-\nponent of machine learning methods. The key to understanding networks\nis the connectivity between network nodes, especially if two nodes are\nconnected to each other or not. In data science applications, it is often\nuseful to study the matrix that captures this connectivity data.\nWe build a connectivity/adjacency matrix A{\\in}R277{\\texttimes}277of the complete\nneural network of the worm C.Elegans . Each row/column represents one\nof the 277neurons of this worm`s brain. The connectivity matrix Ahas\na value of aij= 1 if neuron italks to neuron jthrough a synapse, and\naij= 0 otherwise. The connectivity matrix is not symmetric, which im-\nplies that eigenvalues may not be real valued. Therefore, we compute a\nsymmetrized version of the connectivity matrix as Asym:=A+A{\\top}. This\nnew matrix Asymis shown in Figure 4.5(a) and has a nonzero value aijif\nand only if two neurons are connected (white pixels), irrespective of the\ndirection of the connection. In Figure 4.5(b), we show the correspond-\ning eigenspectrum of Asym. The horizontal axis shows the index of the\neigenvalues, sorted in descending order. The vertical axis shows the corre-\nsponding eigenvalue. The S-like shape of this eigenspectrum is typical for\nmany biological neural networks. The underlying mechanism responsible\nfor this is an area of active neuroscience research.\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mm",
                    "l-book.com .\n4.2 Eigenvalues and Eigenvectors 111\nTheorem 4.12. The eigenvectors x1, . . . ,xnof a matrix A{\\in}Rn{\\texttimes}nwithn\ndistinct eigenvalues {\\lambda}1, . . . , {\\lambda} nare linearly independent.\nThis theorem states that eigenvectors of a matrix with ndistinct eigen-\nvalues form a basis of Rn.\nDefinition 4.13. A square matrix A{\\in}Rn{\\texttimes}nisdefective if it possesses defective\nfewer than nlinearly independent eigenvectors.\nA non-defective matrix A{\\in}Rn{\\texttimes}ndoes not necessarily require ndis-\ntinct eigenvalues, but it does require that the eigenvectors form a basis of\nRn. Looking at the eigenspaces of a defective matrix, it follows that the\nsum of the dimensions of the eigenspaces is less than n. Specifically, a de-\nfective matrix has at least one eigenvalue {\\lambda}iwith an algebraic multiplicity\nm {>} 1and a geometric multiplicity of less than m.\nRemark. A defective matrix cannot have ndistinct eigenvalues, as distinct\neigenvalues have linearly independent eigenvectors (Theorem 4.12). {\\diamond}\nTheorem 4.14. Given a matrix A{\\in}Rm{\\texttimes}n, we can always obtain a sym-\nmetric, positive semidefinite matrix S{\\in}Rn{\\texttimes}nby defining\nS:=A{\\top}A. (4.36)\nRemark. Ifrk(A) =n, then S:=A{\\top}Ais symmetric, positive definite.\n{\\diamond}\nUnderstanding why Theorem 4.14 holds is insightful for how we can\nuse symmetrized matrices: Symmetry requires S=S{\\top}, and by insert-\ning (4.36) we obtain S=A{\\top}A=A{\\top}(A{\\top}){\\top}= (A{\\top}A){\\top}=S{\\top}. More-\nover, positive semidefiniteness (Section 3.2.3) requires that x{\\top",
                    "}Sx{\\geqslant}0\nand inserting (4.36) we obtain x{\\top}Sx=x{\\top}A{\\top}Ax= (x{\\top}A{\\top})(Ax) =\n(Ax){\\top}(Ax){\\geqslant}0, because the dot product computes a sum of squares\n(which are themselves non-negative).\nspectral theorem\nTheorem 4.15 (Spectral Theorem) .IfA{\\in}Rn{\\texttimes}nis symmetric, there ex-\nists an orthonormal basis of the corresponding vector space Vconsisting of\neigenvectors of A, and each eigenvalue is real.\nA direct implication of the spectral theorem is that the eigendecompo-\nsition of a symmetric matrix Aexists (with real eigenvalues), and that\nwe can find an ONB of eigenvectors so that A=PDP{\\top}, where Dis\ndiagonal and the columns of Pcontain the eigenvectors.\nExample 4.8\nConsider the matrix\nA=\n3 2 2\n2 3 2\n2 2 3\n. (4.37)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n112 Matrix Decompositions\nThe characteristic polynomial of Ais\npA({\\lambda}) ={-}({\\lambda}{-}1)2({\\lambda}{-}7), (4.38)\nso that we obtain the eigenvalues {\\lambda}1= 1 and{\\lambda}2= 7, where {\\lambda}1is a\nrepeated eigenvalue. Following our standard procedure for computing\neigenvectors, we obtain the eigenspaces\nE1= span[\n{-}1\n1\n0\n\n|{\\{}z{\\}}\n=:x1,\n{-}1\n0\n1\n\n|{\\{}z{\\}}\n=:x2], E 7= span[\n1\n1\n1\n\n|{\\{}z{\\}}\n=:x3]. (4.39)\nWe see that x3is orthogonal to both x1andx2. However, since x{\\top}\n1x2=\n",
                    "1= 0, they are not orthogonal. The spectral theorem (Theorem 4.15)\nstates that there exists an orthogonal basis, but the one we have is not\northogonal. However, we can construct one.\nTo construct such a basis, we exploit the fact that x1,x2are eigenvec-\ntors associated with the same eigenvalue {\\lambda}. Therefore, for any {\\alpha}, {\\beta}{\\in}Rit\nholds that\nA({\\alpha}x1+{\\beta}x2) =Ax1{\\alpha}+Ax2{\\beta}={\\lambda}({\\alpha}x1+{\\beta}x2), (4.40)\ni.e., any linear combination of x1andx2is also an eigenvector of Aas-\nsociated with {\\lambda}. The Gram-Schmidt algorithm (Section 3.8.3) is a method\nfor iteratively constructing an orthogonal/orthonormal basis from a set of\nbasis vectors using such linear combinations. Therefore, even if x1andx2\nare not orthogonal, we can apply the Gram-Schmidt algorithm and find\neigenvectors associated with {\\lambda}1= 1 that are orthogonal to each other\n(and to x3). In our example, we will obtain\nx{'}\n1=\n{-}1\n1\n0\n,x{'}\n2=1\n2\n{-}1\n{-}1\n2\n, (4.41)\nwhich are orthogonal to each other, orthogonal to x3, and eigenvectors of\nAassociated with {\\lambda}1= 1.\nBefore we conclude our considerations of eigenvalues and eigenvectors\nit is useful to tie these matrix characteristics together with the concepts of\nthe determinant and the trace.\nTheorem 4.16. The determinant of a matrix A{\\in}Rn{\\texttimes}nis the product of\nits eigenvalues, i.e.,\ndet(A) =nY\ni=1{\\lambda}i, (4.42)\nwhere {\\lambda}i{\\in}Care (possibly repeated) eigenvalues of A.\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback",
                    ": https://mml-book.com .\n4.2 Eigenvalues and Eigenvectors 113\nFigure 4.6\nGeometric\ninterpretation of\neigenvalues. The\neigenvectors of A\nget stretched by the\ncorresponding\neigenvalues. The\narea of the unit\nsquare changes by\n|{\\lambda}1{\\lambda}2|, the\nperimeter changes\nby a factor of\n1\n2(|{\\lambda}1|+|{\\lambda}2|).x1x2\nv1v2A\nTheorem 4.17. The trace of a matrix A{\\in}Rn{\\texttimes}nis the sum of its eigenval-\nues, i.e.,\ntr(A) =nX\ni=1{\\lambda}i, (4.43)\nwhere {\\lambda}i{\\in}Care (possibly repeated) eigenvalues of A.\nLet us provide a geometric intuition of these two theorems. Consider\na matrix A{\\in}R2{\\texttimes}2that possesses two linearly independent eigenvectors\nx1,x2. For this example, we assume (x1,x2)are an ONB of R2so that they\nare orthogonal and the area of the square they span is 1; see Figure 4.6.\nFrom Section 4.1, we know that the determinant computes the change of\narea of unit square under the transformation A. In this example, we can\ncompute the change of area explicitly: Mapping the eigenvectors using\nAgives us vectors v1=Ax1={\\lambda}1x1andv2=Ax2={\\lambda}2x2, i.e., the\nnew vectors viare scaled versions of the eigenvectors xi, and the scaling\nfactors are the corresponding eigenvalues {\\lambda}i.v1,v2are still orthogonal,\nand the area of the rectangle they span is |{\\lambda}1{\\lambda}2|.\nGiven that x1,x2(in our example) are orthonormal, we can directly\ncompute the perimeter of the unit square as 2(1 + 1) . Mapping the eigen-\nvectors using Acreates a rectangle whose perimeter is 2(|{\\lambda}1|+|{\\lambda}2|).\nTherefore,",
                    " the sum of the absolute values of the eigenvalues tells us how\nthe perimeter of the unit square changes under the transformation matrix\nA.\nExample 4.9 (Google`s PageRank {\\textendash} Webpages as Eigenvectors)\nGoogle uses the eigenvector corresponding to the maximal eigenvalue of\na matrix Ato determine the rank of a page for search. The idea for the\nPageRank algorithm, developed at Stanford University by Larry Page and\nSergey Brin in 1996, was that the importance of any web page can be ap-\nproximated by the importance of pages that link to it. For this, they write\ndown all web sites as a huge directed graph that shows which page links\nto which. PageRank computes the weight (importance) xi{\\geqslant}0of a web\nsiteaiby counting the number of pages pointing to ai. Moreover, PageR-\nank takes into account the importance of the web sites that link to ai. The\nnavigation behavior of a user is then modeled by a transition matrix Aof\nthis graph that tells us with what (click) probability somebody will end up\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n114 Matrix Decompositions\non a different web site. The matrix Ahas the property that for any ini-\ntial rank/importance vector xof a web site the sequence x,Ax,A2x, . . .\nconverges to a vector x{*}. This vector is called the PageRank and satisfies PageRank\nAx{*}=x{*}, i.e., it is an eigenvector (with corresponding eigenvalue 1) of\nA. After normalizing x{*}, such that {\\parallel}x{*}{\\parallel}= 1, we can interpret the entries\nas probabilities. More details and different perspectives on PageRank can\nbe found in the original technical report (Page et al., 1999).\n4.3 Cholesky Decomposition\nThere are many ways to factorize special types of matrices that we en-\ncounter often in machine learning. In the positive real numbers, we have\nthe square-root operation that gives us a decomposition of the number\ninto identical components, e.g., 9 = 3 {\\text",
                    "periodcentered}3. For matrices, we need to be\ncareful that we compute a square-root-like operation on positive quanti-\nties. For symmetric, positive definite matrices (see Section 3.2.3), we can\nchoose from a number of square-root equivalent operations. The Cholesky Cholesky\ndecomposition decomposition /Cholesky factorization provides a square-root equivalent op-\nCholesky\nfactorizationeration on symmetric, positive definite matrices that is useful in practice.\nTheorem 4.18 (Cholesky Decomposition) .A symmetric, positive definite\nmatrix Acan be factorized into a product A=LL{\\top}, where Lis a lower-\ntriangular matrix with positive diagonal elements:\n\na11{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}a1n\n.........\nan1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}ann\n=\nl11{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} 0\n.........\nln1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}lnn\n\nl11{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}ln1\n.........\n0{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}lnn\n. (4.44)\nLis called the Cholesky factor of A, andLis unique. Cholesky factor\nExample 4.10 (Cholesky Factorization)\nConsider a symmetric, positive definite matrix A{\\in}R3{\\texttimes}3. We are inter-\nested in finding its Cholesky factorization A=LL{\\top}, i.e.,\nA=\na11a21a31\na21a22a32\na31a32a33\n=LL{\\top}=\nl110 0\nl21l220\nl31l32l33\n\nl11l21l31\n0l22l32\n0 0 l33\n.(4.45)\nMultiplying out the right-hand side yields\nA=\nl2\n11 l21l11 l31l11\nl21l11 l2\n21+l2\n22 l31l21+l32l22\nl31l11l31l21+l32l22l2\n31",
                    "+l2\n32+l2\n33\n. (4.46)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.4 Eigendecomposition and Diagonalization 115\nComparing the left-hand side of (4.45) and the right-hand side of (4.46)\nshows that there is a simple pattern in the diagonal elements lii:\nl11={\\sqrt{}}a11, l 22=q\na22{-}l2\n21, l 33=q\na33{-}(l2\n31+l2\n32).(4.47)\nSimilarly for the elements below the diagonal ( lij, where i {>} j ), there is\nalso a repeating pattern:\nl21=1\nl11a21, l 31=1\nl11a31, l 32=1\nl22(a32{-}l31l21). (4.48)\nThus, we constructed the Cholesky decomposition for any symmetric, pos-\nitive definite 3{\\texttimes}3matrix. The key realization is that we can backward\ncalculate what the components lijfor the Lshould be, given the values\naijforAand previously computed values of lij.\nThe Cholesky decomposition is an important tool for the numerical\ncomputations underlying machine learning. Here, symmetric positive def-\ninite matrices require frequent manipulation, e.g., the covariance matrix\nof a multivariate Gaussian variable (see Section 6.5) is symmetric, positive\ndefinite. The Cholesky factorization of this covariance matrix allows us to\ngenerate samples from a Gaussian distribution. It also allows us to perform\na linear transformation of random variables, which is heavily exploited\nwhen computing gradients in deep stochastic models, such as the varia-\ntional auto-encoder (Jimenez Rezende et al., 2014; Kingma and Welling,\n2014). The Cholesky decomposition also allows us to compute determi-\nnants very efficiently. Given the Cholesky decomposition A=LL{\\top}, we\nknow that det(A) = det( L) det(L{\\top}) = det( L)2.",
                    " Since Lis a triangular\nmatrix, the determinant is simply the product of its diagonal entries so\nthatdet(A) =Q\nil2\nii. Thus, many numerical software packages use the\nCholesky decomposition to make computations more efficient.\n4.4 Eigendecomposition and Diagonalization\nAdiagonal matrix is a matrix that has value zero on all off-diagonal ele- diagonal matrix\nments, i.e., they are of the form\nD=\nc1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} 0\n.........\n0{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}cn\n. (4.49)\nThey allow fast computation of determinants, powers, and inverses. The\ndeterminant is the product of its diagonal entries, a matrix power Dkis\ngiven by each diagonal element raised to the power k, and the inverse\nD{-}1is the reciprocal of its diagonal elements if all of them are nonzero.\nIn this section, we will discuss how to transform matrices into diagonal\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n116 Matrix Decompositions\nform. This is an important application of the basis change we discussed in\nSection 2.7.2 and eigenvalues from Section 4.2.\nRecall that two matrices A,Dare similar (Definition 2.22) if there ex-\nists an invertible matrix P, such that D=P{-}1AP. More specifically, we\nwill look at matrices Athat are similar to diagonal matrices Dthat con-\ntain the eigenvalues of Aon the diagonal.\nDefinition 4.19 (Diagonalizable) .A matrix A{\\in}Rn{\\texttimes}nisdiagonalizable diagonalizable\nif it is similar to a diagonal matrix, i.e., if there exists an invertible matrix\nP{\\in}Rn{\\texttimes}nsuch that D=P{-}1AP.\nIn the following, we will see that diagonalizing a matrix A{\\in}Rn{\\texttimes}nis\na way of expressing the same linear mapping but in another basis (see\nSection 2.6.1), which will turn out to be a basis",
                    " that consists of the eigen-\nvectors of A.\nLetA{\\in}Rn{\\texttimes}n, let{\\lambda}1, . . . , {\\lambda} nbe a set of scalars, and let p1, . . . ,pnbe a\nset of vectors in Rn. We define P:= [p1, . . . ,pn]and let D{\\in}Rn{\\texttimes}nbe a\ndiagonal matrix with diagonal entries {\\lambda}1, . . . , {\\lambda} n. Then we can show that\nAP=PD (4.50)\nif and only if {\\lambda}1, . . . , {\\lambda} nare the eigenvalues of Aandp1, . . . ,pnare cor-\nresponding eigenvectors of A.\nWe can see that this statement holds because\nAP=A[p1, . . . ,pn] = [Ap1, . . . ,Apn], (4.51)\nPD= [p1, . . . ,pn]\n{\\lambda}1 0\n...\n0 {\\lambda}n\n= [{\\lambda}1p1, . . . , {\\lambda} npn]. (4.52)\nThus, (4.50) implies that\nAp1={\\lambda}1p1 (4.53)\n...\nApn={\\lambda}npn. (4.54)\nTherefore, the columns of Pmust be eigenvectors of A.\nOur definition of diagonalization requires that P{\\in}Rn{\\texttimes}nis invertible,\ni.e.,Phas full rank (Theorem 4.3). This requires us to have nlinearly\nindependent eigenvectors p1, . . . ,pn, i.e., the piform a basis of Rn.\nTheorem 4.20 (Eigendecomposition) .A square matrix A{\\in}Rn{\\texttimes}ncan be\nfactored into\nA=PDP{-}1, (4.55)\nwhere P{\\in}Rn{\\texttimes}nandDis a diagonal matrix whose diagonal entries are\nthe eigenvalues of A, if and only if the eigenvectors of Aform a basis of Rn.\nDraft (2023-12-19) of {\\textquotedblleft}",
                    "Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.4 Eigendecomposition and Diagonalization 117\nFigure 4.7 Intuition\nbehind the\neigendecomposition\nas sequential\ntransformations.\nTop-left to\nbottom-left: P{-}1\nperforms a basis\nchange (here drawn\ninR2and depicted\nas a rotation-like\noperation) from the\nstandard basis into\nthe eigenbasis.\nBottom-left to\nbottom-right: D\nperforms a scaling\nalong the remapped\northogonal\neigenvectors,\ndepicted here by a\ncircle being\nstretched to an\nellipse. Bottom-right\nto top-right: P\nundoes the basis\nchange (depicted as\na reverse rotation)\nand restores the\noriginal coordinate\nframe.\ne1e2\np1p2\np1p2e1e2\np1p2\n{\\lambda}1p1{\\lambda}2p2\ne1e2\nAe 1Ae 2P{-}1\nDPA\nTheorem 4.20 implies that only non-defective matrices can be diagonal-\nized and that the columns of Pare the neigenvectors of A. For symmetric\nmatrices we can obtain even stronger outcomes for the eigenvalue decom-\nposition.\nTheorem 4.21. A symmetric matrix S{\\in}Rn{\\texttimes}ncan always be diagonalized.\nTheorem 4.21 follows directly from the spectral theorem 4.15. More-\nover, the spectral theorem states that we can find an ONB of eigenvectors\nofRn. This makes Pan orthogonal matrix so that D=P{\\top}AP.\nRemark. The Jordan normal form of a matrix offers a decomposition that\nworks for defective matrices (Lang, 1987) but is beyond the scope of this\nbook. {\\diamond}\nGeometric Intuition for the Eigendecomposition\nWe can interpret the eigendecomposition of a matrix as follows (see also\nFigure 4.7): Let Abe the transformation matrix of a linear mapping with\nrespect to the standard basis ei(blue arrows). P{-}1performs a basis\nchange from the standard",
                    " basis into the eigenbasis. Then, the diagonal\nDscales the vectors along these axes by the eigenvalues {\\lambda}i. Finally, P\ntransforms these scaled vectors back into the standard/canonical coordi-\nnates yielding {\\lambda}ipi.\nExample 4.11 (Eigendecomposition)\nLet us compute the eigendecomposition of A=1\n25{-}2\n{-}2 5\n.\nStep 1: Compute eigenvalues and eigenvectors. The characteristic\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n118 Matrix Decompositions\npolynomial of Ais\ndet(A{-}{\\lambda}I) = det5\n2{-}{\\lambda}{-}1\n{-}15\n2{-}{\\lambda}\n(4.56a)\n= (5\n2{-}{\\lambda})2{-}1 ={\\lambda}2{-}5{\\lambda}+21\n4= ({\\lambda}{-}7\n2)({\\lambda}{-}3\n2). (4.56b)\nTherefore, the eigenvalues of Aare{\\lambda}1=7\n2and{\\lambda}2=3\n2(the roots of the\ncharacteristic polynomial), and the associated (normalized) eigenvectors\nare obtained via\nAp1=7\n2p1,Ap2=3\n2p2. (4.57)\nThis yields\np1=1{\\sqrt{}}\n21\n{-}1\n,p2=1{\\sqrt{}}\n21\n1\n. (4.58)\nStep 2: Check for existence. The eigenvectors p1,p2form a basis of R2.\nTherefore, Acan be diagonalized.\nStep 3: Construct the matrix Pto diagonalize A.We collect the eigen-\nvectors of AinPso that\nP= [p1,p2] =1{\\sqrt{}}\n21 1\n{-}1 1\n. (4.59)\nWe then obtain\nP{-}1AP=7\n20\n03\n2\n=D. (4.60)\nEquival",
                    "ently, we get (exploiting that P{-}1=P{\\top}since the eigenvectors Figure 4.7 visualizes\nthe\neigendecomposition\nofA=5{-}2\n{-}2 5\nas a sequence of\nlinear\ntransformations.p1andp2in this example form an ONB)\n1\n25{-}2\n{-}2 5\n|{\\{}z{\\}}\nA=1{\\sqrt{}}\n21 1\n{-}1 1\n|{\\{}z{\\}}\nP7\n20\n03\n2\n|{\\{}z{\\}}\nD1{\\sqrt{}}\n21{-}1\n1 1\n|{\\{}z{\\}}\nP{-}1. (4.61)\nDiagonal matrices Dcan efficiently be raised to a power. Therefore,\nwe can find a matrix power for a matrix A{\\in}Rn{\\texttimes}nvia the eigenvalue\ndecomposition (if it exists) so that\nAk= (PDP{-}1)k=PDkP{-}1. (4.62)\nComputing Dkis efficient because we apply this operation individually\nto any diagonal element.\nAssume that the eigendecomposition A=PDP{-}1exists. Then,\ndet(A) = det( PDP{-}1) = det( P) det(D) det(P{-}1) (4.63a)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.5 Singular Value Decomposition 119\n= det( D) =Y\nidii (4.63b)\nallows for an efficient computation of the determinant of A.\nThe eigenvalue decomposition requires square matrices. It would be\nuseful to perform a decomposition on general matrices. In the next sec-\ntion, we introduce a more general matrix decomposition technique, the\nsingular value decomposition.\n4.5 Singular Value Decomposition\nThe singular value decomposition (SVD) of a matrix is a central matrix\ndecomposition method in linear algebra. It has been referred to as the",
                    "\n{\\textquotedblleft}fundamental theorem of linear algebra{\\textquotedblright} (Strang, 1993) because it can be\napplied to all matrices, not only to square matrices, and it always exists.\nMoreover, as we will explore in the following, the SVD of a matrix A,\nwhich represents a linear mapping {\\Phi} :V{\\textrightarrow}W, quantifies the change\nbetween the underlying geometry of these two vector spaces. We recom-\nmend the work by Kalman (1996) and Roy and Banerjee (2014) for a\ndeeper overview of the mathematics of the SVD.\nSVD theorem\nTheorem 4.22 (SVD Theorem) .LetA{\\in}Rm{\\texttimes}nbe a rectangular matrix of\nrankr{\\in}[0,min(m, n)]. The SVD of Ais a decomposition of the form SVD\nsingular value\ndecomposition\n= U A V{\\top}{\\Sigma}mn\nmm\nmn\nnn\n(4.64)\nwith an orthogonal matrix U{\\in}Rm{\\texttimes}mwith column vectors ui,i= 1, . . . , m ,\nand an orthogonal matrix V{\\in}Rn{\\texttimes}nwith column vectors vj,j= 1, . . . , n .\nMoreover, {\\Sigma}is an m{\\texttimes}nmatrix with {\\Sigma}ii={\\sigma}i{\\geqslant}0and{\\Sigma}ij= 0, i=j.\nThe diagonal entries {\\sigma}i,i= 1, . . . , r , of{\\Sigma}are called the singular values ,singular values\nuiare called the left-singular vectors , andvjare called the right-singular left-singular vectors\nright-singular\nvectorsvectors . By convention, the singular values are ordered, i.e., {\\sigma}1{\\geqslant}{\\sigma}2{\\geqslant}\n{\\sigma}r{\\geqslant}0.\nThesingular value matrix {\\Sigma}is unique, but it requires some attention. singular value\nmatrix Observe that the {\\Sigma}{\\in}Rm{\\texttimes}nis rectangular. In particular",
                    ", {\\Sigma}is of the same\nsize as A. This means that {\\Sigma}has a diagonal submatrix that contains the\nsingular values and needs additional zero padding. Specifically, if m {>} n ,\nthen the matrix {\\Sigma}has diagonal structure up to row nand then consists of\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n120 Matrix Decompositions\nFigure 4.8 Intuition\nbehind the SVD of a\nmatrix A{\\in}R3{\\texttimes}2\nas sequential\ntransformations.\nTop-left to\nbottom-left: V{\\top}\nperforms a basis\nchange in R2.\nBottom-left to\nbottom-right: {\\Sigma}\nscales and maps\nfromR2toR3. The\nellipse in the\nbottom-right lives in\nR3. The third\ndimension is\northogonal to the\nsurface of the\nelliptical disk.\nBottom-right to\ntop-right: U\nperforms a basis\nchange within R3.v2\nv1\n {\\sigma}2u2\n{\\sigma}1u1\ne2\ne1{\\sigma}2e2\n{\\sigma}1e1A\nV{\\top}\n{\\Sigma}U\n0{\\top}row vectors from n+ 1tombelow so that\n{\\Sigma}=\n{\\sigma}10 0\n0...0\n0 0 {\\sigma}n\n0. . . 0\n......\n0. . . 0\n. (4.65)\nIfm {<} n , the matrix {\\Sigma}has a diagonal structure up to column mand\ncolumns that consist of 0from m+ 1ton:\n{\\Sigma}=\n{\\sigma}10 0 0 . . .0\n0...0......\n0 0 {\\sigma}m0. . .0\n. (4.66)\nRemark. The SVD exists for any matrix A{\\in}Rm{\\texttimes}n. {\\diamond}\n4.5.1 Geometric Intuitions for the SVD\nThe SVD offers geometric intuitions to describe a transformation matrix\nA. In the following, we will discuss",
                    " the SVD as sequential linear trans-\nformations performed on the bases. In Example 4.12, we will then apply\ntransformation matrices of the SVD to a set of vectors in R2, which allows\nus to visualize the effect of each transformation more clearly.\nThe SVD of a matrix can be interpreted as a decomposition of a corre-\nsponding linear mapping (recall Section 2.7.1) {\\Phi} :Rn{\\textrightarrow}Rminto three\noperations; see Figure 4.8. The SVD intuition follows superficially a simi-\nlar structure to our eigendecomposition intuition, see Figure 4.7: Broadly\nspeaking, the SVD performs a basis change via V{\\top}followed by a scal-\ning and augmentation (or reduction) in dimensionality via the singular\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.5 Singular Value Decomposition 121\nvalue matrix {\\Sigma}. Finally, it performs a second basis change via U. The SVD\nentails a number of important details and caveats, which is why we will\nreview our intuition in more detail. It is useful to review\nbasis changes\n(Section 2.7.2),\northogonal matrices\n(Definition 3.8) and\northonormal bases\n(Section 3.5).Assume we are given a transformation matrix of a linear mapping {\\Phi} :\nRn{\\textrightarrow}Rmwith respect to the standard bases BandCofRnandRm,\nrespectively. Moreover, assume a second basis {\\textasciitilde}BofRnand{\\textasciitilde}CofRm. Then\n1. The matrix Vperforms a basis change in the domain Rnfrom {\\textasciitilde}B(rep-\nresented by the red and orange vectors v1andv2in the top-left of Fig-\nure 4.8) to the standard basis B.V{\\top}=V{-}1performs a basis change\nfrom Bto{\\textasciitilde}B. The red and orange vectors are now aligned with the\ncanonical basis in the bottom-left of Figure 4.8.",
                    "\n2. Having changed the coordinate system to {\\textasciitilde}B,{\\Sigma}scales the new coordi-\nnates by the singular values {\\sigma}i(and adds or deletes dimensions), i.e.,\n{\\Sigma}is the transformation matrix of {\\Phi}with respect to {\\textasciitilde}Band {\\textasciitilde}C, rep-\nresented by the red and orange vectors being stretched and lying in\nthee1-e2plane, which is now embedded in a third dimension in the\nbottom-right of Figure 4.8.\n3.Uperforms a basis change in the codomain Rmfrom {\\textasciitilde}Cinto the canoni-\ncal basis of Rm, represented by a rotation of the red and orange vectors\nout of the e1-e2plane. This is shown in the top-right of Figure 4.8.\nThe SVD expresses a change of basis in both the domain and codomain.\nThis is in contrast with the eigendecomposition that operates within the\nsame vector space, where the same basis change is applied and then un-\ndone. What makes the SVD special is that these two different bases are\nsimultaneously linked by the singular value matrix {\\Sigma}.\nExample 4.12 (Vectors and the SVD)\nConsider a mapping of a square grid of vectors X {\\in}R2that fit in a box of\nsize2{\\texttimes}2centered at the origin. Using the standard basis, we map these\nvectors using\nA=\n1{-}0.8\n0 1\n1 0\n=U{\\Sigma}V{\\top}(4.67a)\n=\n{-}0.79 0 {-}0.62\n0.38{-}0.78{-}0.49\n{-}0.48{-}0.62 0 .62\n\n1.62 0\n0 1 .0\n0 0\n{-}0.78 0 .62\n{-}0.62{-}0.78\n.(4.67b)\nWe start with a set of vectors X(colored dots; see top-left panel of Fig-\nure 4.9) arranged in a grid. We then apply V{\\top}{\\in}R2{\\texttimes}2",
                    ", which rotates X.\nThe rotated vectors are shown in the bottom-left panel of Figure 4.9. We\nnow map these vectors using the singular value matrix {\\Sigma}to the codomain\nR3(see the bottom-right panel in Figure 4.9). Note that all vectors lie in\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n122 Matrix Decompositions\nthex1-x2plane. The third coordinate is always 0. The vectors in the x1-x2\nplane have been stretched by the singular values.\nThe direct mapping of the vectors XbyAto the codomain R3equals\nthe transformation of XbyU{\\Sigma}V{\\top}, where Uperforms a rotation within\nthe codomain R3so that the mapped vectors are no longer restricted to\nthex1-x2plane; they still are on a plane as shown in the top-right panel\nof Figure 4.9.\nFigure 4.9 SVD and\nmapping of vectors\n(represented by\ndiscs). The panels\nfollow the same\nanti-clockwise\nstructure of\nFigure 4.8.\n{-}1.5{-}1.0{-}0.5 0.0 0.5 1.0 1.5\nx1{-}1.5{-}1.0{-}0.50.00.51.01.5x2\nx1-1.5-0.5\n0.5\n1.5x2\n-1.5-0.50.51.5x3\n-1.0-0.50.00.51.0\n{-}1.5{-}1.0{-}0.5 0.0 0.5 1.0 1.5\nx1{-}1.5{-}1.0{-}0.50.00.51.01.5x2\nx1-1.5-0.50.51.5x2\n-1.5-0.50.51.5x3\n0\n4.5.2 Construction of the SVD\nWe will next discuss why the SVD exists and show how to compute it\nin detail. The SVD of a",
                    " general matrix shares some similarities with the\neigendecomposition of a square matrix.\nRemark. Compare the eigendecomposition of an SPD matrix\nS=S{\\top}=PDP{\\top}(4.68)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.5 Singular Value Decomposition 123\nwith the corresponding SVD\nS=U{\\Sigma}V{\\top}. (4.69)\nIf we set\nU=P=V,D={\\Sigma}, (4.70)\nwe see that the SVD of SPD matrices is their eigendecomposition. {\\diamond}\nIn the following, we will explore why Theorem 4.22 holds and how\nthe SVD is constructed. Computing the SVD of A{\\in}Rm{\\texttimes}nis equivalent\nto finding two sets of orthonormal bases U= (u1, . . . ,um)andV=\n(v1, . . . ,vn)of the codomain Rmand the domain Rn, respectively. From\nthese ordered bases, we will construct the matrices UandV.\nOur plan is to start with constructing the orthonormal set of right-\nsingular vectors v1, . . . ,vn{\\in}Rn. We then construct the orthonormal set\nof left-singular vectors u1, . . . ,um{\\in}Rm. Thereafter, we will link the two\nand require that the orthogonality of the viis preserved under the trans-\nformation of A. This is important because we know that the images Avi\nform a set of orthogonal vectors. We will then normalize these images by\nscalar factors, which will turn out to be the singular values.\nLet us begin with constructing the right-singular vectors. The spectral\ntheorem (Theorem 4.15) tells us that the eigenvectors of a symmetric\nmatrix form an ONB, which also means it can be diagonalized. More-\nover, from Theorem 4.14 we can always construct a symmetric, positive\nsemidefinite matrix A{\\top}A{\\in}Rn{\\texttimes}nfrom any rectangular",
                    " matrix A{\\in}\nRm{\\texttimes}n. Thus, we can always diagonalize A{\\top}Aand obtain\nA{\\top}A=PDP{\\top}=P\n{\\lambda}1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} 0\n.........\n0{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}{\\lambda}n\nP{\\top}, (4.71)\nwhere Pis an orthogonal matrix, which is composed of the orthonormal\neigenbasis. The {\\lambda}i{\\geqslant}0are the eigenvalues of A{\\top}A. Let us assume the\nSVD of Aexists and inject (4.64) into (4.71). This yields\nA{\\top}A= (U{\\Sigma}V{\\top}){\\top}(U{\\Sigma}V{\\top}) =V{\\Sigma}{\\top}U{\\top}U{\\Sigma}V{\\top}, (4.72)\nwhere U,Vare orthogonal matrices. Therefore, with U{\\top}U=Iwe ob-\ntain\nA{\\top}A=V{\\Sigma}{\\top}{\\Sigma}V{\\top}=V\n{\\sigma}2\n10 0\n0...0\n0 0 {\\sigma}2\nn\nV{\\top}. (4.73)\nComparing now (4.71) and (4.73), we identify\nV{\\top}=P{\\top}, (4.74)\n{\\sigma}2\ni={\\lambda}i. (4.75)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n124 Matrix Decompositions\nTherefore, the eigenvectors of A{\\top}Athat compose Pare the right-singular\nvectors VofA(see (4.74)). The eigenvalues of A{\\top}Aare the squared\nsingular values of {\\Sigma}(see (4.75)).\nTo obtain the left-singular vectors U, we follow a similar procedure.\nWe start by computing the SVD of the symmetric matrix AA{\\top}{\\in}Rm{\\texttimes}m\n(instead of",
                    " the previous A{\\top}A{\\in}Rn{\\texttimes}n). The SVD of Ayields\nAA{\\top}= (U{\\Sigma}V{\\top})(U{\\Sigma}V{\\top}){\\top}=U{\\Sigma}V{\\top}V{\\Sigma}{\\top}U{\\top}(4.76a)\n=U\n{\\sigma}2\n10 0\n0...0\n0 0 {\\sigma}2\nm\nU{\\top}. (4.76b)\nThe spectral theorem tells us that AA{\\top}=SDS{\\top}can be diagonalized\nand we can find an ONB of eigenvectors of AA{\\top}, which are collected in\nS. The orthonormal eigenvectors of AA{\\top}are the left-singular vectors U\nand form an orthonormal basis in the codomain of the SVD.\nThis leaves the question of the structure of the matrix {\\Sigma}. Since AA{\\top}\nandA{\\top}Ahave the same nonzero eigenvalues (see page 106), the nonzero\nentries of the {\\Sigma}matrices in the SVD for both cases have to be the same.\nThe last step is to link up all the parts we touched upon so far. We have\nan orthonormal set of right-singular vectors in V. To finish the construc-\ntion of the SVD, we connect them with the orthonormal vectors U. To\nreach this goal, we use the fact the images of the viunder Ahave to be\northogonal, too. We can show this by using the results from Section 3.4.\nWe require that the inner product between AviandAvjmust be 0for\ni=j. For any two orthogonal eigenvectors vi,vj,i=j, it holds that\n(Avi){\\top}(Avj) =v{\\top}\ni(A{\\top}A)vj=v{\\top}\ni({\\lambda}jvj) ={\\lambda}jv{\\top}\nivj= 0. (4.77)\nFor the case m{\\geqslant}r, it holds that {\\{}Av1, . . . ,Avr{\\}}is a basis of an r-\ndimensional subspace of",
                    " Rm.\nTo complete the SVD construction, we need left-singular vectors that\nare ortho normal : We normalize the images of the right-singular vectors\nAviand obtain\nui:=Avi\n{\\parallel}Avi{\\parallel}=1{\\sqrt{}}{\\lambda}iAvi=1\n{\\sigma}iAvi, (4.78)\nwhere the last equality was obtained from (4.75) and (4.76b), showing\nus that the eigenvalues of AA{\\top}are such that {\\sigma}2\ni={\\lambda}i.\nTherefore, the eigenvectors of A{\\top}A, which we know are the right-\nsingular vectors vi, and their normalized images under A, the left-singular\nvectors ui, form two self-consistent ONBs that are connected through the\nsingular value matrix {\\Sigma}.\nLet us rearrange (4.78) to obtain the singular value equation singular value\nequation\nAvi={\\sigma}iui, i= 1, . . . , r . (4.79)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.5 Singular Value Decomposition 125\nThis equation closely resembles the eigenvalue equation (4.25), but the\nvectors on the left- and the right-hand sides are not the same.\nForn {<} m , (4.79) holds only for i{\\leqslant}n, but (4.79) says nothing about\ntheuifori {>} n . However, we know by construction that they are or-\nthonormal. Conversely, for m {<} n , (4.79) holds only for i{\\leqslant}m. Fori {>} m ,\nwe have Avi=0and we still know that the viform an orthonormal set.\nThis means that the SVD also supplies an orthonormal basis of the kernel\n(null space) of A, the set of vectors xwithAx=0(see Section 2.7.3).\nConcatenating the vias the columns of Vand the uias the columns of\nUyields\n",
                    "AV=U{\\Sigma}, (4.80)\nwhere {\\Sigma}has the same dimensions as Aand a diagonal structure for rows\n1, . . . , r . Hence, right-multiplying with V{\\top}yields A=U{\\Sigma}V{\\top}, which is\nthe SVD of A.\nExample 4.13 (Computing the SVD)\nLet us find the singular value decomposition of\nA=1 0 1\n{-}2 1 0\n. (4.81)\nThe SVD requires us to compute the right-singular vectors vj, the singular\nvalues {\\sigma}k, and the left-singular vectors ui.\nStep 1: Right-singular vectors as the eigenbasis of A{\\top}A.\nWe start by computing\nA{\\top}A=\n1{-}2\n0 1\n1 0\n1 0 1\n{-}2 1 0\n=\n5{-}2 1\n{-}2 1 0\n1 0 1\n. (4.82)\nWe compute the singular values and right-singular vectors vjthrough\nthe eigenvalue decomposition of A{\\top}A, which is given as\nA{\\top}A=\n5{\\sqrt{}}\n300{-}1{\\sqrt{}}\n6\n{-}2{\\sqrt{}}\n301{\\sqrt{}}\n5{-}2{\\sqrt{}}\n6\n1{\\sqrt{}}\n302{\\sqrt{}}\n51{\\sqrt{}}\n6\n\n6 0 0\n0 1 0\n0 0 0\n\n5{\\sqrt{}}\n30{-}2{\\sqrt{}}\n301{\\sqrt{}}\n30\n01{\\sqrt{}}\n52{\\sqrt{}}\n5\n{-}1{\\sqrt{}}\n6{-}2{\\sqrt{}}\n61{\\sqrt{}}\n6\n=PDP{\\top},\n(4.83)\nand we obtain the right-singular vectors as the columns of Pso that\nV=P=\n5{\\sqrt{}}\n300{-}1{\\sqrt{}}\n6\n{-}2{\\sqrt{}}\n301{\\sqrt{}}\n5{-}2{\\sqrt{}}\n",
                    "6\n1{\\sqrt{}}\n302{\\sqrt{}}\n51{\\sqrt{}}\n6\n. (4.84)\nStep 2: Singular-value matrix.\nAs the singular values {\\sigma}iare the square roots of the eigenvalues of\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n126 Matrix Decompositions\nA{\\top}Awe obtain them straight from D. Since rk(A) = 2 , there are only\ntwo nonzero singular values: {\\sigma}1={\\sqrt{}}\n6and{\\sigma}2= 1. The singular value\nmatrix must be the same size as A, and we obtain\n{\\Sigma}={\\sqrt{}}\n6 0 0\n0 1 0\n. (4.85)\nStep 3: Left-singular vectors as the normalized image of the right-\nsingular vectors.\nWe find the left-singular vectors by computing the image of the right-\nsingular vectors under Aand normalizing them by dividing them by their\ncorresponding singular value. We obtain\nu1=1\n{\\sigma}1Av1=1{\\sqrt{}}\n61 0 1\n{-}2 1 0\n5{\\sqrt{}}\n30\n{-}2{\\sqrt{}}\n30\n1{\\sqrt{}}\n30\n={''}\n1{\\sqrt{}}\n5\n{-}2{\\sqrt{}}\n5{\\#}\n, (4.86)\nu2=1\n{\\sigma}2Av2=1\n11 0 1\n{-}2 1 0\n0\n1{\\sqrt{}}\n5\n2{\\sqrt{}}\n5\n={''}\n2{\\sqrt{}}\n5\n1{\\sqrt{}}\n5{\\#}\n, (4.87)\nU= [u1,u2] =1{\\sqrt{}}\n51 2\n{-}2 1\n. (4.88)\nNote that on a computer the approach illustrated here has poor numerical\nbehavior, and the SVD of Ais normally computed without resorting to the\neigenvalue decomposition of A{\\top}A.\n4.5.",
                    "3 Eigenvalue Decomposition vs. Singular Value Decomposition\nLet us consider the eigendecomposition A=PDP{-}1and the SVD A=\nU{\\Sigma}V{\\top}and review the core elements of the past sections.\nThe SVD always exists for any matrix Rm{\\texttimes}n. The eigendecomposition is\nonly defined for square matrices Rn{\\texttimes}nand only exists if we can find a\nbasis of eigenvectors of Rn.\nThe vectors in the eigendecomposition matrix Pare not necessarily\northogonal, i.e., the change of basis is not a simple rotation and scaling.\nOn the other hand, the vectors in the matrices UandVin the SVD are\northonormal, so they do represent rotations.\nBoth the eigendecomposition and the SVD are compositions of three\nlinear mappings:\n1. Change of basis in the domain\n2. Independent scaling of each new basis vector and mapping from do-\nmain to codomain\n3. Change of basis in the codomain\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.5 Singular Value Decomposition 127\nFigure 4.10 Movie\nratings of three\npeople for four\nmovies and its SVD\ndecomposition. 5 4 1\n5 5 0\n0 0 5\n1 0 4\n\nAli\nBeatrix\nChandra\nStar Wars\nBlade Runner\nAmelie\nDelicatessen={-}0.6710 0.0236 0.4647 {-}0.5774\n{-}0.7197 0.2054 {-}0.4759 0.4619\n{-}0.0939 {-}0.7705 {-}0.5268 {-}0.3464\n{-}0.1515 {-}0.6030 0.5293 {-}0.5774\n\n\n9.6438 0 0\n06.3639 0\n0 00.7056\n0 0 0\n\n\n{-}0.7367 {-}0.6515 {-}0.1811\n0.08",
                    "52 0.1762 {-}0.9807\n0.6708 {-}0.7379 {-}0.0743\n\n\nA key difference between the eigendecomposition and the SVD is that\nin the SVD, domain and codomain can be vector spaces of different\ndimensions.\nIn the SVD, the left- and right-singular vector matrices UandVare\ngenerally not inverse of each other (they perform basis changes in dif-\nferent vector spaces). In the eigendecomposition, the basis change ma-\ntricesPandP{-}1are inverses of each other.\nIn the SVD, the entries in the diagonal matrix {\\Sigma}are all real and non-\nnegative, which is not generally true for the diagonal matrix in the\neigendecomposition.\nThe SVD and the eigendecomposition are closely related through their\nprojections\n{\\textendash}The left-singular vectors of Aare eigenvectors of AA{\\top}\n{\\textendash}The right-singular vectors of Aare eigenvectors of A{\\top}A.\n{\\textendash}The nonzero singular values of Aare the square roots of the nonzero\neigenvalues of both AA{\\top}andA{\\top}A.\nFor symmetric matrices A{\\in}Rn{\\texttimes}n, the eigenvalue decomposition and\nthe SVD are one and the same, which follows from the spectral theo-\nrem 4.15.\nExample 4.14 (Finding Structure in Movie Ratings and Consumers)\nLet us add a practical interpretation of the SVD by analyzing data on\npeople and their preferred movies. Consider three viewers (Ali, Beatrix,\nChandra) rating four different movies ( Star Wars ,Blade Runner ,Amelie ,\nDelicatessen ). Their ratings are values between 0(worst) and 5(best) and\nencoded in a data matrix A{\\in}R4{\\texttimes}3as shown in Figure 4.10. Each row\nrepresents a movie and each column a user. Thus, the column vectors of\nmovie ratings, one for each viewer, are xAli,xBeatrix ,xChandra .\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal",
                    ", C. S. Ong. Published by Cambridge University Press (2020).\n128 Matrix Decompositions\nFactoring Ausing the SVD offers us a way to capture the relationships\nof how people rate movies, and especially if there is a structure linking\nwhich people like which movies. Applying the SVD to our data matrix A\nmakes a number of assumptions:\n1. All viewers rate movies consistently using the same linear mapping.\n2. There are no errors or noise in the ratings.\n3. We interpret the left-singular vectors uias stereotypical movies and\nthe right-singular vectors vjas stereotypical viewers.\nWe then make the assumption that any viewer`s specific movie preferences\ncan be expressed as a linear combination of the vj. Similarly, any movie`s\nlike-ability can be expressed as a linear combination of the ui. Therefore,\na vector in the domain of the SVD can be interpreted as a viewer in the\n{\\textquotedblleft}space{\\textquotedblright} of stereotypical viewers, and a vector in the codomain of the SVD\ncorrespondingly as a movie in the {\\textquotedblleft}space{\\textquotedblright} of stereotypical movies. Let us These two {\\textquotedblleft}spaces{\\textquotedblright}\nare only\nmeaningfully\nspanned by the\nrespective viewer\nand movie data if\nthe data itself covers\na sufficient diversity\nof viewers and\nmovies.inspect the SVD of our movie-user matrix. The first left-singular vector u1\nhas large absolute values for the two science fiction movies and a large\nfirst singular value (red shading in Figure 4.10). Thus, this groups a type\nof users with a specific set of movies (science fiction theme). Similarly, the\nfirst right-singular v1shows large absolute values for Ali and Beatrix, who\ngive high ratings to science fiction movies (green shading in Figure 4.10).\nThis suggests that v1reflects the notion of a science fiction lover.\nSimilarly, u2, seems to capture a French art house film theme, and v2in-\ndicates that Chandra is close to an idealized lover of such movies. An ide-\nalized science fiction lover is a purist and only loves science fiction movies,\nso a science fiction lover v1gives a rating of zero to everything but science\nfiction themed{\\textem",
                    "dash}this logic is implied by the diagonal substructure for the\nsingular value matrix {\\Sigma}. A specific movie is therefore represented by how\nit decomposes (linearly) into its stereotypical movies. Likewise, a person\nwould be represented by how they decompose (via linear combination)\ninto movie themes.\nIt is worth to briefly discuss SVD terminology and conventions, as there\nare different versions used in the literature. While these differences can\nbe confusing, the mathematics remains invariant to them.\nFor convenience in notation and abstraction, we use an SVD notation\nwhere the SVD is described as having two square left- and right-singular\nvector matrices, but a non-square singular value matrix. Our defini-\ntion (4.64) for the SVD is sometimes called the full SVD . full SVD\nSome authors define the SVD a bit differently and focus on square sin-\ngular matrices. Then, for A{\\in}Rm{\\texttimes}nandm{\\geqslant}n,\nA\nm{\\texttimes}n=U\nm{\\texttimes}n{\\Sigma}\nn{\\texttimes}nV{\\top}\nn{\\texttimes}n. (4.89)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n4.6 Matrix Approximation 129\nSometimes this formulation is called the reduced SVD (e.g., Datta (2010)) reduced SVD\northeSVD (e.g., Press et al. (2007)). This alternative format changes\nmerely how the matrices are constructed but leaves the mathematical\nstructure of the SVD unchanged. The convenience of this alternative\nformulation is that {\\Sigma}is diagonal, as in the eigenvalue decomposition.\nIn Section 4.6, we will learn about matrix approximation techniques\nusing the SVD, which is also called the truncated SVD . truncated SVD\nIt is possible to define the SVD of a rank- rmatrix Aso that Uis an\nm{\\texttimes}rmatrix, {\\Sigma}a diagonal matrix r{\\texttimes}r, and Vanr{\\texttimes}nmatrix.\nThis construction is very similar to our definition, and ensures that the\ndiagonal",
                    " matrix {\\Sigma}has only nonzero entries along the diagonal. The\nmain convenience of this alternative notation is that {\\Sigma}is diagonal, as\nin the eigenvalue decomposition.\nA restriction that the SVD for Aonly applies to m{\\texttimes}nmatrices with\nm {>} n is practically unnecessary. When m {<} n , the SVD decomposition\nwill yield {\\Sigma}with more zero columns than rows and, consequently, the\nsingular values {\\sigma}m+1, . . . , {\\sigma} nare0.\nThe SVD is used in a variety of applications in machine learning from\nleast-squares problems in curve fitting to solving systems of linear equa-\ntions. These applications harness various important properties of the SVD,\nits relation to the rank of a matrix, and its ability to approximate matrices\nof a given rank with lower-rank matrices. Substituting a matrix with its\nSVD has often the advantage of making calculation more robust to nu-\nmerical rounding errors. As we will explore in the next section, the SVD`s\nability to approximate matrices with {\\textquotedblleft}simpler{\\textquotedblright} matrices in a principled\nmanner opens up machine learning applications ranging from dimension-\nality reduction and topic modeling to data compression and clustering.\n4.6 Matrix Approximation\nWe considered the SVD as a way to factorize A=U{\\Sigma}V{\\top}{\\in}Rm{\\texttimes}ninto\nthe product of three matrices, where U{\\in}Rm{\\texttimes}mandV{\\in}Rn{\\texttimes}nare or-\nthogonal and {\\Sigma}contains the singular values on its main diagonal. Instead\nof doing the full SVD factorization, we will now investigate how the SVD\nallows us to represent a matrix Aas a sum of simpler (low-rank) matrices\nAi, which lends itself to a matrix approximation scheme that is cheaper\nto compute than the full SVD.\nWe construct a rank- 1matrix Ai{\\in}Rm{\\texttimes}nas\nAi:=uiv{\\top}\ni, (4.90)\nwhich is formed by the outer product of the ith orthogonal column vector\nofUandV. Figure 4.11 shows",
                    " an image of Stonehenge, which can be\nrepresented by a matrix A{\\in}R1432{\\texttimes}1910, and some outer products Ai, as\ndefined in (4.90).\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
                ]
            },
            {
                "5": [
                    "146 Vector Calculus\nsuch that the derivative of his given as\nh{'}(x) =g{'}(f)f{'}(x) = (4 f3){\\textperiodcentered}2(5.34)= 4(2 x+ 1)3{\\textperiodcentered}2 = 8(2 x+ 1)3,(5.38)\nwhere we used the chain rule (5.32) and substituted the definition of f\nin (5.34) in g{'}(f).\n5.2 Partial Differentiation and Gradients\nDifferentiation as discussed in Section 5.1 applies to functions fof a\nscalar variable x{\\in}R. In the following, we consider the general case\nwhere the function fdepends on one or more variables x{\\in}Rn, e.g.,\nf(x) =f(x1, x2). The generalization of the derivative to functions of sev-\neral variables is the gradient .\nWe find the gradient of the function fwith respect to xbyvarying one\nvariable at a time and keeping the others constant. The gradient is then\nthe collection of these partial derivatives .\nDefinition 5.5 (Partial Derivative) .For a function f:Rn{\\textrightarrow}R,x7{\\textrightarrow}\nf(x),x{\\in}Rnofnvariables x1, . . . , x nwe define the partial derivatives as partial derivative\n{\\partial}f\n{\\partial}x1= lim\nh{\\textrightarrow}0f(x1+h, x 2, . . . , x n){-}f(x)\nh\n...\n{\\partial}f\n{\\partial}xn= lim\nh{\\textrightarrow}0f(x1, . . . , x n{-}1, xn+h){-}f(x)\nh(5.39)\nand collect them in the row vector\n{\\nabla}xf= grad f=df\ndx={\\partial}f(x)\n{\\partial}x1{\\partial}f(x)\n{\\partial}x2{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}{\\partial}f(x)\n{\\partial}xn\n{\\in}R1{\\texttimes}n,(5.40)",
                    "\nwhere nis the number of variables and 1is the dimension of the image/\nrange/codomain of f. Here, we defined the column vector x= [x1, . . . , x n]{\\top}\n{\\in}Rn. The row vector in (5.40) is called the gradient offor the Jacobian gradient\nJacobian and is the generalization of the derivative from Section 5.1.\nRemark. This definition of the Jacobian is a special case of the general\ndefinition of the Jacobian for vector-valued functions as the collection of\npartial derivatives. We will get back to this in Section 5.3. {\\diamond}We can use results\nfrom scalar\ndifferentiation: Each\npartial derivative is\na derivative with\nrespect to a scalar.Example 5.6 (Partial Derivatives Using the Chain Rule)\nForf(x, y) = (x+ 2y3)2, we obtain the partial derivatives\n{\\partial}f(x, y)\n{\\partial}x= 2(x+ 2y3){\\partial}\n{\\partial}x(x+ 2y3) = 2( x+ 2y3), (5.41)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n5.2 Partial Differentiation and Gradients 147\n{\\partial}f(x, y)\n{\\partial}y= 2(x+ 2y3){\\partial}\n{\\partial}y(x+ 2y3) = 12( x+ 2y3)y2. (5.42)\nwhere we used the chain rule (5.32) to compute the partial derivatives.\nRemark (Gradient as a Row Vector) .It is not uncommon in the literature\nto define the gradient vector as a column vector, following the conven-\ntion that vectors are generally column vectors. The reason why we define\nthe gradient vector as a row vector is twofold: First, we can consistently\ngeneralize the gradient to vector-valued functions f:Rn{\\textrightarrow}Rm(then\nthe gradient becomes a matrix). Second, we can immediately apply the\nmulti-variate chain rule without paying attention to the dimension of the\ngradient. We will discuss both points in Section 5.3.",
                    " {\\diamond}\nExample 5.7 (Gradient)\nForf(x1, x2) =x2\n1x2+x1x3\n2{\\in}R, the partial derivatives (i.e., the deriva-\ntives of fwith respect to x1andx2) are\n{\\partial}f(x1, x2)\n{\\partial}x1= 2x1x2+x3\n2 (5.43)\n{\\partial}f(x1, x2)\n{\\partial}x2=x2\n1+ 3x1x2\n2 (5.44)\nand the gradient is then\ndf\ndx={\\partial}f(x1, x2)\n{\\partial}x1{\\partial}f(x1, x2)\n{\\partial}x2\n=2x1x2+x3\n2x2\n1+ 3x1x2\n2{\\in}R1{\\texttimes}2.\n(5.45)\n5.2.1 Basic Rules of Partial Differentiation\nProduct rule:\n(fg){'}=f{'}g+fg{'},\nSum rule:\n(f+g){'}=f{'}+g{'},\nChain rule:\n(g(f)){'}=g{'}(f)f{'}In the multivariate case, where x{\\in}Rn, the basic differentiation rules that\nwe know from school (e.g., sum rule, product rule, chain rule; see also\nSection 5.1.2) still apply. However, when we compute derivatives with re-\nspect to vectors x{\\in}Rnwe need to pay attention: Our gradients now\ninvolve vectors and matrices, and matrix multiplication is not commuta-\ntive (Section 2.2.1), i.e., the order matters.\nHere are the general product rule, sum rule, and chain rule:\nProduct rule:{\\partial}\n{\\partial}xf(x)g(x)={\\partial}f\n{\\partial}xg(x) +f(x){\\partial}g\n{\\partial}x(5.46)\nSum rule:{\\partial}\n{\\partial}xf(x) +g(x)={\\partial}f\n{\\partial}x+{\\partial}g\n",
                    "{\\partial}x(5.47)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n148 Vector Calculus\nChain rule:{\\partial}\n{\\partial}x(g{\\textopenbullet}f)(x) ={\\partial}\n{\\partial}xg(f(x))={\\partial}g\n{\\partial}f{\\partial}f\n{\\partial}x(5.48)\nLet us have a closer look at the chain rule. The chain rule (5.48) resem- This is only an\nintuition, but not\nmathematically\ncorrect since the\npartial derivative is\nnot a fraction.bles to some degree the rules for matrix multiplication where we said that\nneighboring dimensions have to match for matrix multiplication to be de-\nfined; see Section 2.2.1. If we go from left to right, the chain rule exhibits\nsimilar properties: {\\partial}fshows up in the {\\textquotedblleft}denominator{\\textquotedblright} of the first factor\nand in the {\\textquotedblleft}numerator{\\textquotedblright} of the second factor. If we multiply the factors to-\ngether, multiplication is defined, i.e., the dimensions of {\\partial}fmatch, and {\\partial}f\n{\\textquotedblleft}cancels{\\textquotedblright}, such that {\\partial}g/{\\partial}xremains.\n5.2.2 Chain Rule\nConsider a function f:R2{\\textrightarrow}Rof two variables x1, x2. Furthermore,\nx1(t)andx2(t)are themselves functions of t. To compute the gradient of\nfwith respect to t, we need to apply the chain rule (5.48) for multivariate\nfunctions as\ndf\ndt=h\n{\\partial}f\n{\\partial}x1{\\partial}f\n{\\partial}x2i{''}\n{\\partial}x1(t)\n{\\partial}t{\\partial}x2(t)\n{\\partial}t{\\#}\n={\\partial}f\n{\\partial}x1{\\partial}x1\n{\\partial}t+{\\partial}f\n{\\partial}x2{\\partial}",
                    "x2\n{\\partial}t, (5.49)\nwhere ddenotes the gradient and {\\partial}partial derivatives.\nExample 5.8\nConsider f(x1, x2) =x2\n1+ 2x2, where x1= sin tandx2= cos t, then\ndf\ndt={\\partial}f\n{\\partial}x1{\\partial}x1\n{\\partial}t+{\\partial}f\n{\\partial}x2{\\partial}x2\n{\\partial}t(5.50a)\n= 2 sin t{\\partial}sint\n{\\partial}t+ 2{\\partial}cost\n{\\partial}t(5.50b)\n= 2 sin tcost{-}2 sint= 2 sin t(cost{-}1) (5.50c)\nis the corresponding derivative of fwith respect to t.\nIff(x1, x2)is a function of x1andx2, where x1(s, t)andx2(s, t)are\nthemselves functions of two variables sandt, the chain rule yields the\npartial derivatives\n{\\partial}f\n{\\partial}s={\\partial}f\n{\\partial}x1{\\partial}x1\n{\\partial}s+{\\partial}f\n{\\partial}x2{\\partial}x2\n{\\partial}s, (5.51)\n{\\partial}f\n{\\partial}t={\\partial}f\n{\\partial}x1{\\partial}x1\n{\\partial}t+{\\partial}f\n{\\partial}x2{\\partial}x2\n{\\partial}t, (5.52)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n5.3 Gradients of Vector-Valued Functions 149\nand the gradient is obtained by the matrix multiplication\ndf\nd(s, t)={\\partial}f\n{\\partial}x{\\partial}x\n{\\partial}(s, t)=h{\\partial}f\n{\\partial}x1{\\partial}f\n{\\partial}x2i\n|{\\{}z{\\}}\n={\\partial}f\n{\\partial}x\n{\\partial}x1\n{\\partial}s{\\partial}x1\n{\\partial}t\n",
                    "{\\partial}x2\n{\\partial}s{\\partial}x2\n{\\partial}t\n\n|{\\{}z {\\}}\n={\\partial}x\n{\\partial}(s, t). (5.53)\nThis compact way of writing the chain rule as a matrix multiplication only The chain rule can\nbe written as a\nmatrix\nmultiplication.makes sense if the gradient is defined as a row vector. Otherwise, we will\nneed to start transposing gradients for the matrix dimensions to match.\nThis may still be straightforward as long as the gradient is a vector or a\nmatrix; however, when the gradient becomes a tensor (we will discuss this\nin the following), the transpose is no longer a triviality.\nRemark (Verifying the Correctness of a Gradient Implementation) .The\ndefinition of the partial derivatives as the limit of the corresponding dif-\nference quotient (see (5.39)) can be exploited when numerically checking\nthe correctness of gradients in computer programs: When we compute Gradient checking\ngradients and implement them, we can use finite differences to numer-\nically test our computation and implementation: We choose the value h\nto be small (e.g., h= 10{-}4) and compare the finite-difference approxima-\ntion from (5.39) with our (analytic) implementation of the gradient. If the\nerror is small, our gradient implementation is probably correct. {\\textquotedblleft}Small{\\textquotedblright}\ncould mean thatqP\ni(dhi{-}d fi)2\nP\ni(dhi+d fi)2{<}10{-}6, where dhiis the finite-difference\napproximation and d fiis the analytic gradient of fwith respect to the ith\nvariable xi. {\\diamond}\n5.3 Gradients of Vector-Valued Functions\nThus far, we discussed partial derivatives and gradients of functions f:\nRn{\\textrightarrow}Rmapping to the real numbers. In the following, we will generalize\nthe concept of the gradient to vector-valued functions (vector fields) f:\nRn{\\textrightarrow}Rm, where n{\\geqslant}1andm {>} 1.\nFor a function f:Rn{\\textrightarrow}Rmand a vector x= [x1",
                    ", . . . , x n]{\\top}{\\in}Rn, the\ncorresponding vector of function values is given as\nf(x) =\nf1(x)\n...\nfm(x)\n{\\in}Rm. (5.54)\nWriting the vector-valued function in this way allows us to view a vector-\nvalued function f:Rn{\\textrightarrow}Rmas a vector of functions [f1, . . . , f m]{\\top},\nfi:Rn{\\textrightarrow}Rthat map onto R. The differentiation rules for every fiare\nexactly the ones we discussed in Section 5.2.\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
                ]
            },
            {
                "6": [
                    "6.5 Gaussian Distribution 197\nFigure 6.7\nGaussian\ndistribution of two\nrandom variables x1\nandx2.\nx1{-}101x2\n{-}5.0{-}2.50.02.55.07.5p(x1,x2)\n0.000.050.100.150.20\nfrom the preceding definition of inner products) to compare probability\ndistributions, it is unfortunately not the best way to obtain distances be-\ntween distributions. Recall that the probability mass (or density) is posi-\ntive and needs to add up to 1. These constraints mean that distributions\nlive on something called a statistical manifold. The study of this space of\nprobability distributions is called information geometry. Computing dis-\ntances between distributions are often done using Kullback-Leibler diver-\ngence, which is a generalization of distances that account for properties of\nthe statistical manifold. Just like the Euclidean distance is a special case of\na metric (Section 3.3), the Kullback-Leibler divergence is a special case of\ntwo more general classes of divergences called Bregman divergences and\nf-divergences. The study of divergences is beyond the scope of this book,\nand we refer for more details to the recent book by Amari (2016), one of\nthe founders of the field of information geometry. {\\diamond}\n6.5 Gaussian Distribution\nThe Gaussian distribution is the most well-studied probability distribution\nfor continuous-valued random variables. It is also referred to as the normal normal distribution\ndistribution . Its importance originates from the fact that it has many com- The Gaussian\ndistribution arises\nnaturally when we\nconsider sums of\nindependent and\nidentically\ndistributed random\nvariables. This is\nknown as the\ncentral limit\ntheorem (Grinstead\nand Snell, 1997).putationally convenient properties, which we will be discussing in the fol-\nlowing. In particular, we will use it to define the likelihood and prior for\nlinear regression (Chapter 9), and consider a mixture of Gaussians for\ndensity estimation (Chapter 11).\nThere are many other areas of machine learning that also benefit from\nusing a Gaussian distribution, for example Gaussian processes, variational\ninference, and reinforcement learning. It is also widely used in",
                    " other ap-\nplication areas such as signal processing (e.g., Kalman filter), control (e.g.,\nlinear quadratic regulator), and statistics (e.g., hypothesis testing).\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n198 Probability and Distributions\nFigure 6.8\nGaussian\ndistributions\noverlaid with 100\nsamples. (a) One-\ndimensional case;\n(b) two-dimensional\ncase.\n{-}5.0{-}2.5 0.0 2.5 5.0 7.5\nx0.000.050.100.150.20\np(x)\nMean\nSample\n2{\\sigma}\n(a) Univariate (one-dimensional) Gaussian;\nThe red cross shows the mean and the red\nline shows the extent of the variance.\n{-}1 0 1\nx1{-}4{-}202468x2Mean\nSample(b) Multivariate (two-dimensional) Gaus-\nsian, viewed from top. The red cross shows\nthe mean and the colored lines show the con-\ntour lines of the density.\nFor a univariate random variable, the Gaussian distribution has a den-\nsity that is given by\np(x|{\\textmu}, {\\sigma}2) =1{\\sqrt{}}\n2{\\pi}{\\sigma}2exp\n{-}(x{-}{\\textmu})2\n2{\\sigma}2\n. (6.62)\nThe multivariate Gaussian distribution is fully characterized by a mean multivariate\nGaussian\ndistribution\nmean vectorvector {\\textmu}and a covariance matrix {\\Sigma}and defined as\ncovariance matrixp(x|{\\textmu},{\\Sigma}) = (2 {\\pi}){-}D\n2|{\\Sigma}|{-}1\n2exp{-}1\n2(x{-}{\\textmu}){\\top}{\\Sigma}{-}1(x{-}{\\textmu}),(6.63)\nwhere x{\\in}RD. We write p(x) =Nx|{\\textmu},{\\Sigma}",
                    "\norX{\\sim} N{\\textmu},{\\Sigma}\n. Fig- Also known as a\nmultivariate normal\ndistribution.ure 6.7 shows a bivariate Gaussian (mesh), with the corresponding con-\ntour plot. Figure 6.8 shows a univariate Gaussian and a bivariate Gaussian\nwith corresponding samples. The special case of the Gaussian with zero\nmean and identity covariance, that is, {\\textmu}=0and{\\Sigma}=I, is referred to as\nthestandard normal distribution . standard normal\ndistribution Gaussians are widely used in statistical estimation and machine learn-\ning as they have closed-form expressions for marginal and conditional dis-\ntributions. In Chapter 9, we use these closed-form expressions extensively\nfor linear regression. A major advantage of modeling with Gaussian ran-\ndom variables is that variable transformations (Section 6.7) are often not\nneeded. Since the Gaussian distribution is fully specified by its mean and\ncovariance, we often can obtain the transformed distribution by applying\nthe transformation to the mean and covariance of the random variable.\n6.5.1 Marginals and Conditionals of Gaussians are Gaussians\nIn the following, we present marginalization and conditioning in the gen-\neral case of multivariate random variables. If this is confusing at first read-\ning, the reader is advised to consider two univariate random variables in-\nstead. Let XandYbe two multivariate random variables, that may have\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n6.5 Gaussian Distribution 199\ndifferent dimensions. To consider the effect of applying the sum rule of\nprobability and the effect of conditioning, we explicitly write the Gaus-\nsian distribution in terms of the concatenated states [x{\\top},y{\\top}],\np(x,y) =N{\\textmu}x\n{\\textmu}y\n,{\\Sigma}xx{\\Sigma}xy\n{\\Sigma}yx{\\Sigma}yy\n. (6.64)\nwhere {\\Sigma}xx= Cov[ x,x]and{\\Sigma}yy= Cov[ y,y]are the marginal covari-\nance matrices of xandy,",
                    " respectively, and {\\Sigma}xy= Cov[ x,y]is the cross-\ncovariance matrix between xandy.\nThe conditional distribution p(x|y)is also Gaussian (illustrated in Fig-\nure 6.9(c)) and given by (derived in Section 2.3 of Bishop, 2006)\np(x|y) =N{\\textmu}x|y,{\\Sigma}x|y\n(6.65)\n{\\textmu}x|y={\\textmu}x+{\\Sigma}xy{\\Sigma}{-}1\nyy(y{-}{\\textmu}y) (6.66)\n{\\Sigma}x|y={\\Sigma}xx{-}{\\Sigma}xy{\\Sigma}{-}1\nyy{\\Sigma}yx. (6.67)\nNote that in the computation of the mean in (6.66), the y-value is an\nobservation and no longer random.\nRemark. The conditional Gaussian distribution shows up in many places,\nwhere we are interested in posterior distributions:\nThe Kalman filter (Kalman, 1960), one of the most central algorithms\nfor state estimation in signal processing, does nothing but computing\nGaussian conditionals of joint distributions (Deisenroth and Ohlsson,\n2011; S {\\textasciidieresis}arkk{\\textasciidieresis}a, 2013).\nGaussian processes (Rasmussen and Williams, 2006), which are a prac-\ntical implementation of a distribution over functions. In a Gaussian pro-\ncess, we make assumptions of joint Gaussianity of random variables. By\n(Gaussian) conditioning on observed data, we can determine a poste-\nrior distribution over functions.\nLatent linear Gaussian models (Roweis and Ghahramani, 1999; Mur-\nphy, 2012), which include probabilistic principal component analysis\n(PPCA) (Tipping and Bishop, 1999). We will look at PPCA in more de-\ntail in Section 10.7.\n{\\diamond}\nThe marginal distribution p(x)of a joint Gaussian distribution p(x,y)\n(see (6.64)) is itself Gaussian and computed by applying the sum rule\n(6.20) and given by\np(x) =Z\np(",
                    "x,y)dy=Nx|{\\textmu}x,{\\Sigma}xx. (6.68)\nThe corresponding result holds for p(y), which is obtained by marginaliz-\ning with respect to x. Intuitively, looking at the joint distribution in (6.64),\nwe ignore (i.e., integrate out) everything we are not interested in. This is\nillustrated in Figure 6.9(b).\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n200 Probability and Distributions\nExample 6.6\nFigure 6.9\n(a) Bivariate\nGaussian;\n(b) marginal of a\njoint Gaussian\ndistribution is\nGaussian; (c) the\nconditional\ndistribution of a\nGaussian is also\nGaussian.\n{-}1 0 1\nx1{-}4{-}202468x2\nx2={-}1\n(a) Bivariate Gaussian.\n{-}1.5{-}1.0{-}0.5 0.0 0.5 1.0 1.5\nx10.00.20.40.6p(x1)\nMean\n2{\\sigma}\n(b) Marginal distribution.\n{-}1.5{-}1.0{-}0.5 0.0 0.5 1.0 1.5\nx10.00.20.40.60.81.01.2p(x1|x2={-}1)\nMean\n2{\\sigma} (c) Conditional distribution.\nConsider the bivariate Gaussian distribution (illustrated in Figure 6.9):\np(x1, x2) =N0\n2\n,0.3{-}1\n{-}1 5\n. (6.69)\nWe can compute the parameters of the univariate Gaussian, conditioned\nonx2={-}1, by applying (6.66) and (6.67) to obtain the mean and vari-\nance respectively. Numerically, this is\n{\\textmu}x1|x2={-}1= 0 + ( {-}1){\\textperiodcentered}0.2{\\text",
                    "periodcentered}({-}1{-}2) = 0 .6 (6.70)\nand\n{\\sigma}2\nx1|x2={-}1= 0.3{-}({-}1){\\textperiodcentered}0.2{\\textperiodcentered}({-}1) = 0 .1. (6.71)\nTherefore, the conditional Gaussian is given by\np(x1|x2={-}1) =N0.6,0.1. (6.72)\nThe marginal distribution p(x1), in contrast, can be obtained by apply-\ning (6.68), which is essentially using the mean and variance of the random\nvariable x1, giving us\np(x1) =N0,0.3. (6.73)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n6.5 Gaussian Distribution 201\n6.5.2 Product of Gaussian Densities\nFor linear regression (Chapter 9), we need to compute a Gaussian likeli-\nhood. Furthermore, we may wish to assume a Gaussian prior (Section 9.3).\nWe apply Bayes` Theorem to compute the posterior, which results in a mul-\ntiplication of the likelihood and the prior, that is, the multiplication of two\nGaussian densities. The product of two Gaussians Nx|a,ANx|b,B\nThe derivation is an\nexercise at the end\nof this chapter.is a Gaussian distribution scaled by a c{\\in}R, given by cNx|c,C\nwith\nC= (A{-}1+B{-}1){-}1(6.74)\nc=C(A{-}1a+B{-}1b) (6.75)\nc= (2{\\pi}){-}D\n2|A+B|{-}1\n2exp{-}1\n2(a{-}b){\\top}(A+B){-}1(a{-}b).(6.76)\nThe scaling constant citself can be written in the form of a Gaussian\ndensity either in aor inbwith an {\\text",
                    "quotedblleft}inflated{\\textquotedblright} covariance matrix A+B,\ni.e.,c=Na|b,A+B=Nb|a,A+B\n.\nRemark. For notation convenience, we will sometimes use Nx|m,S\nto describe the functional form of a Gaussian density even if xis not a\nrandom variable. We have just done this in the preceding demonstration\nwhen we wrote\nc=Na|b,A+B=Nb|a,A+B. (6.77)\nHere, neither anorbare random variables. However, writing cin this way\nis more compact than (6.76). {\\diamond}\n6.5.3 Sums and Linear Transformations\nIfX, Y are independent Gaussian random variables (i.e., the joint distri-\nbution is given as p(x,y) =p(x)p(y)) with p(x) =Nx|{\\textmu}x,{\\Sigma}x\nand\np(y) =Ny|{\\textmu}y,{\\Sigma}y\n, then x+yis also Gaussian distributed and given\nby\np(x+y) =N{\\textmu}x+{\\textmu}y,{\\Sigma}x+{\\Sigma}y. (6.78)\nKnowing that p(x+y)is Gaussian, the mean and covariance matrix can\nbe determined immediately using the results from (6.46) through (6.49).\nThis property will be important when we consider i.i.d. Gaussian noise\nacting on random variables, as is the case for linear regression (Chap-\nter 9).\nExample 6.7\nSince expectations are linear operations, we can obtain the weighted sum\nof independent Gaussian random variables\np(ax+by) =Na{\\textmu}x+b{\\textmu}y, a2{\\Sigma}x+b2{\\Sigma}y. (6.79)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n202 Probability and Distributions\nRemark. A case that will be useful in Chapter 11 is the weighted sum of\nGaussian densities. This is",
                    " different from the weighted sum of Gaussian\nrandom variables. {\\diamond}\nIn Theorem 6.12, the random variable xis from a density that is a\nmixture of two densities p1(x)andp2(x), weighted by {\\alpha}. The theorem can\nbe generalized to the multivariate random variable case, since linearity of\nexpectations holds also for multivariate random variables. However, the\nidea of a squared random variable needs to be replaced by xx{\\top}.\nTheorem 6.12. Consider a mixture of two univariate Gaussian densities\np(x) ={\\alpha}p1(x) + (1 {-}{\\alpha})p2(x), (6.80)\nwhere the scalar 0{<} {\\alpha} {<} 1is the mixture weight, and p1(x)andp2(x)are\nunivariate Gaussian densities (Equation (6.62) ) with different parameters,\ni.e.,({\\textmu}1, {\\sigma}2\n1)= ({\\textmu}2, {\\sigma}2\n2).\nThen the mean of the mixture density p(x)is given by the weighted sum\nof the means of each random variable:\nE[x] ={\\alpha}{\\textmu}1+ (1{-}{\\alpha}){\\textmu}2. (6.81)\nThe variance of the mixture density p(x)is given by\nV[x] ={\\alpha}{\\sigma}2\n1+ (1{-}{\\alpha}){\\sigma}2\n2+{\\alpha}{\\textmu}2\n1+ (1{-}{\\alpha}){\\textmu}2\n2{-}[{\\alpha}{\\textmu}1+ (1{-}{\\alpha}){\\textmu}2]2\n.\n(6.82)\nProof The mean of the mixture density p(x)is given by the weighted\nsum of the means of each random variable. We apply the definition of the\nmean (Definition 6.4), and plug in our mixture (6.80), which yields\nE[x] =Z{\\infty}\n{-}{\\infty}xp(x)dx (6.83a)\n=Z{\\infty}\n{-}{\\infty}({",
                    "\\alpha}xp 1(x) + (1 {-}{\\alpha})xp2(x)) dx (6.83b)\n={\\alpha}Z{\\infty}\n{-}{\\infty}xp1(x)dx+ (1{-}{\\alpha})Z{\\infty}\n{-}{\\infty}xp2(x)dx (6.83c)\n={\\alpha}{\\textmu}1+ (1{-}{\\alpha}){\\textmu}2. (6.83d)\nTo compute the variance, we can use the raw-score version of the vari-\nance from (6.44), which requires an expression of the expectation of the\nsquared random variable. Here we use the definition of an expectation of\na function (the square) of a random variable (Definition 6.3),\nE[x2] =Z{\\infty}\n{-}{\\infty}x2p(x)dx (6.84a)\n=Z{\\infty}\n{-}{\\infty}{\\alpha}x2p1(x) + (1 {-}{\\alpha})x2p2(x)dx (6.84b)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n6.5 Gaussian Distribution 203\n={\\alpha}Z{\\infty}\n{-}{\\infty}x2p1(x)dx+ (1{-}{\\alpha})Z{\\infty}\n{-}{\\infty}x2p2(x)dx (6.84c)\n={\\alpha}({\\textmu}2\n1+{\\sigma}2\n1) + (1 {-}{\\alpha})({\\textmu}2\n2+{\\sigma}2\n2), (6.84d)\nwhere in the last equality, we again used the raw-score version of the\nvariance (6.44) giving {\\sigma}2=E[x2]{-}{\\textmu}2. This is rearranged such that the\nexpectation of a squared random variable is the sum of the squared mean\nand the variance.\nTherefore, the variance is given by subtracting (6.83",
                    "d) from (6.84d),\nV[x] =E[x2]{-}(E[x])2(6.85a)\n={\\alpha}({\\textmu}2\n1+{\\sigma}2\n1) + (1 {-}{\\alpha})({\\textmu}2\n2+{\\sigma}2\n2){-}({\\alpha}{\\textmu}1+ (1{-}{\\alpha}){\\textmu}2)2(6.85b)\n={\\alpha}{\\sigma}2\n1+ (1{-}{\\alpha}){\\sigma}2\n2\n+{\\alpha}{\\textmu}2\n1+ (1{-}{\\alpha}){\\textmu}2\n2{-}[{\\alpha}{\\textmu}1+ (1{-}{\\alpha}){\\textmu}2]2\n. (6.85c)\nRemark. The preceding derivation holds for any density, but since the\nGaussian is fully determined by the mean and variance, the mixture den-\nsity can be determined in closed form. {\\diamond}\nFor a mixture density, the individual components can be considered\nto be conditional distributions (conditioned on the component identity).\nEquation (6.85c) is an example of the conditional variance formula, also\nknown as the law of total variance , which generally states that for two ran- law of total variance\ndom variables XandYit holds that VX[x] =EY[VX[x|y]]+VY[EX[x|y]],\ni.e., the (total) variance of Xis the expected conditional variance plus the\nvariance of a conditional mean.\nWe consider in Example 6.17 a bivariate standard Gaussian random\nvariable Xand performed a linear transformation Axon it. The outcome\nis a Gaussian random variable with mean zero and covariance AA{\\top}. Ob-\nserve that adding a constant vector will change the mean of the distribu-\ntion, without affecting its variance, that is, the random variable x+{\\textmu}is\nGaussian with mean {\\textmu}and identity covariance. Hence, any linear/affine\ntransformation of a Gaussian random variable is Gaussian distributed. Any linear/affine\ntransformation of a\nGaussian random",
                    "\nvariable is also\nGaussian\ndistributed.Consider a Gaussian distributed random variable X{\\sim} N{\\textmu},{\\Sigma}\n. For\na given matrix Aof appropriate shape, let Ybe a random variable such\nthaty=Axis a transformed version of x. We can compute the mean of\nyby exploiting that the expectation is a linear operator (6.50) as follows:\nE[y] =E[Ax] =AE[x] =A{\\textmu}. (6.86)\nSimilarly the variance of ycan be found by using (6.51):\nV[y] =V[Ax] =AV[x]A{\\top}=A{\\Sigma}A{\\top}. (6.87)\nThis means that the random variable yis distributed according to\np(y) =Ny|A{\\textmu},A{\\Sigma}A{\\top}. (6.88)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n204 Probability and Distributions\nLet us now consider the reverse transformation: when we know that a\nrandom variable has a mean that is a linear transformation of another\nrandom variable. For a given full rank matrix A{\\in}RM{\\texttimes}N, where M{\\geqslant}N,\nlety{\\in}RMbe a Gaussian random variable with mean Ax, i.e.,\np(y) =Ny|Ax,{\\Sigma}. (6.89)\nWhat is the corresponding probability distribution p(x)? IfAis invert-\nible, then we can write x=A{-}1yand apply the transformation in the\nprevious paragraph. However, in general Ais not invertible, and we use\nan approach similar to that of the pseudo-inverse (3.57). That is, we pre-\nmultiply both sides with A{\\top}and then invert A{\\top}A, which is symmetric\nand positive definite, giving us the relation\ny=Ax{\\Leftarrow}{\\Rightarrow} (A{\\top}A){-}1A{\\top}y=x. (6.90)\nHence, xis a linear transformation of y, and we obtain\np(x) =Nx|(",
                    "A{\\top}A){-}1A{\\top}y,(A{\\top}A){-}1A{\\top}{\\Sigma}A(A{\\top}A){-}1.(6.91)\n6.5.4 Sampling from Multivariate Gaussian Distributions\nWe will not explain the subtleties of random sampling on a computer, and\nthe interested reader is referred to Gentle (2004). In the case of a mul-\ntivariate Gaussian, this process consists of three stages: first, we need a\nsource of pseudo-random numbers that provide a uniform sample in the\ninterval [0,1]; second, we use a non-linear transformation such as the\nBox-M {\\textasciidieresis}uller transform (Devroye, 1986) to obtain a sample from a univari-\nate Gaussian; and third, we collate a vector of these samples to obtain a\nsample from a multivariate standard normal N0,I\n.\nFor a general multivariate Gaussian, that is, where the mean is non\nzero and the covariance is not the identity matrix, we use the proper-\nties of linear transformations of a Gaussian random variable. Assume we\nare interested in generating samples xi, i= 1, . . . , n, from a multivariate\nGaussian distribution with mean {\\textmu}and covariance matrix {\\Sigma}. We would To compute the\nCholesky\nfactorization of a\nmatrix, it is required\nthat the matrix is\nsymmetric and\npositive definite\n(Section 3.2.3).\nCovariance matrices\npossess this\nproperty.like to construct the sample from a sampler that provides samples from\nthe multivariate standard normal N0,I\n.\nTo obtain samples from a multivariate normal N{\\textmu},{\\Sigma}\n, we can use\nthe properties of a linear transformation of a Gaussian random variable:\nIfx{\\sim} N0,I\n, then y=Ax+{\\textmu}, where AA{\\top}={\\Sigma}is Gaussian dis-\ntributed with mean {\\textmu}and covariance matrix {\\Sigma}. One convenient choice of\nAis to use the Cholesky decomposition (Section 4.3) of the covariance\nmatrix {\\Sigma}=AA{\\top}. The Cholesky decom",
                    "position has the benefit that Ais\ntriangular, leading to efficient computation.\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n6.6 Conjugacy and the Exponential Family 205\n6.6 Conjugacy and the Exponential Family\nMany of the probability distributions {\\textquotedblleft}with names{\\textquotedblright} that we find in statis-\ntics textbooks were discovered to model particular types of phenomena.\nFor example, we have seen the Gaussian distribution in Section 6.5. The\ndistributions are also related to each other in complex ways (Leemis and\nMcQueston, 2008). For a beginner in the field, it can be overwhelming to\nfigure out which distribution to use. In addition, many of these distribu-\ntions were discovered at a time that statistics and computation were done {\\textquotedblleft}Computers{\\textquotedblright} used to\nbe a job description. by pencil and paper. It is natural to ask what are meaningful concepts\nin the computing age (Efron and Hastie, 2016). In the previous section,\nwe saw that many of the operations required for inference can be conve-\nniently calculated when the distribution is Gaussian. It is worth recalling\nat this point the desiderata for manipulating probability distributions in\nthe machine learning context:\n1. There is some {\\textquotedblleft}closure property{\\textquotedblright} when applying the rules of probability,\ne.g., Bayes` theorem. By closure, we mean that applying a particular\noperation returns an object of the same type.\n2. As we collect more data, we do not need more parameters to describe\nthe distribution.\n3. Since we are interested in learning from data, we want parameter es-\ntimation to behave nicely.\nIt turns out that the class of distributions called the exponential family exponential family\nprovides the right balance of generality while retaining favorable compu-\ntation and inference properties. Before we introduce the exponential fam-\nily, let us see three more members of {\\textquotedblleft}named{\\textquotedblright} probability distributions,\nthe Bernoulli (Example 6.8), Binomial (Example 6.9), and Beta (Exam-\nple 6.",
                    "10) distributions.\nExample 6.8\nThe Bernoulli distribution is a distribution for a single binary random Bernoulli\ndistribution variable Xwith state x{\\in} {\\{}0,1{\\}}. It is governed by a single continuous pa-\nrameter {\\textmu}{\\in}[0,1]that represents the probability of X= 1. The Bernoulli\ndistribution Ber ({\\textmu})is defined as\np(x|{\\textmu}) ={\\textmu}x(1{-}{\\textmu})1{-}x, x{\\in} {\\{}0,1{\\}}, (6.92)\nE[x] ={\\textmu} , (6.93)\nV[x] ={\\textmu}(1{-}{\\textmu}), (6.94)\nwhere E[x]andV[x]are the mean and variance of the binary random\nvariable X.\nAn example where the Bernoulli distribution can be used is when we\nare interested in modeling the probability of {\\textquotedblleft}heads{\\textquotedblright} when flipping a coin.\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
                ]
            },
            {
                "7": [
                    "7.1 Optimization Using Gradient Descent 227\nFigure 7.2 Example\nobjective function.\nNegative gradients\nare indicated by\narrows, and the\nglobal minimum is\nindicated by the\ndashed blue line.\n{-}6{-}5{-}4{-}3{-}2{-}1 0 1 2\nValue of parameter{-}60{-}40{-}200204060Objectivex4+ 7x3+ 5x2{-}17x+ 3\nright, but not how far (this is called the step-size). Furthermore, if we According to the\nAbel{\\textendash}Ruffini\ntheorem, there is in\ngeneral no algebraic\nsolution for\npolynomials of\ndegree 5 or more\n(Abel, 1826).had started at the right side (e.g., x0= 0) the negative gradient would\nhave led us to the wrong minimum. Figure 7.2 illustrates the fact that for\nx {>}{-}1, the negative gradient points toward the minimum on the right of\nthe figure, which has a larger objective value.\nIn Section 7.3, we will learn about a class of functions, called convex\nfunctions, that do not exhibit this tricky dependency on the starting point\nof the optimization algorithm. For convex functions, all local minimums\nare global minimum. It turns out that many machine learning objective For convex functions\nall local minima are\nglobal minimum.functions are designed such that they are convex, and we will see an ex-\nample in Chapter 12.\nThe discussion in this chapter so far was about a one-dimensional func-\ntion, where we are able to visualize the ideas of gradients, descent direc-\ntions, and optimal values. In the rest of this chapter we develop the same\nideas in high dimensions. Unfortunately, we can only visualize the con-\ncepts in one dimension, but some concepts do not generalize directly to\nhigher dimensions, therefore some care needs to be taken when reading.\n7.1 Optimization Using Gradient Descent\nWe now consider the problem of solving for the minimum of a real-valued\nfunction\nmin\nxf(x), (7.4)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S",
                    ". Ong. Published by Cambridge University Press (2020).\n228 Continuous Optimization\nwhere f:Rd{\\textrightarrow}Ris an objective function that captures the machine\nlearning problem at hand. We assume that our function fis differentiable,\nand we are unable to analytically find a solution in closed form.\nGradient descent is a first-order optimization algorithm. To find a local\nminimum of a function using gradient descent, one takes steps propor-\ntional to the negative of the gradient of the function at the current point.\nRecall from Section 5.1 that the gradient points in the direction of the We use the\nconvention of row\nvectors for\ngradients.steepest ascent. Another useful intuition is to consider the set of lines\nwhere the function is at a certain value ( f(x) =cfor some value c{\\in}R),\nwhich are known as the contour lines. The gradient points in a direction\nthat is orthogonal to the contour lines of the function we wish to optimize.\nLet us consider multivariate functions. Imagine a surface (described by\nthe function f(x)) with a ball starting at a particular location x0. When\nthe ball is released, it will move downhill in the direction of steepest de-\nscent. Gradient descent exploits the fact that f(x0)decreases fastest if one\nmoves from x0in the direction of the negative gradient {-}(({\\nabla}f)(x0)){\\top}of\nfatx0. We assume in this book that the functions are differentiable, and\nrefer the reader to more general settings in Section 7.4. Then, if\nx1=x0{-}{\\gamma}(({\\nabla}f)(x0)){\\top}(7.5)\nfor a small step-size {\\gamma}{\\geqslant}0, then f(x1){\\leqslant}f(x0). Note that we use the\ntranspose for the gradient since otherwise the dimensions will not work\nout.\nThis observation allows us to define a simple gradient descent algo-\nrithm: If we want to find a local optimum f(x{*})of a function f:Rn{\\textrightarrow}\nR,x7{\\textrightarrow}f(x), we start with an initial guess x",
                    "0of the parameters we wish\nto optimize and then iterate according to\nxi+1=xi{-}{\\gamma}i(({\\nabla}f)(xi)){\\top}. (7.6)\nFor suitable step-size {\\gamma}i, the sequence f(x0){\\geqslant}f(x1){\\geqslant}. . .converges to\na local minimum.\nExample 7.1\nConsider a quadratic function in two dimensions\nfx1\nx2\n=1\n2x1\nx2{\\top}2 1\n1 20x1\nx2\n{-}5\n3{\\top}x1\nx2\n(7.7)\nwith gradient\n{\\nabla}fx1\nx2\n=x1\nx2{\\top}2 1\n1 20\n{-}5\n3{\\top}\n. (7.8)\nStarting at the initial location x0= [{-}3,{-}1]{\\top}, we iteratively apply (7.6)\nto obtain a sequence of estimates that converge to the minimum value\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n7.1 Optimization Using Gradient Descent 229\nFigure 7.3 Gradient\ndescent on a\ntwo-dimensional\nquadratic surface\n(shown as a\nheatmap). See\nExample 7.1 for a\ndescription.\n{-}4{-}2 0 2 4\nx1{-}2{-}1012x20.0\n10.0\n20.030.0\n40.040.0\n50.050.0\n60.0 70.0\n80.0{-}150153045607590\n(illustrated in Figure 7.3). We can see (both from the figure and by plug-\ngingx0into (7.8) with {\\gamma}= 0.085) that the negative gradient at x0points\nnorth and east, leading to x1= [{-}1.98,1.21]{\\top}. Repeating that argument\ngives us x2= [{-}1.32,{-}0.42]{\\",
                    "top}, and so on.\nRemark. Gradient descent can be relatively slow close to the minimum:\nIts asymptotic rate of convergence is inferior to many other methods. Us-\ning the ball rolling down the hill analogy, when the surface is a long, thin\nvalley, the problem is poorly conditioned (Trefethen and Bau III, 1997).\nFor poorly conditioned convex problems, gradient descent increasingly\n{\\textquotedblleft}zigzags{\\textquotedblright} as the gradients point nearly orthogonally to the shortest di-\nrection to a minimum point; see Figure 7.3. {\\diamond}\n7.1.1 Step-size\nAs mentioned earlier, choosing a good step-size is important in gradient\ndescent. If the step-size is too small, gradient descent can be slow. If the The step-size is also\ncalled the learning\nrate.step-size is chosen too large, gradient descent can overshoot, fail to con-\nverge, or even diverge. We will discuss the use of momentum in the next\nsection. It is a method that smoothes out erratic behavior of gradient up-\ndates and dampens oscillations.\nAdaptive gradient methods rescale the step-size at each iteration, de-\npending on local properties of the function. There are two simple heuris-\ntics (Toussaint, 2012):\nWhen the function value increases after a gradient step, the step-size\nwas too large. Undo the step and decrease the step-size.\nWhen the function value decreases the step could have been larger. Try\nto increase the step-size.\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n230 Continuous Optimization\nAlthough the {\\textquotedblleft}undo{\\textquotedblright} step seems to be a waste of resources, using this\nheuristic guarantees monotonic convergence.\nExample 7.2 (Solving a Linear Equation System)\nWhen we solve linear equations of the form Ax=b, in practice we solve\nAx{-}b=0approximately by finding x{*}that minimizes the squared error\n{\\parallel}Ax{-}b{\\parallel}2= (Ax{-}b){\\top}(Ax{-",
                    "}b) (7.9)\nif we use the Euclidean norm. The gradient of (7.9) with respect to xis\n{\\nabla}x= 2(Ax{-}b){\\top}A. (7.10)\nWe can use this gradient directly in a gradient descent algorithm. How-\never, for this particular special case, it turns out that there is an analytic\nsolution, which can be found by setting the gradient to zero. We will see\nmore on solving squared error problems in Chapter 9.\nRemark. When applied to the solution of linear systems of equations Ax=\nb, gradient descent may converge slowly. The speed of convergence of gra-\ndient descent is dependent on the condition number {\\kappa}={\\sigma}(A)max\n{\\sigma}(A)min, which condition number\nis the ratio of the maximum to the minimum singular value (Section 4.5)\nofA. The condition number essentially measures the ratio of the most\ncurved direction versus the least curved direction, which corresponds to\nour imagery that poorly conditioned problems are long, thin valleys: They\nare very curved in one direction, but very flat in the other. Instead of di-\nrectly solving Ax=b, one could instead solve P{-}1(Ax{-}b) =0, where\nPis called the preconditioner . The goal is to design P{-}1such that P{-}1A preconditioner\nhas a better condition number, but at the same time P{-}1is easy to com-\npute. For further information on gradient descent, preconditioning, and\nconvergence we refer to Boyd and Vandenberghe (2004, chapter 9). {\\diamond}\n7.1.2 Gradient Descent With Momentum\nAs illustrated in Figure 7.3, the convergence of gradient descent may be\nvery slow if the curvature of the optimization surface is such that there\nare regions that are poorly scaled. The curvature is such that the gradient\ndescent steps hops between the walls of the valley and approaches the\noptimum in small steps. The proposed tweak to improve convergence is\nto give gradient descent some memory. Goh (2017) wrote\nan intuitive blog\npost on gradient\ndescent with\nmomentum.Gradient descent with momentum (Rumelhart et al., 1986) is a method",
                    "\nthat introduces an additional term to remember what happened in the\nprevious iteration. This memory dampens oscillations and smoothes out\nthe gradient updates. Continuing the ball analogy, the momentum term\nemulates the phenomenon of a heavy ball that is reluctant to change di-\nrections. The idea is to have a gradient update with memory to implement\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n7.1 Optimization Using Gradient Descent 231\na moving average. The momentum-based method remembers the update\n{\\Delta}xiat each iteration iand determines the next update as a linear combi-\nnation of the current and previous gradients\nxi+1=xi{-}{\\gamma}i(({\\nabla}f)(xi)){\\top}+{\\alpha}{\\Delta}xi (7.11)\n{\\Delta}xi=xi{-}xi{-}1={\\alpha}{\\Delta}xi{-}1{-}{\\gamma}i{-}1(({\\nabla}f)(xi{-}1)){\\top}, (7.12)\nwhere {\\alpha}{\\in}[0,1]. Sometimes we will only know the gradient approxi-\nmately. In such cases, the momentum term is useful since it averages out\ndifferent noisy estimates of the gradient. One particularly useful way to\nobtain an approximate gradient is by using a stochastic approximation,\nwhich we discuss next.\n7.1.3 Stochastic Gradient Descent\nComputing the gradient can be very time consuming. However, often it is\npossible to find a {\\textquotedblleft}cheap{\\textquotedblright} approximation of the gradient. Approximating\nthe gradient is still useful as long as it points in roughly the same direction\nas the true gradient. stochastic gradient\ndescent Stochastic gradient descent (often shortened as SGD) is a stochastic ap-\nproximation of the gradient descent method for minimizing an objective\nfunction that is written as a sum of differentiable functions. The word\nstochastic here refers to the fact that we acknowledge that we do not\nknow the gradient precisely, but instead only know a noisy approxima-\ntion to it. By constraining the probability",
                    " distribution of the approximate\ngradients, we can still theoretically guarantee that SGD will converge.\nIn machine learning, given n= 1, . . . , N data points, we often consider\nobjective functions that are the sum of the losses Lnincurred by each\nexample n. In mathematical notation, we have the form\nL({\\theta}) =NX\nn=1Ln({\\theta}), (7.13)\nwhere {\\theta}is the vector of parameters of interest, i.e., we want to find {\\theta}that\nminimizes L. An example from regression (Chapter 9) is the negative log-\nlikelihood, which is expressed as a sum over log-likelihoods of individual\nexamples so that\nL({\\theta}) ={-}NX\nn=1logp(yn|xn,{\\theta}), (7.14)\nwhere xn{\\in}RDare the training inputs, ynare the training targets, and {\\theta}\nare the parameters of the regression model.\nStandard gradient descent, as introduced previously, is a {\\textquotedblleft}batch{\\textquotedblright} opti-\nmization method, i.e., optimization is performed using the full training set\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n232 Continuous Optimization\nby updating the vector of parameters according to\n{\\theta}i+1={\\theta}i{-}{\\gamma}i({\\nabla}L({\\theta}i)){\\top}={\\theta}i{-}{\\gamma}iNX\nn=1({\\nabla}Ln({\\theta}i)){\\top}(7.15)\nfor a suitable step-size parameter {\\gamma}i. Evaluating the sum gradient may re-\nquire expensive evaluations of the gradients from all individual functions\nLn. When the training set is enormous and/or no simple formulas exist,\nevaluating the sums of gradients becomes very expensive.\nConsider the termPN\nn=1({\\nabla}Ln({\\theta}i))in (7.15). We can reduce the amount\nof computation by taking a sum over",
                    " a smaller set of Ln. In contrast to\nbatch gradient descent, which uses all Lnforn= 1, . . . , N , we randomly\nchoose a subset of Lnfor mini-batch gradient descent. In the extreme\ncase, we randomly select only a single Lnto estimate the gradient. The\nkey insight about why taking a subset of data is sensible is to realize that\nfor gradient descent to converge, we only require that the gradient is an\nunbiased estimate of the true gradient. In fact the termPN\nn=1({\\nabla}Ln({\\theta}i))\nin (7.15) is an empirical estimate of the expected value (Section 6.4.1) of\nthe gradient. Therefore, any other unbiased empirical estimate of the ex-\npected value, for example using any subsample of the data, would suffice\nfor convergence of gradient descent.\nRemark. When the learning rate decreases at an appropriate rate, and sub-\nject to relatively mild assumptions, stochastic gradient descent converges\nalmost surely to local minimum (Bottou, 1998). {\\diamond}\nWhy should one consider using an approximate gradient? A major rea-\nson is practical implementation constraints, such as the size of central\nprocessing unit (CPU)/graphics processing unit (GPU) memory or limits\non computational time. We can think of the size of the subset used to esti-\nmate the gradient in the same way that we thought of the size of a sample\nwhen estimating empirical means (Section 6.4.1). Large mini-batch sizes\nwill provide accurate estimates of the gradient, reducing the variance in\nthe parameter update. Furthermore, large mini-batches take advantage of\nhighly optimized matrix operations in vectorized implementations of the\ncost and gradient. The reduction in variance leads to more stable conver-\ngence, but each gradient calculation will be more expensive.\nIn contrast, small mini-batches are quick to estimate. If we keep the\nmini-batch size small, the noise in our gradient estimate will allow us to\nget out of some bad local optima, which we may otherwise get stuck in.\nIn machine learning, optimization methods are used for training by min-\nimizing an objective function on the training data, but the overall goal\nis to improve generalization performance (Chapter 8). Since the goal in\nmachine learning does not necessarily need a precise estimate of the min-\nimum of the objective",
                    " function, approximate gradients using mini-batch\napproaches have been widely used. Stochastic gradient descent is very\neffective in large-scale machine learning problems (Bottou et al., 2018),\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n7.2 Constrained Optimization and Lagrange Multipliers 233\nFigure 7.4\nIllustration of\nconstrained\noptimization. The\nunconstrained\nproblem (indicated\nby the contour\nlines) has a\nminimum on the\nright side (indicated\nby the circle). The\nbox constraints\n({-}1{\\leqslant}x{\\leqslant}1and\n{-}1{\\leqslant}y{\\leqslant}1) require\nthat the optimal\nsolution is within\nthe box, resulting in\nan optimal value\nindicated by the\nstar.\n{-}3{-}2{-}1 0 1 2 3\nx1{-}3{-}2{-}10123x2\nsuch as training deep neural networks on millions of images (Dean et al.,\n2012), topic models (Hoffman et al., 2013), reinforcement learning (Mnih\net al., 2015), or training of large-scale Gaussian process models (Hensman\net al., 2013; Gal et al., 2014).\n7.2 Constrained Optimization and Lagrange Multipliers\nIn the previous section, we considered the problem of solving for the min-\nimum of a function\nmin\nxf(x), (7.16)\nwhere f:RD{\\textrightarrow}R.\nIn this section, we have additional constraints. That is, for real-valued\nfunctions gi:RD{\\textrightarrow}Rfori= 1, . . . , m , we consider the constrained\noptimization problem (see Figure 7.4 for an illustration)\nmin\nxf(x) (7.17)\nsubject to gi(x){\\leqslant}0for all i= 1, . . . , m .\nIt is worth pointing out that the functions fandgicould be non-convex\nin general, and we will consider the convex case in the next section.",
                    "\nOne obvious, but not very practical, way of converting the constrained\nproblem (7.17) into an unconstrained one is to use an indicator function\nJ(x) =f(x) +mX\ni=11(gi(x)), (7.18)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n234 Continuous Optimization\nwhere 1(z)is an infinite step function\n1(z) =(\n0ifz{\\leqslant}0\n{\\infty}otherwise. (7.19)\nThis gives infinite penalty if the constraint is not satisfied, and hence\nwould provide the same solution. However, this infinite step function is\nequally difficult to optimize. We can overcome this difficulty by introduc-\ningLagrange multipliers . The idea of Lagrange multipliers is to replace the Lagrange multiplier\nstep function with a linear function.\nWe associate to problem (7.17) the Lagrangian by introducing the La- Lagrangian\ngrange multipliers {\\lambda}i{\\geqslant}0corresponding to each inequality constraint re-\nspectively (Boyd and Vandenberghe, 2004, chapter 4) so that\nL(x,{\\lambda}) =f(x) +mX\ni=1{\\lambda}igi(x) (7.20a)\n=f(x) +{\\lambda}{\\top}g(x), (7.20b)\nwhere in the last line we have concatenated all constraints gi(x)into a\nvector g(x), and all the Lagrange multipliers into a vector {\\lambda}{\\in}Rm.\nWe now introduce the idea of Lagrangian duality. In general, duality\nin optimization is the idea of converting an optimization problem in one\nset of variables x(called the primal variables), into another optimization\nproblem in a different set of variables {\\lambda}(called the dual variables). We\nintroduce two different approaches to duality: In this section, we discuss\nLagrangian duality; in Section 7.3.3, we discuss Legendre-Fenchel duality.\nDefinition 7.1. The problem in (7.17)\nmin\nxf(x) (7.21)\nsubject to g",
                    "i(x){\\leqslant}0for all i= 1, . . . , m\nis known as the primal problem , corresponding to the primal variables x. primal problem\nThe associated Lagrangian dual problem is given by Lagrangian dual\nproblem\nmax\n{\\lambda}{\\in}RmD({\\lambda})\nsubject to {\\lambda}{\\geqslant}0,(7.22)\nwhere {\\lambda}are the dual variables and D({\\lambda}) = min x{\\in}RdL(x,{\\lambda}).\nRemark. In the discussion of Definition 7.1, we use two concepts that are\nalso of independent interest (Boyd and Vandenberghe, 2004).\nFirst is the minimax inequality , which says that for any function with minimax inequality\ntwo arguments {\\varphi}(x,y), the maximin is less than the minimax, i.e.,\nmax\nymin\nx{\\varphi}(x,y){\\leqslant}min\nxmax\ny{\\varphi}(x,y). (7.23)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n7.2 Constrained Optimization and Lagrange Multipliers 235\nThis inequality can be proved by considering the inequality\nFor all x,y min\nx{\\varphi}(x,y){\\leqslant}max\ny{\\varphi}(x,y). (7.24)\nNote that taking the maximum over yof the left-hand side of (7.24) main-\ntains the inequality since the inequality is true for all y. Similarly, we can\ntake the minimum over xof the right-hand side of (7.24) to obtain (7.23).\nThe second concept is weak duality , which uses (7.23) to show that weak duality\nprimal values are always greater than or equal to dual values. This is de-\nscribed in more detail in (7.27). {\\diamond}\nRecall that the difference between J(x)in (7.18) and the Lagrangian\nin (7.20b) is that we have relaxed the indicator function to a linear func-\ntion. Therefore, when {\\lambda}{",
                    "\\geqslant}0, the Lagrangian L(x,{\\lambda})is a lower bound of\nJ(x). Hence, the maximum of L(x,{\\lambda})with respect to {\\lambda}is\nJ(x) = max\n{\\lambda}{\\geqslant}0L(x,{\\lambda}). (7.25)\nRecall that the original problem was minimizing J(x),\nmin\nx{\\in}Rdmax\n{\\lambda}{\\geqslant}0L(x,{\\lambda}). (7.26)\nBy the minimax inequality (7.23), it follows that swapping the order of\nthe minimum and maximum results in a smaller value, i.e.,\nmin\nx{\\in}Rdmax\n{\\lambda}{\\geqslant}0L(x,{\\lambda}){\\geqslant}max\n{\\lambda}{\\geqslant}0min\nx{\\in}RdL(x,{\\lambda}). (7.27)\nThis is also known as weak duality . Note that the inner part of the right- weak duality\nhand side is the dual objective function D({\\lambda})and the definition follows.\nIn contrast to the original optimization problem, which has constraints,\nminx{\\in}RdL(x,{\\lambda})is an unconstrained optimization problem for a given\nvalue of {\\lambda}. If solving minx{\\in}RdL(x,{\\lambda})is easy, then the overall problem is\neasy to solve. We can see this by observing from (7.20b) that L(x,{\\lambda})is\naffine with respect to {\\lambda}. Therefore minx{\\in}RdL(x,{\\lambda})is a pointwise min-\nimum of affine functions of {\\lambda}, and hence D({\\lambda})is concave even though\nf({\\textperiodcentered})andgi({\\textperiodcentered})may be nonconvex. The outer problem, maximization over\n{\\lambda}, is the maximum of a concave function and can be efficiently computed.\nAssuming f({\\textperiodcentered})andgi({\\textperiodcentered})are differentiable, we find the Lagrange dual\nproblem by differentiating the Lagrangian with respect to x, setting the\ndifferential to zero, and solving for the optimal value. We will discuss two\nconcrete",
                    " examples in Sections 7.3.1 and 7.3.2, where f({\\textperiodcentered})andgi({\\textperiodcentered})are\nconvex.\nRemark (Equality Constraints) .Consider (7.17) with additional equality\nconstraints\nmin\nxf(x)\nsubject to gi(x){\\leqslant}0for all i= 1, . . . , m\nhj(x) = 0 for all j= 1, . . . , n .(7.28)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n236 Continuous Optimization\nWe can model equality constraints by replacing them with two inequality\nconstraints. That is for each equality constraint hj(x) = 0 we equivalently\nreplace it by two constraints hj(x){\\leqslant}0andhj(x){\\geqslant}0. It turns out that\nthe resulting Lagrange multipliers are then unconstrained.\nTherefore, we constrain the Lagrange multipliers corresponding to the\ninequality constraints in (7.28) to be non-negative, and leave the La-\ngrange multipliers corresponding to the equality constraints unconstrained.\n{\\diamond}\n7.3 Convex Optimization\nWe focus our attention of a particularly useful class of optimization prob-\nlems, where we can guarantee global optimality. When f({\\textperiodcentered})is a convex\nfunction, and when the constraints involving g({\\textperiodcentered})andh({\\textperiodcentered})are convex sets,\nthis is called a convex optimization problem . In this setting, we have strong convex optimization\nproblem\nstrong dualityduality : The optimal solution of the dual problem is the same as the opti-\nmal solution of the primal problem. The distinction between convex func-\ntions and convex sets are often not strictly presented in machine learning\nliterature, but one can often infer the implied meaning from context.\nDefinition 7.2. A setCis aconvex set if for any x, y{\\in} Cand for any scalar convex set\n{\\theta}with0{\\leqslant}{\\theta}{\\leqslant}1, we have\n{\\",
                    "theta}x+ (1{-}{\\theta})y{\\in} C. (7.29)\nFigure 7.5 Example\nof a convex set.\n Convex sets are sets such that a straight line connecting any two ele-\nments of the set lie inside the set. Figures 7.5 and 7.6 illustrate convex\nand nonconvex sets, respectively.\nFigure 7.6 Example\nof a nonconvex set.\nConvex functions are functions such that a straight line between any\ntwo points of the function lie above the function. Figure 7.2 shows a non-\nconvex function, and Figure 7.3 shows a convex function. Another convex\nfunction is shown in Figure 7.7.\nDefinition 7.3. Let function f:RD{\\textrightarrow}Rbe a function whose domain is a\nconvex set. The function fis aconvex function if for all x,yin the domain\nconvex functionoff, and for any scalar {\\theta}with0{\\leqslant}{\\theta}{\\leqslant}1, we have\nf({\\theta}x+ (1{-}{\\theta})y){\\leqslant}{\\theta}f(x) + (1 {-}{\\theta})f(y). (7.30)\nRemark. Aconcave function is the negative of a convex function. {\\diamond}\nconcave functionThe constraints involving g({\\textperiodcentered})andh({\\textperiodcentered})in (7.28) truncate functions at a\nscalar value, resulting in sets. Another relation between convex functions\nand convex sets is to consider the set obtained by {\\textquotedblleft}filling in{\\textquotedblright} a convex\nfunction. A convex function is a bowl-like object, and we imagine pouring\nwater into it to fill it up. This resulting filled-in set, called the epigraph of epigraph\nthe convex function, is a convex set.\nIf a function f:Rn{\\textrightarrow}Ris differentiable, we can specify convexity in\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mm",
                    "l-book.com ."
                ]
            },
            {
                "12": [
                    "12\nClassification with Support Vector Machines\nIn many situations, we want our machine learning algorithm to predict\none of a number of (discrete) outcomes. For example, an email client sorts\nmail into personal mail and junk mail, which has two outcomes. Another\nexample is a telescope that identifies whether an object in the night sky\nis a galaxy, star, or planet. There are usually a small number of outcomes,\nand more importantly there is usually no additional structure on these\noutcomes. In this chapter, we consider predictors that output binary val- An example of\nstructure is if the\noutcomes were\nordered, like in the\ncase of small,\nmedium, and large\nt-shirts.ues, i.e., there are only two possible outcomes. This machine learning task\nis called binary classification . This is in contrast to Chapter 9, where we\nbinary classificationconsidered a prediction problem with continuous-valued outputs.\nFor binary classification, the set of possible values that the label/output\ncan attain is binary, and for this chapter we denote them by {\\{}+1,{-}1{\\}}. In\nother words, we consider predictors of the form\nf:RD{\\textrightarrow} {\\{}+1,{-}1{\\}}. (12.1)\nRecall from Chapter 8 that we represent each example (data point) xn\nas a feature vector of Dreal numbers. The labels are often referred to as Input example xn\nmay also be referred\nto as inputs, data\npoints, features, or\ninstances.the positive and negative classes , respectively. One should be careful not\nclassto infer intuitive attributes of positiveness of the +1class. For example,\nin a cancer detection task, a patient with cancer is often labeled +1. In\nprinciple, any two distinct values can be used, e.g., {\\{}True,False{\\}},{\\{}0,1{\\}}\nor{\\{}red,blue{\\}}. The problem of binary classification is well studied, and For probabilistic\nmodels, it is\nmathematically\nconvenient to use\n{\\{}0,1{\\}}as a binary\nrepresentation; see\nthe remark after\nExample 6.12.we defer a survey of other approaches to Section 12.6.\nWe present an approach known as the support vector machine (SVM),\nwhich solves the binary",
                    " classification task. As in regression, we have a su-\npervised learning task, where we have a set of examples xn{\\in}RDalong\nwith their corresponding (binary) labels yn{\\in} {\\{}+1,{-}1{\\}}. Given a train-\ning data set consisting of example{\\textendash}label pairs {\\{}(x1, y1), . . . , (xN, yN){\\}}, we\nwould like to estimate parameters of the model that will give the smallest\nclassification error. Similar to Chapter 9, we consider a linear model, and\nhide away the nonlinearity in a transformation {\\phi}of the examples (9.13).\nWe will revisit {\\phi}in Section 12.4.\nThe SVM provides state-of-the-art results in many applications, with\nsound theoretical guarantees (Steinwart and Christmann, 2008). There\nare two main reasons why we chose to illustrate binary classification using\n370\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n{\\textcopyright}by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com .\nClassification with Support Vector Machines 371\nFigure 12.1\nExample 2D data,\nillustrating the\nintuition of data\nwhere we can find a\nlinear classifier that\nseparates orange\ncrosses from blue\ndiscs.\nx(1)x(2)\nSVMs. First, the SVM allows for a geometric way to think about supervised\nmachine learning. While in Chapter 9 we considered the machine learning\nproblem in terms of probabilistic models and attacked it using maximum\nlikelihood estimation and Bayesian inference, here we will consider an\nalternative approach where we reason geometrically about the machine\nlearning task. It relies heavily on concepts, such as inner products and\nprojections, which we discussed in Chapter 3. The second reason why we\nfind SVMs instructive is that in contrast to Chapter 9, the optimization\nproblem for SVM does not admit an analytic solution so that we",
                    " need to\nresort to a variety of optimization tools introduced in Chapter 7.\nThe SVM view of machine learning is subtly different from the max-\nimum likelihood view of Chapter 9. The maximum likelihood view pro-\nposes a model based on a probabilistic view of the data distribution, from\nwhich an optimization problem is derived. In contrast, the SVM view starts\nby designing a particular function that is to be optimized during training,\nbased on geometric intuitions. We have seen something similar already\nin Chapter 10, where we derived PCA from geometric principles. In the\nSVM case, we start by designing a loss function that is to be minimized\non training data, following the principles of empirical risk minimization\n(Section 8.2).\nLet us derive the optimization problem corresponding to training an\nSVM on example{\\textendash}label pairs. Intuitively, we imagine binary classification\ndata, which can be separated by a hyperplane as illustrated in Figure 12.1.\nHere, every example xn(a vector of dimension 2) is a two-dimensional\nlocation ( x(1)\nnandx(2)\nn), and the corresponding binary label ynis one of\ntwo different symbols (orange cross or blue disc). {\\textquotedblleft}Hyperplane{\\textquotedblright} is a word\nthat is commonly used in machine learning, and we encountered hyper-\nplanes already in Section 2.8. A hyperplane is an affine subspace of di-\nmension D{-}1(if the corresponding vector space is of dimension D).\nThe examples consist of two classes (there are two possible labels) that\nhave features (the components of the vector representing the example)\narranged in such a way as to allow us to separate/classify them by draw-\ning a straight line.\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n372 Classification with Support Vector Machines\nIn the following, we formalize the idea of finding a linear separator\nof the two classes. We introduce the idea of the margin and then extend\nlinear separators to allow for examples to fall on the {\\textquotedblleft}wrong{\\textquotedblright} side, incur-\nring a classification error. We present two equivalent ways of formalizing\nthe SVM: the geometric",
                    " view (Section 12.2.4) and the loss function view\n(Section 12.2.5). We derive the dual version of the SVM using Lagrange\nmultipliers (Section 7.2). The dual SVM allows us to observe a third way\nof formalizing the SVM: in terms of the convex hulls of the examples of\neach class (Section 12.3.2). We conclude by briefly describing kernels and\nhow to numerically solve the nonlinear kernel-SVM optimization problem.\n12.1 Separating Hyperplanes\nGiven two examples represented as vectors xiandxj, one way to compute\nthe similarity between them is using an inner product {\\langle}xi,xj{\\rangle}. Recall from\nSection 3.2 that inner products are closely related to the angle between\ntwo vectors. The value of the inner product between two vectors depends\non the length (norm) of each vector. Furthermore, inner products allow\nus to rigorously define geometric concepts such as orthogonality and pro-\njections.\nThe main idea behind many classification algorithms is to represent\ndata in RDand then partition this space, ideally in a way that examples\nwith the same label (and no other examples) are in the same partition.\nIn the case of binary classification, the space would be divided into two\nparts corresponding to the positive and negative classes, respectively. We\nconsider a particularly convenient partition, which is to (linearly) split\nthe space into two halves using a hyperplane. Let example x{\\in}RDbe an\nelement of the data space. Consider a function\nf:RD{\\textrightarrow}R (12.2a)\nx7{\\textrightarrow}f(x) :={\\langle}w,x{\\rangle}+b , (12.2b)\nparametrized by w{\\in}RDandb{\\in}R. Recall from Section 2.8 that hy-\nperplanes are affine subspaces. Therefore, we define the hyperplane that\nseparates the two classes in our binary classification problem as\nx{\\in}RD:f(x) = 0\t. (12.3)\nAn illustration of the hyperplane is shown in Figure 12.2, where the\nvector wis a vector normal to the hyperplane and bthe intercept. We can\nderive that wis a normal vector to the hyperplane in (12.3",
                    ") by choosing\nany two examples xaandxbon the hyperplane and showing that the\nvector between them is orthogonal to w. In the form of an equation,\nf(xa){-}f(xb) ={\\langle}w,xa{\\rangle}+b{-}({\\langle}w,xb{\\rangle}+b) (12.4a)\n={\\langle}w,xa{-}xb{\\rangle}, (12.4b)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n12.1 Separating Hyperplanes 373\nFigure 12.2\nEquation of a\nseparating\nhyperplane (12.3).\n(a) The standard\nway of representing\nthe equation in 3D.\n(b) For ease of\ndrawing, we look at\nthe hyperplane edge\non.w\n(a) Separating hyperplane in 3Dw\n.\n0.Positive\n.\nNegativeb\n(b) Projection of the setting in (a) onto\na plane\nwhere the second line is obtained by the linearity of the inner product\n(Section 3.2). Since we have chosen xaandxbto be on the hyperplane,\nthis implies that f(xa) = 0 andf(xb) = 0 and hence {\\langle}w,xa{-}xb{\\rangle}= 0.\nRecall that two vectors are orthogonal when their inner product is zero. wis orthogonal to\nany vector on the\nhyperplane.Therefore, we obtain that wis orthogonal to any vector on the hyperplane.\nRemark. Recall from Chapter 2 that we can think of vectors in different\nways. In this chapter, we think of the parameter vector was an arrow\nindicating a direction, i.e., we consider wto be a geometric vector. In\ncontrast, we think of the example vector xas a data point (as indicated\nby its coordinates), i.e., we consider xto be the coordinates of a vector\nwith respect to the standard basis. {\\diamond}\nWhen presented with a test example, we classify the example as pos-\nitive or negative depending on the side of the hyperplane on which it",
                    "\noccurs. Note that (12.3) not only defines a hyperplane; it additionally de-\nfines a direction. In other words, it defines the positive and negative side\nof the hyperplane. Therefore, to classify a test example xtest, we calcu-\nlate the value of the function f(xtest)and classify the example as +1if\nf(xtest){\\geqslant}0and{-}1otherwise. Thinking geometrically, the positive ex-\namples lie {\\textquotedblleft}above{\\textquotedblright} the hyperplane and the negative examples {\\textquotedblleft}below{\\textquotedblright} the\nhyperplane.\nWhen training the classifier, we want to ensure that the examples with\npositive labels are on the positive side of the hyperplane, i.e.,\n{\\langle}w,xn{\\rangle}+b{\\geqslant}0 when yn= +1 (12.5)\nand the examples with negative labels are on the negative side, i.e.,\n{\\langle}w,xn{\\rangle}+b {<}0 when yn={-}1. (12.6)\nRefer to Figure 12.2 for a geometric intuition of positive and negative\nexamples. These two conditions are often presented in a single equation\nyn({\\langle}w,xn{\\rangle}+b){\\geqslant}0. (12.7)\nEquation (12.7) is equivalent to (12.5) and (12.6) when we multiply both\nsides of (12.5) and (12.6) with yn= 1andyn={-}1, respectively.\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n374 Classification with Support Vector Machines\nFigure 12.3\nPossible separating\nhyperplanes. There\nare many linear\nclassifiers (green\nlines) that separate\norange crosses from\nblue discs.\nx(1)x(2)\n12.2 Primal Support Vector Machine\nBased on the concept of distances from points to a hyperplane, we now\nare in a position to discuss the support vector machine. For a dataset\n{\\{}(x1",
                    ", y1), . . . , (xN, yN){\\}}that is linearly separable, we have infinitely many\ncandidate hyperplanes (refer to Figure 12.3), and therefore classifiers,\nthat solve our classification problem without any (training) errors. To find\na unique solution, one idea is to choose the separating hyperplane that\nmaximizes the margin between the positive and negative examples. In\nother words, we want the positive and negative examples to be separated\nby a large margin (Section 12.2.1). In the following, we compute the dis- A classifier with\nlarge margin turns\nout to generalize\nwell (Steinwart and\nChristmann, 2008).tance between an example and a hyperplane to derive the margin. Recall\nthat the closest point on the hyperplane to a given point (example xn) is\nobtained by the orthogonal projection (Section 3.8).\n12.2.1 Concept of the Margin\nThe concept of the margin is intuitively simple: It is the distance of the margin\nseparating hyperplane to the closest examples in the dataset, assuming There could be two\nor more closest\nexamples to a\nhyperplane.that the dataset is linearly separable. However, when trying to formalize\nthis distance, there is a technical wrinkle that may be confusing. The tech-\nnical wrinkle is that we need to define a scale at which to measure the\ndistance. A potential scale is to consider the scale of the data, i.e., the raw\nvalues of xn. There are problems with this, as we could change the units\nof measurement of xnand change the values in xn, and, hence, change\nthe distance to the hyperplane. As we will see shortly, we define the scale\nbased on the equation of the hyperplane (12.3) itself.\nConsider a hyperplane {\\langle}w,x{\\rangle}+b, and an example xaas illustrated in\nFigure 12.4. Without loss of generality, we can consider the example xa\nto be on the positive side of the hyperplane, i.e., {\\langle}w,xa{\\rangle}+b {>}0. We\nwould like to compute the distance r {>}0ofxafrom the hyperplane. We\ndo so by considering the orthogonal projection (Section 3",
                    ".8) of xaonto\nthe hyperplane, which we denote by x{'}\na. Since wis orthogonal to the\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n12.2 Primal Support Vector Machine 375\nFigure 12.4 Vector\naddition to express\ndistance to\nhyperplane:\nxa=x{'}\na+rw\n{\\parallel}w{\\parallel}.\n.0.xa\nw.x{'}\nar\nhyperplane, we know that the distance ris just a scaling of this vector w.\nIf the length of wis known, then we can use this scaling factor rfactor\nto work out the absolute distance between xaandx{'}\na. For convenience,\nwe choose to use a vector of unit length (its norm is 1) and obtain this\nby dividing wby its norm,w\n{\\parallel}w{\\parallel}. Using vector addition (Section 2.4), we\nobtain\nxa=x{'}\na+rw\n{\\parallel}w{\\parallel}. (12.8)\nAnother way of thinking about ris that it is the coordinate of xain the\nsubspace spanned by w/{\\parallel}w{\\parallel}. We have now expressed the distance of xa\nfrom the hyperplane as r, and if we choose xato be the point closest to\nthe hyperplane, this distance ris the margin.\nRecall that we would like the positive examples to be further than r\nfrom the hyperplane, and the negative examples to be further than dis-\ntance r(in the negative direction) from the hyperplane. Analogously to\nthe combination of (12.5) and (12.6) into (12.7), we formulate this ob-\njective as\nyn({\\langle}w,xn{\\rangle}+b){\\geqslant}r . (12.9)\nIn other words, we combine the requirements that examples are at least\nraway from the hyperplane (in the positive and negative direction) into\none single inequality.\nSince we are interested only in the direction, we add an assumption to\nour model that the parameter vector wis of unit length, i.e., {\\parallel",
                    "}w{\\parallel}= 1,\nwhere we use the Euclidean norm {\\parallel}w{\\parallel}={\\sqrt{}}\nw{\\top}w(Section 3.1). This We will see other\nchoices of inner\nproducts\n(Section 3.2) in\nSection 12.4.assumption also allows a more intuitive interpretation of the distance r\n(12.8) since it is the scaling factor of a vector of length 1.\nRemark. A reader familiar with other presentations of the margin would\nnotice that our definition of {\\parallel}w{\\parallel}= 1 is different from the standard\npresentation if the SVM was the one provided by Sch {\\textasciidieresis}olkopf and Smola\n(2002), for example. In Section 12.2.3, we will show the equivalence of\nboth approaches. {\\diamond}\nCollecting the three requirements into a single constrained optimization\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n376 Classification with Support Vector Machines\nFigure 12.5\nDerivation of the\nmargin: r=1\n{\\parallel}w{\\parallel}..xa\nw\n{\\langle}w,x{\\rangle}+\nb= 0{\\langle}w,x{\\rangle}+\nb= 1.x{'}\nar\nproblem, we obtain the objective\nmax\nw,b,rr|{\\{}z{\\}}\nmargin\nsubject to yn({\\langle}w,xn{\\rangle}+b){\\geqslant}r| {\\{}z {\\}}\ndata fitting,{\\parallel}w{\\parallel}= 1|{\\{}z{\\}}\nnormalization, r {>} 0,(12.10)\nwhich says that we want to maximize the margin rwhile ensuring that\nthe data lies on the correct side of the hyperplane.\nRemark. The concept of the margin turns out to be highly pervasive in ma-\nchine learning. It was used by Vladimir Vapnik and Alexey Chervonenkis\nto show that when the margin is large, the {\\textquotedblleft}complexity{\\textquotedblright} of the function\nclass is low, and hence",
                    " learning is possible (Vapnik, 2000). It turns out\nthat the concept is useful for various different approaches for theoret-\nically analyzing generalization error (Steinwart and Christmann, 2008;\nShalev-Shwartz and Ben-David, 2014). {\\diamond}\n12.2.2 Traditional Derivation of the Margin\nIn the previous section, we derived (12.10) by making the observation that\nwe are only interested in the direction of wand not its length, leading to\nthe assumption that {\\parallel}w{\\parallel}= 1. In this section, we derive the margin max-\nimization problem by making a different assumption. Instead of choosing\nthat the parameter vector is normalized, we choose a scale for the data.\nWe choose this scale such that the value of the predictor {\\langle}w,x{\\rangle}+bis1at\nthe closest example. Let us also denote the example in the dataset that is Recall that we\ncurrently consider\nlinearly separable\ndata.closest to the hyperplane by xa.\nFigure 12.5 is identical to Figure 12.4, except that now we rescaled the\naxes, such that the example xalies exactly on the margin, i.e., {\\langle}w,xa{\\rangle}+\nb= 1. Since x{'}\nais the orthogonal projection of xaonto the hyperplane, it\nmust by definition lie on the hyperplane, i.e.,\n{\\langle}w,x{'}\na{\\rangle}+b= 0. (12.11)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n12.2 Primal Support Vector Machine 377\nBy substituting (12.8) into (12.11), we obtain\n\nw,xa{-}rw\n{\\parallel}w{\\parallel}\n+b= 0. (12.12)\nExploiting the bilinearity of the inner product (see Section 3.2), we get\n{\\langle}w,xa{\\rangle}+b{-}r{\\langle}w,w{\\rangle}\n{\\parallel}w{\\parallel}= 0. (12.13)\nObserve that",
                    " the first term is 1by our assumption of scale, i.e., {\\langle}w,xa{\\rangle}+\nb= 1. From (3.16) in Section 3.1, we know that {\\langle}w,w{\\rangle}={\\parallel}w{\\parallel}2. Hence,\nthe second term reduces to r{\\parallel}w{\\parallel}. Using these simplifications, we obtain\nr=1\n{\\parallel}w{\\parallel}. (12.14)\nThis means we derived the distance rin terms of the normal vector w\nof the hyperplane. At first glance, this equation is counterintuitive as we We can also think of\nthe distance as the\nprojection error that\nincurs when\nprojecting xaonto\nthe hyperplane.seem to have derived the distance from the hyperplane in terms of the\nlength of the vector w, but we do not yet know this vector. One way to\nthink about it is to consider the distance rto be a temporary variable\nthat we only use for this derivation. Therefore, for the rest of this section\nwe will denote the distance to the hyperplane by1\n{\\parallel}w{\\parallel}. In Section 12.2.3,\nwe will see that the choice that the margin equals 1is equivalent to our\nprevious assumption of {\\parallel}w{\\parallel}= 1in Section 12.2.1.\nSimilar to the argument to obtain (12.9), we want the positive and\nnegative examples to be at least 1away from the hyperplane, which yields\nthe condition\nyn({\\langle}w, xn{\\rangle}+b){\\geqslant}1. (12.15)\nCombining the margin maximization with the fact that examples need to\nbe on the correct side of the hyperplane (based on their labels) gives us\nmax\nw,b1\n{\\parallel}w{\\parallel}(12.16)\nsubject to yn({\\langle}w,xn{\\rangle}+b){\\geqslant}1 for all n= 1, . . . , N. (12.17)\nInstead of maximizing the reciprocal of the norm as in (12.16), we often\nminimize the squared norm. We also often include a constant1\n2that does The squared norm\nresults in",
                    " a convex\nquadratic\nprogramming\nproblem for the\nSVM (Section 12.5).not affect the optimal w, bbut yields a tidier form when we compute the\ngradient. Then, our objective becomes\nmin\nw,b1\n2{\\parallel}w{\\parallel}2(12.18)\nsubject to yn({\\langle}w,xn{\\rangle}+b){\\geqslant}1 for all n= 1, . . . , N . (12.19)\nEquation (12.18) is known as the hard margin SVM . The reason for the hard margin SVM\nexpression {\\textquotedblleft}hard{\\textquotedblright} is because the formulation does not allow for any vi-\nolations of the margin condition. We will see in Section 12.2.4 that this\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n378 Classification with Support Vector Machines\n{\\textquotedblleft}hard{\\textquotedblright} condition can be relaxed to accommodate violations if the data is\nnot linearly separable.\n12.2.3 Why We Can Set the Margin to 1\nIn Section 12.2.1, we argued that we would like to maximize some value\nr, which represents the distance of the closest example to the hyperplane.\nIn Section 12.2.2, we scaled the data such that the closest example is of\ndistance 1to the hyperplane. In this section, we relate the two derivations,\nand show that they are equivalent.\nTheorem 12.1. Maximizing the margin r, where we consider normalized\nweights as in (12.10) ,\nmax\nw,b,rr|{\\{}z{\\}}\nmargin\nsubject to yn({\\langle}w,xn{\\rangle}+b){\\geqslant}r| {\\{}z {\\}}\ndata fitting,{\\parallel}w{\\parallel}= 1|{\\{}z{\\}}\nnormalization, r {>} 0,(12.20)\nis equivalent to scaling the data, such that the margin is unity:\nmin\nw,b1\n2{\\parallel}w{\\parallel}2\n|{\\{}",
                    "z{\\}}\nmargin\nsubject to yn({\\langle}w,xn{\\rangle}+b){\\geqslant}1| {\\{}z {\\}}\ndata fitting.(12.21)\nProof Consider (12.20). Since the square is a strictly monotonic trans-\nformation for non-negative arguments, the maximum stays the same if we\nconsider r2in the objective. Since {\\parallel}w{\\parallel}= 1 we can reparametrize the\nequation with a new weight vector w{'}that is not normalized by explicitly\nusingw{'}\n{\\parallel}w{'}{\\parallel}. We obtain\nmax\nw{'},b,rr2\nsubject to ynw{'}\n{\\parallel}w{'}{\\parallel},xn\n+b\n{\\geqslant}r, r {>} 0.(12.22)\nEquation (12.22) explicitly states that the distance ris positive. Therefore,\nwe can divide the first constraint by r, which yields Note that r {>}0\nbecause we\nassumed linear\nseparability, and\nhence there is no\nissue to divide by r.max\nw{'},b,rr2\nsubject to yn\n*\nw{'}\n{\\parallel}w{'}{\\parallel}r|{\\{}z{\\}}\nw{'}{'},xn+\n+b\nr|{\\{}z{\\}}\nb{'}{'}\n{\\geqslant}1, r {>} 0(12.23)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n12.2 Primal Support Vector Machine 379\nFigure 12.6\n(a) Linearly\nseparable and\n(b) non-linearly\nseparable data.\nx(1)x(2)\n(a) Linearly separable data, with a large\nmargin\nx(1)x(2)(b) Non-linearly separable data\nrenaming the parameters to w{'}{'}andb{'}{'}. Since w{'}{'}=w{'}\n{\\parallel}w{'}{\\par",
                    "allel}r, rearranging for\nrgives\n{\\parallel}w{'}{'}{\\parallel}=\n\n\n\nw{'}\n{\\parallel}w{'}{\\parallel}r\n\n\n\n=1\nr{\\textperiodcentered}\n\n\n\nw{'}\n{\\parallel}w{'}{\\parallel}\n\n\n\n=1\nr. (12.24)\nBy substituting this result into (12.23), we obtain\nmax\nw{'}{'},b{'}{'}1\n{\\parallel}w{'}{'}{\\parallel}2\nsubject to yn({\\langle}w{'}{'},xn{\\rangle}+b{'}{'}){\\geqslant}1.(12.25)\nThe final step is to observe that maximizing1\n{\\parallel}w{'}{'}{\\parallel}2yields the same solution\nas minimizing1\n2{\\parallel}w{'}{'}{\\parallel}2, which concludes the proof of Theorem 12.1.\n12.2.4 Soft Margin SVM: Geometric View\nIn the case where data is not linearly separable, we may wish to allow\nsome examples to fall within the margin region, or even to be on the\nwrong side of the hyperplane as illustrated in Figure 12.6.\nThe model that allows for some classification errors is called the soft soft margin SVM\nmargin SVM . In this section, we derive the resulting optimization problem\nusing geometric arguments. In Section 12.2.5, we will derive an equiv-\nalent optimization problem using the idea of a loss function. Using La-\ngrange multipliers (Section 7.2), we will derive the dual optimization\nproblem of the SVM in Section 12.3. This dual optimization problem al-\nlows us to observe a third interpretation of the SVM: as a hyperplane that\nbisects the line between convex hulls corresponding to the positive and\nnegative data examples (Section 12.3.2).\nThe key geometric idea is to introduce a slack variable {\\xi}ncorresponding slack variable\nto each example{\\textendash}label pair (xn, yn)that allows a particular example to be\nwithin the margin or even on the wrong side of the hyperplane (refer",
                    " to\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n380 Classification with Support Vector Machines\nFigure 12.7 Soft\nmargin SVM allows\nexamples to be\nwithin the margin or\non the wrong side of\nthe hyperplane. The\nslack variable {\\xi}\nmeasures the\ndistance of a\npositive example\nx+to the positive\nmargin hyperplane\n{\\langle}w,x{\\rangle}+b= 1\nwhenx+is on the\nwrong side..x+w\n{\\langle}w,x{\\rangle}+\nb= 0{\\langle}w,x{\\rangle}+\nb= 1.\n{\\xi}\nFigure 12.7). We subtract the value of {\\xi}nfrom the margin, constraining\n{\\xi}nto be non-negative. To encourage correct classification of the samples,\nwe add {\\xi}nto the objective\nmin\nw,b,{\\xi}1\n2{\\parallel}w{\\parallel}2+CNX\nn=1{\\xi}n (12.26a)\nsubject to yn({\\langle}w,xn{\\rangle}+b){\\geqslant}1{-}{\\xi}n (12.26b)\n{\\xi}n{\\geqslant}0 (12.26c)\nforn= 1, . . . , N . In contrast to the optimization problem (12.18) for the\nhard margin SVM, this one is called the soft margin SVM . The parameter soft margin SVM\nC {>}0trades off the size of the margin and the total amount of slack that\nwe have. This parameter is called the regularization parameter since, as regularization\nparameter we will see in the following section, the margin term in the objective func-\ntion (12.26a) is a regularization term. The margin term {\\parallel}w{\\parallel}2is called\ntheregularizer , and in many books on numerical optimization, the reg- regularizer\nularization parameter is multiplied with this term (Section 8.2.3). This\nis in contrast to our formulation in this section. Here a large value of C\nimplies low regularization, as we give the slack",
                    " variables larger weight,\nhence giving more priority to examples that do not lie on the correct side\nof the margin. There are\nalternative\nparametrizations of\nthis regularization,\nwhich is\nwhy (12.26a) is also\noften referred to as\ntheC-SVM.Remark. In the formulation of the soft margin SVM (12.26a) wis reg-\nularized, but bis not regularized. We can see this by observing that the\nregularization term does not contain b. The unregularized term bcom-\nplicates theoretical analysis (Steinwart and Christmann, 2008, chapter 1)\nand decreases computational efficiency (Fan et al., 2008). {\\diamond}\n12.2.5 Soft Margin SVM: Loss Function View\nLet us consider a different approach for deriving the SVM, following the\nprinciple of empirical risk minimization (Section 8.2). For the SVM, we\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n12.2 Primal Support Vector Machine 381\nchoose hyperplanes as the hypothesis class, that is\nf(x) ={\\langle}w,x{\\rangle}+b. (12.27)\nWe will see in this section that the margin corresponds to the regulariza-\ntion term. The remaining question is, what is the loss function ? In con- loss function\ntrast to Chapter 9, where we consider regression problems (the output\nof the predictor is a real number), in this chapter, we consider binary\nclassification problems (the output of the predictor is one of two labels\n{\\{}+1,{-}1{\\}}). Therefore, the error/loss function for each single example{\\textendash}\nlabel pair needs to be appropriate for binary classification. For example,\nthe squared loss that is used for regression (9.10b) is not suitable for bi-\nnary classification.\nRemark. The ideal loss function between binary labels is to count the num-\nber of mismatches between the prediction and the label. This means that\nfor a predictor fapplied to an example xn, we compare the output f(xn)\nwith the label yn. We define the loss to be zero if they match",
                    ", and one if\nthey do not match. This is denoted by 1(f(xn)=yn)and is called the\nzero-one loss . Unfortunately, the zero-one loss results in a combinatorial zero-one loss\noptimization problem for finding the best parameters w, b. Combinatorial\noptimization problems (in contrast to continuous optimization problems\ndiscussed in Chapter 7) are in general more challenging to solve. {\\diamond}\nWhat is the loss function corresponding to the SVM? Consider the error\nbetween the output of a predictor f(xn)and the label yn. The loss de-\nscribes the error that is made on the training data. An equivalent way to\nderive (12.26a) is to use the hinge loss hinge loss\n{\\ell}(t) = max {\\{}0,1{-}t{\\}}where t=yf(x) =y({\\langle}w,x{\\rangle}+b).(12.28)\nIff(x)is on the correct side (based on the corresponding label y) of the\nhyperplane, and further than distance 1, this means that t{\\geqslant}1and the\nhinge loss returns a value of zero. If f(x)is on the correct side but too\nclose to the hyperplane ( 0{<} t {<} 1), the example xis within the margin,\nand the hinge loss returns a positive value. When the example is on the\nwrong side of the hyperplane ( t {<}0), the hinge loss returns an even larger\nvalue, which increases linearly. In other words, we pay a penalty once we\nare closer than the margin to the hyperplane, even if the prediction is\ncorrect, and the penalty increases linearly. An alternative way to express\nthe hinge loss is by considering it as two linear pieces\n{\\ell}(t) =(\n0 if t{\\geqslant}1\n1{-}tift {<}1, (12.29)\nas illustrated in Figure 12.8. The loss corresponding to the hard margin\nSVM 12.18 is defined as\n{\\ell}(t) =(\n0 if t{\\geqslant}1\n{\\infty}ift {<}1. (12.30)\n{\\textcopyright}2024 M. P. Deisenroth",
                    ", A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n382 Classification with Support Vector Machines\nFigure 12.8 The\nhinge loss is a\nconvex upper bound\nof zero-one loss.\n{-}2 0 2\nt024max{\\{}0,1{-}t{\\}}Zero-one loss\nHinge loss\nThis loss can be interpreted as never allowing any examples inside the\nmargin.\nFor a given training set {\\{}(x1, y1), . . . , (xN, yN){\\}}, we seek to minimize\nthe total loss, while regularizing the objective with {\\ell}2-regularization (see\nSection 8.2.3). Using the hinge loss (12.28) gives us the unconstrained\noptimization problem\nmin\nw,b1\n2{\\parallel}w{\\parallel}2\n|{\\{}z{\\}}\nregularizer+CNX\nn=1max{\\{}0,1{-}yn({\\langle}w,xn{\\rangle}+b){\\}}\n| {\\{}z {\\}}\nerror term. (12.31)\nThe first term in (12.31) is called the regularization term or the regularizer regularizer\n(see Section 8.2.3), and the second term is called the loss term or the error loss term\nerror termterm. Recall from Section 12.2.4 that the term1\n2{\\parallel}w{\\parallel}2arises directly from\nthe margin. In other words, margin maximization can be interpreted as\nregularization . regularization\nIn principle, the unconstrained optimization problem in (12.31) can\nbe directly solved with (sub-)gradient descent methods as described in\nSection 7.1. To see that (12.31) and (12.26a) are equivalent, observe that\nthe hinge loss (12.28) essentially consists of two linear parts, as expressed\nin (12.29). Consider the hinge loss for a single example-label pair (12.28).\nWe can equivalently replace minimization of the hinge loss over twith a\nminimization of a slack variable {\\xi}with two constraints. In equation form,\nmin\ntmax{\\{}0,1{-}t{\\}} (12.32)",
                    "\nis equivalent to\nmin\n{\\xi},t{\\xi}\nsubject to {\\xi}{\\geqslant}0, {\\xi}{\\geqslant}1{-}t .(12.33)\nBy substituting this expression into (12.31) and rearranging one of the\nconstraints, we obtain exactly the soft margin SVM (12.26a).\nRemark. Let us contrast our choice of the loss function in this section to the\nloss function for linear regression in Chapter 9. Recall from Section 9.2.1\nthat for finding maximum likelihood estimators, we usually minimize the\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n12.3 Dual Support Vector Machine 383\nnegative log-likelihood. Furthermore, since the likelihood term for linear\nregression with Gaussian noise is Gaussian, the negative log-likelihood for\neach example is a squared error function. The squared error function is the\nloss function that is minimized when looking for the maximum likelihood\nsolution. {\\diamond}\n12.3 Dual Support Vector Machine\nThe description of the SVM in the previous sections, in terms of the vari-\nableswandb, is known as the primal SVM. Recall that we consider inputs\nx{\\in}RDwith Dfeatures. Since wis of the same dimension as x, this\nmeans that the number of parameters (the dimension of w) of the opti-\nmization problem grows linearly with the number of features.\nIn the following, we consider an equivalent optimization problem (the\nso-called dual view), which is independent of the number of features. In-\nstead, the number of parameters increases with the number of examples\nin the training set. We saw a similar idea appear in Chapter 10, where we\nexpressed the learning problem in a way that does not scale with the num-\nber of features. This is useful for problems where we have more features\nthan the number of examples in the training dataset. The dual SVM also\nhas the additional advantage that it easily allows kernels to be applied,\nas we shall see at the end of this chapter. The word {\\textquotedblleft}dual{\\textquotedblright} appears often\nin mathematical literature, and in this particular case it refers to conve",
                    "x\nduality. The following subsections are essentially an application of convex\nduality, which we discussed in Section 7.2.\n12.3.1 Convex Duality via Lagrange Multipliers\nRecall the primal soft margin SVM (12.26a). We call the variables w,b,\nand{\\xi}corresponding to the primal SVM the primal variables. We use {\\alpha}n{\\geqslant} In Chapter 7, we\nused{\\lambda}as Lagrange\nmultipliers. In this\nsection, we follow\nthe notation\ncommonly chosen in\nSVM literature, and\nuse{\\alpha}and{\\gamma}.0as the Lagrange multiplier corresponding to the constraint (12.26b) that\nthe examples are classified correctly and {\\gamma}n{\\geqslant}0as the Lagrange multi-\nplier corresponding to the non-negativity constraint of the slack variable;\nsee (12.26c). The Lagrangian is then given by\nL(w, b, {\\xi}, {\\alpha}, {\\gamma} ) =1\n2{\\parallel}w{\\parallel}2+CNX\nn=1{\\xi}n (12.34)\n{-}NX\nn=1{\\alpha}n(yn({\\langle}w,xn{\\rangle}+b){-}1 +{\\xi}n)\n| {\\{}z {\\}}\nconstraint (12.26b){-}NX\nn=1{\\gamma}n{\\xi}n\n|{\\{}z{\\}}\nconstraint (12.26c).\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n384 Classification with Support Vector Machines\nBy differentiating the Lagrangian (12.34) with respect to the three primal\nvariables w,b, and {\\xi}respectively, we obtain\n{\\partial}L\n{\\partial}w=w{\\top}{-}NX\nn=1{\\alpha}nynxn{\\top}, (12.35)\n{\\partial}L\n{\\partial}b={-}NX\nn=1{\\alpha}nyn, (12.36)\n{\\partial}L\n{\\partial}{\\xi}",
                    "n=C{-}{\\alpha}n{-}{\\gamma}n. (12.37)\nWe now find the maximum of the Lagrangian by setting each of these\npartial derivatives to zero. By setting (12.35) to zero, we find\nw=NX\nn=1{\\alpha}nynxn, (12.38)\nwhich is a particular instance of the representer theorem (Kimeldorf and representer theorem\nWahba, 1970). Equation (12.38) states that the optimal weight vector in The representer\ntheorem is actually\na collection of\ntheorems saying\nthat the solution of\nminimizing\nempirical risk lies in\nthe subspace\n(Section 2.4.3)\ndefined by the\nexamples.the primal is a linear combination of the examples xn. Recall from Sec-\ntion 2.6.1 that this means that the solution of the optimization problem\nlies in the span of training data. Additionally, the constraint obtained by\nsetting (12.36) to zero implies that the optimal weight vector is an affine\ncombination of the examples. The representer theorem turns out to hold\nfor very general settings of regularized empirical risk minimization (Hof-\nmann et al., 2008; Argyriou and Dinuzzo, 2014). The theorem has more\ngeneral versions (Sch {\\textasciidieresis}olkopf et al., 2001), and necessary and sufficient\nconditions on its existence can be found in Yu et al. (2013).\nRemark. The representer theorem (12.38) also provides an explanation\nof the name {\\textquotedblleft}support vector machine.{\\textquotedblright} The examples xn, for which the\ncorresponding parameters {\\alpha}n= 0, do not contribute to the solution wat\nall. The other examples, where {\\alpha}n{>}0, are called support vectors since support vector\nthey {\\textquotedblleft}support{\\textquotedblright} the hyperplane. {\\diamond}\nBy substituting the expression for winto the Lagrangian (12.34), we\nobtain the dual\nD({\\xi}, {\\alpha}, {\\gamma} ) =1\n2NX\ni=1NX\nj=1yiyj{\\alpha}i{\\alpha}j{\\langle",
                    "}xi,xj{\\rangle} {-}NX\ni=1yi{\\alpha}i*NX\nj=1yj{\\alpha}jxj,xi+\n+CNX\ni=1{\\xi}i{-}bNX\ni=1yi{\\alpha}i+NX\ni=1{\\alpha}i{-}NX\ni=1{\\alpha}i{\\xi}i{-}NX\ni=1{\\gamma}i{\\xi}i.\n(12.39)\nNote that there are no longer any terms involving the primal variable w.\nBy setting (12.36) to zero, we obtainPN\nn=1yn{\\alpha}n= 0. Therefore, the term\ninvolving balso vanishes. Recall that inner products are symmetric and\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n12.3 Dual Support Vector Machine 385\nbilinear (see Section 3.2). Therefore, the first two terms in (12.39) are\nover the same objects. These terms (colored blue) can be simplified, and\nwe obtain the Lagrangian\nD({\\xi}, {\\alpha}, {\\gamma} ) ={-}1\n2NX\ni=1NX\nj=1yiyj{\\alpha}i{\\alpha}j{\\langle}xi,xj{\\rangle}+NX\ni=1{\\alpha}i+NX\ni=1(C{-}{\\alpha}i{-}{\\gamma}i){\\xi}i.\n(12.40)\nThe last term in this equation is a collection of all terms that contain slack\nvariables {\\xi}i. By setting (12.37) to zero, we see that the last term in (12.40)\nis also zero. Furthermore, by using the same equation and recalling that\nthe Lagrange multiplers {\\gamma}iare non-negative, we conclude that {\\alpha}i{\\leqslant}C.\nWe now obtain the dual optimization problem of the SVM, which is ex-\npressed exclusively in terms of the Lagrange multipliers {\\alpha}i. Recall from\nLagrangian duality (Definition",
                    " 7.1) that we maximize the dual problem.\nThis is equivalent to minimizing the negative dual problem, such that we\nend up with the dual SVM dual SVM\nmin\n{\\alpha}1\n2NX\ni=1NX\nj=1yiyj{\\alpha}i{\\alpha}j{\\langle}xi,xj{\\rangle} {-}NX\ni=1{\\alpha}i\nsubject toNX\ni=1yi{\\alpha}i= 0\n0{\\leqslant}{\\alpha}i{\\leqslant}Cfor all i= 1, . . . , N .(12.41)\nThe equality constraint in (12.41) is obtained from setting (12.36) to\nzero. The inequality constraint {\\alpha}i{\\geqslant}0is the condition imposed on La-\ngrange multipliers of inequality constraints (Section 7.2). The inequality\nconstraint {\\alpha}i{\\leqslant}Cis discussed in the previous paragraph.\nThe set of inequality constraints in the SVM are called {\\textquotedblleft}box constraints{\\textquotedblright}\nbecause they limit the vector {\\alpha}= [{\\alpha}1,{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}, {\\alpha}N]{\\top}{\\in}RNof Lagrange mul-\ntipliers to be inside the box defined by 0andCon each axis. These\naxis-aligned boxes are particularly efficient to implement in numerical\nsolvers (Dost {\\textasciiacute}al, 2009, chapter 5). It turns out that\nexamples that lie\nexactly on the\nmargin are\nexamples whose\ndual parameters lie\nstrictly inside the\nbox constraints,\n0{<} {\\alpha}i{<} C. This is\nderived using the\nKarush Kuhn Tucker\nconditions, for\nexample in\nSch{\\textasciidieresis}olkopf and\nSmola (2002).Once we obtain the dual parameters {\\alpha}, we can recover the primal pa-\nrameters wby using the representer theorem (12.38). Let us call the op-\ntimal primal parameter w{*}. However, there remains the question on how\nto obtain the parameter b{*}. Consider an example xnthat lies exactly on\nthe margin`s",
                    " boundary, i.e., {\\langle}w{*},xn{\\rangle}+b=yn. Recall that ynis either +1\nor{-}1. Therefore, the only unknown is b, which can be computed by\nb{*}=yn{-} {\\langle}w{*},xn{\\rangle}. (12.42)\nRemark. In principle, there may be no examples that lie exactly on the\nmargin. In this case, we should compute |yn{-} {\\langle}w{*},xn{\\rangle}|for all support\nvectors and take the median value of this absolute value difference to be\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n386 Classification with Support Vector Machines\nFigure 12.9 Convex\nhulls. (a) Convex\nhull of points, some\nof which lie within\nthe boundary;\n(b) convex hulls\naround positive and\nnegative examples.\n(a) Convex hull.\nc\nd (b) Convex hulls around positive (blue) and\nnegative (orange) examples. The distance be-\ntween the two convex sets is the length of the\ndifference vector c{-}d.\nthe value of b{*}. A derivation of this can be found in http://fouryears.\neu/2012/06/07/the-svm-bias-term-conspiracy/ . {\\diamond}\n12.3.2 Dual SVM: Convex Hull View\nAnother approach to obtain the dual SVM is to consider an alternative\ngeometric argument. Consider the set of examples xnwith the same label.\nWe would like to build a convex set that contains all the examples such\nthat it is the smallest possible set. This is called the convex hull and is\nillustrated in Figure 12.9.\nLet us first build some intuition about a convex combination of points.\nConsider two points x1andx2and corresponding non-negative weights\n{\\alpha}1, {\\alpha}2{\\geqslant}0such that {\\alpha}1+{\\alpha}2= 1. The equation {\\alpha}1x1+{\\alpha}2x2describes each",
                    "\npoint on a line between x1andx2. Consider what happens when we add\na third point x3along with a weight {\\alpha}3{\\geqslant}0such thatP3\nn=1{\\alpha}n= 1.\nThe convex combination of these three points x1,x2,x3spans a two-\ndimensional area. The convex hull of this area is the triangle formed by convex hull\nthe edges corresponding to each pair of of points. As we add more points,\nand the number of points becomes greater than the number of dimen-\nsions, some of the points will be inside the convex hull, as we can see in\nFigure 12.9(a).\nIn general, building a convex convex hull can be done by introducing\nnon-negative weights {\\alpha}n{\\geqslant}0corresponding to each example xn. Then\nthe convex hull can be described as the set\nconv ( X) =(NX\nn=1{\\alpha}nxn)\nwithNX\nn=1{\\alpha}n= 1 and {\\alpha}n{\\geqslant}0,(12.43)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n12.3 Dual Support Vector Machine 387\nfor all n= 1, . . . , N . If the two clouds of points corresponding to the\npositive and negative classes are separated, then the convex hulls do not\noverlap. Given the training data (x1, y1), . . . , (xN, yN), we form two con-\nvex hulls, corresponding to the positive and negative classes respectively.\nWe pick a point c, which is in the convex hull of the set of positive exam-\nples, and is closest to the negative class distribution. Similarly, we pick a\npointdin the convex hull of the set of negative examples and is closest to\nthe positive class distribution; see Figure 12.9(b). We define a difference\nvector between dandcas\nw:=c{-}d. (12.44)\nPicking the points canddas in the preceding cases, and requiring them\nto be closest to each other is equivalent to minimizing the length/norm",
                    " of\nw, so that we end up with the corresponding optimization problem\narg min\nw{\\parallel}w{\\parallel}= arg min\nw1\n2{\\parallel}w{\\parallel}2. (12.45)\nSincecmust be in the positive convex hull, it can be expressed as a convex\ncombination of the positive examples, i.e., for non-negative coefficients\n{\\alpha}+\nn\nc=X\nn:yn=+1{\\alpha}+\nnxn. (12.46)\nIn (12.46), we use the notation n:yn= +1 to indicate the set of indices\nnfor which yn= +1 . Similarly, for the examples with negative labels, we\nobtain\nd=X\nn:yn={-}1{\\alpha}{-}\nnxn. (12.47)\nBy substituting (12.44), (12.46), and (12.47) into (12.45), we obtain the\nobjective\nmin\n{\\alpha}1\n2\n\n\n\n\nX\nn:yn=+1{\\alpha}+\nnxn{-}X\nn:yn={-}1{\\alpha}{-}\nnxn\n\n\n\n\n2\n. (12.48)\nLet{\\alpha}be the set of all coefficients, i.e., the concatenation of {\\alpha}+and{\\alpha}{-}.\nRecall that we require that for each convex hull that their coefficients sum\nto one,\nX\nn:yn=+1{\\alpha}+\nn= 1 andX\nn:yn={-}1{\\alpha}{-}\nn= 1. (12.49)\nThis implies the constraint\nNX\nn=1yn{\\alpha}n= 0. (12.50)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n388 Classification with Support Vector Machines\nThis result can be seen by multiplying out the individual classes\nNX\nn=1yn{\\alpha}n=X\nn:yn=+1(+1){\\alpha}+\nn+X\nn:yn={-}1({-}1){\\alpha}{-}\nn (12.51a)\n",
                    "=X\nn:yn=+1{\\alpha}+\nn{-}X\nn:yn={-}1{\\alpha}{-}\nn= 1{-}1 = 0 . (12.51b)\nThe objective function (12.48) and the constraint (12.50), along with the\nassumption that {\\alpha}{\\geqslant}0, give us a constrained (convex) optimization prob-\nlem. This optimization problem can be shown to be the same as that of\nthe dual hard margin SVM (Bennett and Bredensteiner, 2000a).\nRemark. To obtain the soft margin dual, we consider the reduced hull. The\nreduced hull is similar to the convex hull but has an upper bound to the reduced hull\nsize of the coefficients {\\alpha}. The maximum possible value of the elements\nof{\\alpha}restricts the size that the convex hull can take. In other words, the\nbound on {\\alpha}shrinks the convex hull to a smaller volume (Bennett and\nBredensteiner, 2000b). {\\diamond}\n12.4 Kernels\nConsider the formulation of the dual SVM (12.41). Notice that the in-\nner product in the objective occurs only between examples xiandxj.\nThere are no inner products between the examples and the parameters.\nTherefore, if we consider a set of features {\\phi}(xi)to represent xi, the only\nchange in the dual SVM will be to replace the inner product. This mod-\nularity, where the choice of the classification method (the SVM) and the\nchoice of the feature representation {\\phi}(x)can be considered separately,\nprovides flexibility for us to explore the two problems independently. In\nthis section, we discuss the representation {\\phi}(x)and briefly introduce the\nidea of kernels, but do not go into the technical details.\nSince{\\phi}(x)could be a non-linear function, we can use the SVM (which\nassumes a linear classifier) to construct classifiers that are nonlinear in\nthe examples xn. This provides a second avenue, in addition to the soft\nmargin, for users to deal with a dataset that is not linearly separable. It\nturns out that there are many algorithms and statistical methods that have\nthis property that we observed in the dual SVM: the only",
                    " inner products\nare those that occur between examples. Instead of explicitly defining a\nnon-linear feature map {\\phi}({\\textperiodcentered})and computing the resulting inner product\nbetween examples xiandxj, we define a similarity function k(xi,xj)be-\ntween xiandxj. For a certain class of similarity functions, called kernels , kernel\nthe similarity function implicitly defines a non-linear feature map {\\phi}({\\textperiodcentered}).\nKernels are by definition functions k:X {\\texttimes} X {\\textrightarrow} Rfor which there exists The inputs Xof the\nkernel function can\nbe very general and\nare not necessarily\nrestricted to RD.a Hilbert space Hand{\\phi}:X {\\textrightarrow} H a feature map such that\nk(xi,xj) ={\\langle}{\\phi}(xi),{\\phi}(xj){\\rangle}H. (12.52)\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n12.4 Kernels 389\nFigure 12.10 SVM\nwith different\nkernels. Note that\nwhile the decision\nboundary is\nnonlinear, the\nunderlying problem\nbeing solved is for a\nlinear separating\nhyperplane (albeit\nwith a nonlinear\nkernel).\nFirst featureSecond feature\n(a) SVM with linear kernel\nFirst featureSecond feature (b) SVM with RBF kernel\nFirst featureSecond feature\n(c) SVM with polynomial (degree 2) kernel\nFirst featureSecond feature (d) SVM with polynomial (degree 3) kernel\nThere is a unique reproducing kernel Hilbert space associated with every\nkernel k(Aronszajn, 1950; Berlinet and Thomas-Agnan, 2004). In this\nunique association, {\\phi}(x) =k({\\textperiodcentered},x)is called the canonical feature map .canonical feature\nmap The generalization from an inner product to a kernel function (12.52) is\nknown as the kernel trick (Sch{\\textasciidieresis}olkopf and Smola, 2002; Shawe-Taylor and kernel trick\nCristianini, 2004), as it hides away the explicit non-linear feature map.\nThe matrix K",
                    "{\\in}RN{\\texttimes}N, resulting from the inner products or the appli-\ncation of k({\\textperiodcentered},{\\textperiodcentered})to a dataset, is called the Gram matrix , and is often just Gram matrix\nreferred to as the kernel matrix . Kernels must be symmetric and positive kernel matrix\nsemidefinite functions so that every kernel matrix Kis symmetric and\npositive semidefinite (Section 3.2.3):\n{\\forall}z{\\in}RN:z{\\top}Kz{\\geqslant}0. (12.53)\nSome popular examples of kernels for multivariate real-valued data xi{\\in}\nRDare the polynomial kernel, the Gaussian radial basis function kernel,\nand the rational quadratic kernel (Sch {\\textasciidieresis}olkopf and Smola, 2002; Rasmussen\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n390 Classification with Support Vector Machines\nand Williams, 2006). Figure 12.10 illustrates the effect of different kernels\non separating hyperplanes on an example dataset. Note that we are still\nsolving for hyperplanes, that is, the hypothesis class of functions are still\nlinear. The non-linear surfaces are due to the kernel function.\nRemark. Unfortunately for the fledgling machine learner, there are mul-\ntiple meanings of the word {\\textquotedblleft}kernel.{\\textquotedblright} In this chapter, the word {\\textquotedblleft}kernel{\\textquotedblright}\ncomes from the idea of the reproducing kernel Hilbert space (RKHS) (Aron-\nszajn, 1950; Saitoh, 1988). We have discussed the idea of the kernel in lin-\near algebra (Section 2.7.3), where the kernel is another word for the null\nspace. The third common use of the word {\\textquotedblleft}kernel{\\textquotedblright} in machine learning is\nthe smoothing kernel in kernel density estimation (Section 11.5). {\\diamond}\nSince the explicit representation {\\phi}(x)is mathematically equivalent to\nthe kernel representation k(xi,xj), a practitioner will often design the\nkernel function such that it can be computed more efficiently than the",
                    "\ninner product between explicit feature maps. For example, consider the\npolynomial kernel (Sch {\\textasciidieresis}olkopf and Smola, 2002), where the number of\nterms in the explicit expansion grows very quickly (even for polynomials\nof low degree) when the input dimension is large. The kernel function\nonly requires one multiplication per input dimension, which can provide\nsignificant computational savings. Another example is the Gaussian ra-\ndial basis function kernel (Sch {\\textasciidieresis}olkopf and Smola, 2002; Rasmussen and\nWilliams, 2006), where the corresponding feature space is infinite dimen-\nsional. In this case, we cannot explicitly represent the feature space but\ncan still compute similarities between a pair of examples using the kernel. The choice of\nkernel, as well as\nthe parameters of\nthe kernel, is often\nchosen using nested\ncross-validation\n(Section 8.6.1).Another useful aspect of the kernel trick is that there is no need for\nthe original data to be already represented as multivariate real-valued\ndata. Note that the inner product is defined on the output of the function\n{\\phi}({\\textperiodcentered}), but does not restrict the input to real numbers. Hence, the function\n{\\phi}({\\textperiodcentered})and the kernel function k({\\textperiodcentered},{\\textperiodcentered})can be defined on any object, e.g.,\nsets, sequences, strings, graphs, and distributions (Ben-Hur et al., 2008;\nG{\\textasciidieresis}artner, 2008; Shi et al., 2009; Sriperumbudur et al., 2010; Vishwanathan\net al., 2010).\n12.5 Numerical Solution\nWe conclude our discussion of SVMs by looking at how to express the\nproblems derived in this chapter in terms of the concepts presented in\nChapter 7. We consider two different approaches for finding the optimal\nsolution for the SVM. First we consider the loss view of SVM 8.2.2 and ex-\npress this as an unconstrained optimization problem. Then we express the\nconstrained versions of the primal and dual SVMs as quadratic programs\nin standard form 7.3.2.\nConsider the loss function view of the SVM (12.31). This is a convex\nunconstrained optimization problem, but",
                    " the hinge loss (12.28) is not dif-\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com .\n12.5 Numerical Solution 391\nferentiable. Therefore, we apply a subgradient approach for solving it.\nHowever, the hinge loss is differentiable almost everywhere, except for\none single point at the hinge t= 1. At this point, the gradient is a set of\npossible values that lie between 0and{-}1. Therefore, the subgradient gof\nthe hinge loss is given by\ng(t) =\n\n{-}1 t {<}1\n[{-}1,0]t= 1\n0 t {>}1. (12.54)\nUsing this subgradient, we can apply the optimization methods presented\nin Section 7.1.\nBoth the primal and the dual SVM result in a convex quadratic pro-\ngramming problem (constrained optimization). Note that the primal SVM\nin (12.26a) has optimization variables that have the size of the dimen-\nsionDof the input examples. The dual SVM in (12.41) has optimization\nvariables that have the size of the number Nof examples.\nTo express the primal SVM in the standard form (7.45) for quadratic\nprogramming, let us assume that we use the dot product (3.5) as the\ninner product. We rearrange the equation for the primal SVM (12.26a), Recall from\nSection 3.2 that we\nuse the phrase dot\nproduct to mean the\ninner product on\nEuclidean vector\nspace.such that the optimization variables are all on the right and the inequality\nof the constraint matches the standard form. This yields the optimization\nmin\nw,b,{\\xi}1\n2{\\parallel}w{\\parallel}2+CNX\nn=1{\\xi}n\nsubject to{-}ynx{\\top}\nnw{-}ynb{-}{\\xi}n{\\leqslant}{-}1\n{-}{\\xi}n{\\leqslant}0(12.55)\nn= 1, . . . , N . By concatenating the variables w",
                    ", b,xninto a single vector,\nand carefully collecting the terms, we obtain the following matrix form of\nthe soft margin SVM:\nmin\nw,b,{\\xi}1\n2\nw\nb\n{\\xi}\n{\\top}ID 0D,N+1\n0N+1,D0N+1,N+1\nw\nb\n{\\xi}\n+0D+1,1C1N,1{\\top}\nw\nb\n{\\xi}\n\nsubject to{-}Y X {-}y{-}IN\n0N,D+1 {-}IN\nw\nb\n{\\xi}\n{\\leqslant}{-}1N,1\n0N,1\n.\n(12.56)\nIn the preceding optimization problem, the minimization is over the pa-\nrameters [w{\\top}, b,{\\xi}{\\top}]{\\top}{\\in}RD+1+N, and we use the notation: Imto rep-\nresent the identity matrix of size m{\\texttimes}m,0m,nto represent the matrix\nof zeros of size m{\\texttimes}n, and 1m,nto represent the matrix of ones of size\nm{\\texttimes}n. In addition, yis the vector of labels [y1,{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}, yN]{\\top},Y= diag( y)\n{\\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n392 Classification with Support Vector Machines\nis an NbyNmatrix where the elements of the diagonal are from y, and\nX{\\in}RN{\\texttimes}Dis the matrix obtained by concatenating all the examples.\nWe can similarly perform a collection of terms for the dual version of the\nSVM (12.41). To express the dual SVM in standard form, we first have to\nexpress the kernel matrix Ksuch that each entry is Kij=k(xi,xj). If we\nhave an explicit feature representation xithen we define Kij={\\langle}xi,xj{\\rangle}.\nFor convenience of notation we introduce a matrix with zeros everywhere\nexcept on the diagonal, where we store the labels, that",
                    " is, Y= diag( y).\nThe dual SVM can be written as\nmin\n{\\alpha}1\n2{\\alpha}{\\top}Y KY {\\alpha} {-}1{\\top}\nN,1{\\alpha}\nsubject to\ny{\\top}\n{-}y{\\top}\n{-}IN\nIN\n{\\alpha}{\\leqslant}0N+2,1\nC1N,1\n.(12.57)\nRemark. In Sections 7.3.1 and 7.3.2, we introduced the standard forms\nof the constraints to be inequality constraints. We will express the dual\nSVM`s equality constraint as two inequality constraints, i.e.,\nAx=bis replaced by Ax{\\leqslant}band Ax{\\geqslant}b. (12.58)\nParticular software implementations of convex optimization methods may\nprovide the ability to express equality constraints. {\\diamond}\nSince there are many different possible views of the SVM, there are\nmany approaches for solving the resulting optimization problem. The ap-\nproach presented here, expressing the SVM problem in standard convex\noptimization form, is not often used in practice. The two main implemen-\ntations of SVM solvers are Chang and Lin (2011) (which is open source)\nand Joachims (1999). Since SVMs have a clear and well-defined optimiza-\ntion problem, many approaches based on numerical optimization tech-\nniques (Nocedal and Wright, 2006) can be applied (Shawe-Taylor and\nSun, 2011).\n12.6 Further Reading\nThe SVM is one of many approaches for studying binary classification.\nOther approaches include the perceptron, logistic regression, Fisher dis-\ncriminant, nearest neighbor, naive Bayes, and random forest (Bishop, 2006;\nMurphy, 2012). A short tutorial on SVMs and kernels on discrete se-\nquences can be found in Ben-Hur et al. (2008). The development of SVMs\nis closely linked to empirical risk minimization, discussed in Section 8.2.\nHence, the SVM has strong theoretical properties (Vapnik, 2000; Stein-\nwart and Christmann, 2008). The book about kernel methods (Sch {\\textasciidieresis}olkopf\nand Smola, 2002) includes many details",
                    " of support vector machines and\nDraft (2023-12-19) of {\\textquotedblleft}Mathematics for Machine Learning{\\textquotedblright}. Feedback: https://mml-book.com ."
                ]
            }
        ]
    },
    "The Elements of Statistical Learning": {
        "authors": [
            "Trevor Hastie",
            "Robert Tibshirani",
            "Jerome Friedman"
        ],
        "year": 2017,
        "chapters": [
            {
                "7.10": [
                    "7.10 Cross-Validation 241\nand worst possible model choices: min {\\alpha}ErrT({\\alpha}) and max {\\alpha}ErrT({\\alpha}). The\nboxplots show the distribution of the quantity\n100{\\texttimes}ErrT({\\textasciicircum}{\\alpha}){-}min{\\alpha}ErrT({\\alpha})\nmax{\\alpha}ErrT({\\alpha}){-}min{\\alpha}ErrT({\\alpha}),\nwhich represents the error in using the chosen model relativ e to the best\nmodel. For linear regression the model complexity was measu red by the\nnumber of features; as mentioned in Section 7.5, this undere stimates the\ndf, since it does not chargefor the search for the best model of that size.\nThis was also used for the VC dimension of the linear classifie r. Fork-\nnearest neighbors, we used the quantity N/k. Under an additive-error re-\ngression model, this can be justified as the exact effective de grees of free-\ndom (Exercise 7.6); we do not know if it corresponds to the VC d imen-\nsion. We used a1=a2= 1 for the constants in (7.46); the results for SRM\nchangedwithdifferentconstants,andthischoicegavethemo stfavorablere-\nsults. We repeated the SRM selection using the alternative p ractical bound\n(7.47), and got almost identical results. For misclassifica tion error we used\n{\\textasciicircum}{\\sigma}{\\varepsilon}2= [N/(N{-}d)]{\\textperiodcentered}err({\\alpha}) for the least restrictive model ( k= 5 for KNN,\nsincek= 1 results in zero training error). The AIC criterion seems t o work\nwell in all four scenarios, despite the lack of theoretical s upport with 0{\\textendash}1\nloss. BIC does nearly as well, while the performance of SRM is mixed.\n7.10 Cross-Validation\nProbably the simplest and most widely used method for estima ting predic-\ntion error is cross-validation. This method directly estim ates the expected\nextra-sample error Err = E[ L(Y,{\\textasciicircum",
                    "}f(X))], the average generalization error\nwhen the method {\\textasciicircum}f(X) is applied to an independent test sample from the\njoint distribution of XandY. As mentioned earlier, we might hope that\ncross-validation estimates the conditional error, with th e training setT\nheld fixed. But as we will see in Section 7.12, cross-validati on typically\nestimates well only the expected prediction error.\n7.10.1K-Fold Cross-Validation\nIdeally, if we had enough data, we would set aside a validatio n set and use\nit to assess the performance of our prediction model. Since d ata are often\nscarce, this is usually not possible. To finesse the problem, K-fold cross-\nvalidation uses part of the available data to fit the model, an d a different\npart to test it. We split the data into Kroughly equal-sized parts; for\nexample, when K= 5, the scenario looks like this:\n242 7. Model Assessment and Selection\nValidation Train1 2 3 4 5\nTrain Train Train\nFor thekth part (third above), we fit the model to the other K{-}1 parts\nof the data, and calculate the prediction error of the fitted m odel when\npredicting the kth part of the data. We do this for k= 1,2,...,Kand\ncombine the Kestimates of prediction error.\nHere are more details. Let {\\kappa}:{\\{}1,...,N{\\}}{\\mapsto}{\\textrightarrow}{\\{}1,...,K{\\}}be an indexing\nfunction that indicates the partition to which observation iis allocated by\nthe randomization. Denote by {\\textasciicircum}f{-}k(x) the fitted function, computed with\nthekth part of the data removed. Then the cross-validation estim ate of\nprediction error is\nCV({\\textasciicircum}f) =1\nNN{\\sum}\ni=1L(yi,{\\textasciicircum}f{-}{\\kappa}(i)(xi)). (7.48)\nTypical choices of Kare 5 or 10 (see below). The case K=Nis known\nasleave-one-out cross-validation. In this",
                    " case {\\kappa}(i) =i, and for the ith\nobservation the fit is computed using all the data except the ith.\nGiven a set of models f(x,{\\alpha}) indexed by a tuning parameter {\\alpha}, denote\nby{\\textasciicircum}f{-}k(x,{\\alpha}) the{\\alpha}th model fit with the kth part of the data removed. Then\nfor this set of models we define\nCV({\\textasciicircum}f,{\\alpha}) =1\nNN{\\sum}\ni=1L(yi,{\\textasciicircum}f{-}{\\kappa}(i)(xi,{\\alpha})). (7.49)\nThe function CV( {\\textasciicircum}f,{\\alpha}) provides an estimate of the test error curve, and we\nfind the tuning parameter {\\textasciicircum} {\\alpha}that minimizes it. Our final chosen model is\nf(x,{\\textasciicircum}{\\alpha}), which we then fit to all the data.\nIt is interesting to wonder about what quantity K-fold cross-validation\nestimates. With K= 5 or 10, we might guess that it estimates the ex-\npected error Err, since the training sets in each fold are qui te different\nfrom the original training set. On the other hand, if K=Nwe might\nguess that cross-validation estimates the conditional err or Err T. It turns\nout that cross-validation only estimates effectively the av erage error Err,\nas discussed in Section 7.12.\nWhat value should we choose for K? WithK=N, the cross-validation\nestimator is approximately unbiased for the true (expected ) prediction er-\nror, but can have high variance because the N{\\textquotedblleft}training sets{\\textquotedblright} are so similar\nto one another. The computational burden is also considerab le, requiring\nNapplications of the learning method. In certain special pro blems, this\ncomputation can be done quickly{\\textemdash}see Exercises 7.3 and 5.13.\n7.10 Cross-Validation 243\nSize of Training Set1-Err\n0 50 100 150 2000.0 0.2 0.4 0.6 0.8\nFIG",
                    "URE 7.8. Hypothetical learning curve for a classifier on a given task: a\nplot of1{-}Errversus the size of the training set N. With a dataset of 200\nobservations, 5-fold cross-validation would use training sets of size 160, which\nwould behave much like the full set. However, with a dataset of 50observations\nfivefold cross-validation would use training sets of size 40, and this would result\nin a considerable overestimate of prediction error.\nOn the other hand, with K= 5 say, cross-validation has lower variance.\nBut bias could be a problem, depending on how the performance of the\nlearning method varies with the size of the training set. Fig ure 7.8 shows\na hypothetical {\\textquotedblleft}learning curve{\\textquotedblright} for a classifier on a given ta sk, a plot of\n1{-}Err versus the size of the training set N. The performance of the\nclassifier improves as the training set size increases to 100 observations;\nincreasing the number further to 200 brings only a small bene fit. If our\ntraining set had 200 observations, fivefold cross-validati on would estimate\nthe performance of our classifier over training sets of size 1 60, which from\nFigure 7.8 is virtually the same as the performance for train ing set size\n200. Thus cross-validation would not suffer from much bias. H owever if the\ntraining set had 50 observations, fivefold cross-validatio n would estimate\nthe performance of our classifier over training sets of size 4 0, and from the\nfigure that would be an underestimate of 1 {-}Err. Hence as an estimate of\nErr, cross-validation would be biased upward.\nTo summarize, if the learning curve has a considerable slope at the given\ntraining set size, five- or tenfold cross-validation will ov erestimate the true\nprediction error. Whether this bias is a drawback in practic e depends on\nthe objective. On the other hand, leave-one-out cross-vali dation has low\nbias but can have high variance. Overall, five- or tenfold cro ss-validation\nare recommended as a good compromise: see Breiman and Specto r (1992)\nand Kohavi (1995).\nFigure 7.9 shows the prediction error and tenfold cross-",
                    "val idation curve\nestimated from a single training set, from the scenario in th e bottom right\npanel of Figure 7.3. This is a two-class classification probl em, using a lin-\n244 7. Model Assessment and Selection\nSubset Size pMisclassification Error\n5 10 15 200.0 0.1 0.2 0.3 0.4 0.5 0.6\n\n\n\n\n  \n\n\n\n\n\n      \nFIGURE 7.9. Prediction error (orange) and tenfold cross-validation curv e\n(blue) estimated from a single training set, from the scenario in the bottom right\npanel of Figure 7.3.\near model with best subsets regression of subset size p. Standard error bars\nare shown, which are the standard errors of the individual mi sclassification\nerror rates for each of the ten parts. Both curves have minima atp= 10,\nalthough the CV curve is rather {fl}at beyond 10. Often a {\\textquotedblleft}one-st andard\nerror{\\textquotedblright} rule is used with cross-validation, in which we choos e the most par-\nsimonious model whose error is no more than one standard erro r above\nthe error of the best model. Here it looks like a model with abo utp= 9\npredictors would be chosen, while the true model uses p= 10.\nGeneralized cross-validation providesaconvenientapproximationtoleave-\none out cross-validation, for linear fitting under squared- error loss. As de-\nfined in Section 7.6, a linear fitting method is one for which we can write\n{\\textasciicircum}y=Sy. (7.50)\nNow for many linear fitting methods,\n1\nNN{\\sum}\ni=1[yi{-}{\\textasciicircum}f{-}i(xi)]2=1\nNN{\\sum}\ni=1[yi{-}{\\textasciicircum}f(xi)\n1{-}Sii]2\n,(7.51)\nwhereSiiis theith diagonal element of S(see Exercise 7.3). The GCV\napproximation is\nGCV({\\textasciicircum}f) =1\nNN{\\sum}\ni=1[",
                    "\nyi{-}{\\textasciicircum}f(xi)\n1{-}trace(S)/N]2\n. (7.52)\n7.10 Cross-Validation 245\nThe quantity trace( S) is the effective number of parameters, as defined in\nSection 7.6.\nGCV can have a computational advantage in some settings, whe re the\ntrace ofScan be computed more easily than the individual elements Sii.\nIn smoothing problems, GCV can also alleviate the tendency o f cross-\nvalidation to undersmooth. The similarity between GCV and A IC can be\nseen from the approximation 1 /(1{-}x)2{\\approx}1+2x(Exercise 7.7).\n7.10.2 The Wrong and Right Way to Do Cross-validation\nConsider a classification problem with a large number of pred ictors, as may\narise, for example, in genomic or proteomic applications. A typical strategy\nfor analysis might be as follows:\n1. Screen the predictors: find a subset of {\\textquotedblleft}good{\\textquotedblright} predictors t hat show\nfairly strong (univariate) correlation with the class labe ls\n2. Using just this subset of predictors, build a multivariat e classifier.\n3. Use cross-validation to estimate the unknown tuning para meters and\nto estimate the prediction error of the final model.\nIs this a correct application of cross-validation? Conside r a scenario with\nN= 50 samples in two equal-sized classes, and p= 5000 quantitative\npredictors (standard Gaussian) that are independent of the class labels.\nThe true (test) error rate of any classifier is 50{\\%}. We carried out the above\nrecipe, choosing in step (1) the 100 predictors having highe st correlation\nwith the class labels, and then using a 1-nearest neighbor cl assifier, based\non just these 100 predictors, in step (2). Over 50 simulation s from this\nsetting, the average CV error rate was 3{\\%}. This is far lower th an the true\nerror rate of 50{\\%}.\nWhat has happened? The problem is that the predictors have an unfair\nadvantage, as they were chosen in step (1) on the basis of all of the samples .\nLeaving samples out afterthe",
                    " variables have been selected does not cor-\nrectly mimic the application of the classifier to a completel y independent\ntest set, since these predictors {\\textquotedblleft}have already seen{\\textquotedblright} the lef t out samples.\nFigure 7.10 (top panel) illustrates the problem. We selecte d the 100 pre-\ndictors having largest correlation with the class labels ov er all 50 samples.\nThenwechosearandomsetof10samples,aswewoulddoinfive-f oldcross-\nvalidation, and computed the correlations of the pre-selec ted 100 predictors\nwith the class labels over just these 10 samples (top panel). We see that\nthe correlations average about 0.28, rather than 0, as one mi ght expect.\nHere is the correct way to carry out cross-validation in this example:\n1. Divide the samples into Kcross-validation folds (groups) at random.\n2. For each fold k= 1,2,...,K\n246 7. Model Assessment and Selection\nCorrelations of Selected Predictors with OutcomeFrequency\n{-}1.0 {-}0.5 0.0 0.5 1.00 10 20 30Wrong way\nCorrelations of Selected Predictors with OutcomeFrequency\n{-}1.0 {-}0.5 0.0 0.5 1.00 10 20 30Right way\nFIGURE 7.10. Cross-validation the wrong and right way: histograms shows t he\ncorrelation of class labels, in 10randomly chosen samples, with the 100predic-\ntors chosen using the incorrect (upper red) and correct (lowe r green) versions of\ncross-validation.\n(a) Find a subset of {\\textquotedblleft}good{\\textquotedblright} predictors that show fairly stron g (uni-\nvariate) correlation with the class labels, using all of the samples\nexcept those in fold k.\n(b) Using just this subset of predictors, build a multivaria te classi-\nfier, using all of the samples except those in fold k.\n(c) Use the classifier to predict the class labels for the samp les in\nfoldk.\nTheerrorestimatesfromstep2(c)arethenaccumulated over allKfolds,to\nprodu",
                    "ce the cross-validation estimate of prediction error . The lower panel\nof Figure 7.10 shows the correlations of class labels with th e 100 predictors\nchosen in step 2(a) of the correct procedure, over the sample s in a typical\nfoldk. We see that they average about zero, as they should.\nIn general, with a multistep modeling procedure, cross-val idation must\nbe applied to the entire sequence of modeling steps. In parti cular, samples\nmust be {\\textquotedblleft}left out{\\textquotedblright} before any selection or filtering steps are applied. There\nis one qualification: initial unsupervised screening steps can be done be-\nfore samples are left out. For example, we could select the 10 00 predictors\n7.10 Cross-Validation 247\nwith highest variance across all 50 samples, before startin g cross-validation.\nSince this filtering does not involve the class labels, it doe s not give the\npredictors an unfair advantage.\nWhile this point may seem obvious to the reader, we have seen t his\nblunder committed many times in published papers in top rank journals.\nWith the large numbers of predictors that are so common in gen omic and\nother areas, the potential consequences of this error have a lso increased\ndramatically; see Ambroise and McLachlan (2002) for a detai led discussion\nof this issue.\n7.10.3 Does Cross-Validation Really Work?\nWeonceagainexaminethebehaviorofcross-validationinah igh-dimensional\nclassification problem. Consider a scenario with N= 20 samples in two\nequal-sized classes, and p= 500 quantitative predictors that are indepen-\ndent of the class labels. Once again, the true error rate of an y classifier is\n50{\\%}. Consider a simple univariate classifier: a single split that minimizes\nthe misclassification error (a {\\textquotedblleft}stump{\\textquotedblright}). Stumps are trees wi th a single split,\nand are used in boosting methods (Chapter 10). A simple argum ent sug-\ngests that cross-validation will not work properly in this s etting2:\nFitting to the entire training set, we will find a predictor th at\nsplits the data very well. If we do 5-fold cross",
                    "-validation, this\nsame predictor should split any 4/5ths and 1/5th of the data\nwell too, and hence its cross-validation error will be small (much\nless than 50{\\%}.) Thus CV does not give an accurate estimate of\nerror.\nTo investigate whether this argument is correct, Figure 7.1 1 shows the\nresult of a simulation from this setting. There are 500 predi ctors and 20\nsamples, in each of two equal-sized classes, with all predic tors having a\nstandard Gaussian distribution. The panel in the top left sh ows the number\nof training errors for each of the 500 stumps fit to the trainin g data. We\nhave marked in color the six predictors yielding the fewest e rrors. In the top\nright panel, the training errors are shown for stumps fit to a r andom 4/5ths\npartition of the data (16 samples), and tested on the remaini ng 1/5th (four\nsamples). The colored points indicate the same predictors m arked in the\ntop left panel. We see that the stump for the blue predictor (w hose stump\nwas the best in the top left panel), makes two out of four test e rrors (50{\\%}),\nand is no better than random.\nWhat has happened? The preceding argument has ignored the fa ct that\nin cross-validation, the model must be completely retraine d for each fold\n2This argument was made to us by a scientist at a proteomics lab meeting, a nd led\nto material in this section.\n248 7. Model Assessment and Selection\n0 100 200 300 400 5002 3 4 5 6 7 8 9\nPredictorError on Full Training Set\n1 2 3 4 5 6 7 80 1 2 3 4\nError on 4/5Error on 1/5\n{-}1 0 1 2\nPredictor 436 (blue)Class Label\n0 1\nfull\n4/5\n0.0 0.2 0.4 0.6 0.8 1.0\nCV Errors\nFIGURE 7.11. Simulation study to investigate the performance of cross vali -\ndation in a high-dimensional problem where the predictors ar e independent of the\nclass labels. The top-left panel shows the number of errors made b y individual\nstump classifiers on the full training set ( 20observations",
                    "). The top right panel\nshows the errors made by individual stumps trained on a rando m split of the\ndataset into 4/5ths (16observations) and tested on the remaining 1/5th (4ob-\nservations). The best performers are depicted by colored dot s in each panel. The\nbottom left panel shows the effect of re-estimating the split po int in each fold: the\ncolored points correspond to the four samples in the 1/5th validation set. The split\npoint derived from the full dataset classifies all four samples correctly, but when\nthe split point is re-estimated on the 4/5ths data (as it should be), it commits\ntwo errors on the four validation samples. In the bottom right w e see the overall\nresult of five-fold cross-validation applied to 50simulated datasets. The average\nerror rate is about 50{\\%}, as it should be.\n7.11 Bootstrap Methods 249\nof the process. In the present example, this means that the be st predictor\nand corresponding split point are found from 4 /5ths of the data. The effect\nof predictor choice is seen in the top right panel. Since the c lass labels are\nindependent of the predictors, the performance of a stump on the 4/5ths\ntraining data contains no information about its performanc e in the remain-\ning 1/5th. The effect of the choice of split point is shown in the bott om left\npanel. Here we see the data for predictor 436, corresponding to the blue\ndot in the top left plot. The colored points indicate the 1 /5th data, while\nthe remaining points belong to the 4 /5ths. The optimal split points for this\npredictor based on both the full training set and 4 /5ths data are indicated.\nThe split based on the full data makes no errors on the 1 /5ths data. But\ncross-validation must base its split on the 4 /5ths data, and this incurs two\nerrors out of four samples.\nThe results of applying five-fold cross-validation to each o f 50 simulated\ndatasets is shown in the bottom right panel. As we would hope, the average\ncross-validation error is around 50{\\%}, which is the true expe cted prediction\nerror for this classifier. Hence cross-validation has behav ed as it should.\nOn the other",
                    " hand, there is considerable variability in the e rror, underscor-\ning the importance of reporting the estimated standard erro r of the CV\nestimate. See Exercise 7.10 for another variation of this pr oblem.\n7.11 Bootstrap Methods\nThe bootstrap is a general tool for assessing statistical ac curacy. First we\ndescribe the bootstrap in general, and then show how it can be used to\nestimate extra-sample prediction error. As with cross-val idation, the boot-\nstrap seeks to estimate the conditional error Err T, but typically estimates\nwell only the expected prediction error Err.\nSuppose we have a model fit to a set of training data. We denote t he\ntraining set by Z= (z1,z2,...,z N) wherezi= (xi,yi). The basic idea is\nto randomly draw datasets with replacement from the trainin g data, each\nsample the same size as the original training set. This is don eBtimes\n(B= 100 say), producing Bbootstrap datasets, as shown in Figure 7.12.\nThen we refit the model to each of the bootstrap datasets, and e xamine\nthe behavior of the fits over the Breplications.\nIn the figure, S(Z) is any quantity computed from the data Z, for ex-\nample, the prediction at some input point. From the bootstra p sampling\nwe can estimate any aspect of the distribution of S(Z), for example, its\nvariance,\n{\\textasciicircum}Var[S(Z)] =1\nB{-}1B{\\sum}\nb=1(S(Z{*}b){-}{\\textasciimacron}S{*})2, (7.53)"
                ]
            },
            {
                "8.2": [
                    "This is page 261\nPrinter: Opaque this\n8\nModel Inference and Averaging\n8.1 Introduction\nFor most of this book, the fitting (learning) of models has bee n achieved by\nminimizing a sum of squares for regression, or by minimizing cross-entropy\nfor classification. In fact, both of these minimizations are instances of the\nmaximum likelihood approach to fitting.\nIn this chapter we provide a general exposition of the maximu m likeli-\nhood approach, as well as the Bayesian method for inference. The boot-\nstrap, introduced in Chapter 7, is discussed in this context , and its relation\nto maximum likelihood and Bayes is described. Finally, we pr esent some\nrelated techniques for model averaging and improvement, in cluding com-\nmittee methods, bagging, stacking and bumping.\n8.2 The Bootstrap and Maximum Likelihood\nMethods\n8.2.1 A Smoothing Example\nThe bootstrap method provides a direct computational way of assessing\nuncertainty, by sampling from the training data. Here we ill ustrate the\nbootstrap in a simple one-dimensional smoothing problem, a nd show its\nconnection to maximum likelihood.\n262 8. Model Inference and Averaging\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\nxy\n\n\n\n\n\n\n\n\n\n\n\n0.0 0.5 1.0 1.5 2.0 2.5 3.00.0 0.2 0.4 0.6 0.8 1.0\nxB-spline Basis\nFIGURE 8.1. (Left panel): Data for smoothing example. (Right panel:) Set of\nsevenB-spline basis functions. The broken vertical lines indicate t he placement\nof the three knots.\nDenote the training data by Z={\\{}z1,z2,...,z N{\\}}, withzi= (xi,yi),\ni= 1,2,...,N. Herexiis a one-dimensional input, and yithe outcome,\neither continuous or categorical. As an example, consider t heN= 50 data\npoints shown in the left panel of Figure 8.1.\nSuppose we decide to fit a cubic spline to the data, with three k nots\nplaced at the quartiles of the Xvalues. This",
                    " is a seven-dimensional lin-\near space of functions, and can be represented, for example, by a linear\nexpansion of B-spline basis functions (see Section 5.9.2):\n{\\textmu}(x) =7{\\sum}\nj=1{\\beta}jhj(x). (8.1)\nHere thehj(x),j= 1,2,...,7 are the seven functions shown in the right\npanel of Figure 8.1. We can think of {\\textmu}(x) as representing the conditional\nmean E(Y|X=x).\nLetHbe theN{\\texttimes}7 matrix with ijth element hj(xi). The usual estimate\nof{\\beta}, obtained by minimizing the squared error over the training set, is\ngiven by\n{\\textasciicircum}{\\beta}= (HTH){-}1HTy. (8.2)\nThe corresponding fit {\\textasciicircum} {\\textmu}(x) ={\\sum}7\nj=1{\\textasciicircum}{\\beta}jhj(x) is shown in the top left panel\nof Figure 8.2.\nThe estimated covariance matrix of {\\textasciicircum}{\\beta}is\n{\\textasciicircum}Var({\\textasciicircum}{\\beta}) = (HTH){-}1{\\textasciicircum}{\\sigma}2, (8.3)\nwhere we have estimated the noise variance by {\\textasciicircum} {\\sigma}2={\\sum}N\ni=1(yi{-}{\\textasciicircum}{\\textmu}(xi))2/N.\nLettingh(x)T= (h1(x),h2(x),...,h 7(x)), the standard error of a predic-\n8.2 The Bootstrap and Maximum Likelihood Methods 263\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\n\n\n\n\n\n\n\n\n\n\nxy\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\nxy\n\n\n\n\n\n\n\n\n\n\n\n0.0 0.5 1.0 1.5",
                    " 2.0 2.5 3.0-1 0 1 2 3 4 5\nxy\n\n\n\n\n\n\n\n\n\n\n\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\nxy\n\n\n\n\n\n\n\n\n\n\n\nFIGURE 8.2. (Top left:)B-spline smooth of data. (Top right:) B-spline smooth\nplus and minus 1.96{\\texttimes}standard error bands. (Bottom left:) Ten bootstrap repli-\ncates of the B-spline smooth. (Bottom right:) B-spline smooth with 95{\\%} standard\nerror bands computed from the bootstrap distribution.\n264 8. Model Inference and Averaging\ntion {\\textasciicircum}{\\textmu}(x) =h(x)T{\\textasciicircum}{\\beta}is\n{\\textasciicircum}se[{\\textasciicircum}{\\textmu}(x)] = [h(x)T(HTH){-}1h(x)]1\n2{\\textasciicircum}{\\sigma}. (8.4)\nIn the top right panel of Figure 8.2 we have plotted {\\textasciicircum} {\\textmu}(x){\\pm}1.96{\\textperiodcentered}{\\textasciicircum}se[{\\textasciicircum}{\\textmu}(x)].\nSince 1.96 is the 97.5{\\%} point of the standard normal distribu tion, these\nrepresent approximate 100 {-}2{\\texttimes}2.5{\\%} = 95{\\%} pointwise confidence bands\nfor{\\textmu}(x).\nHere is how we could apply the bootstrap in this example. We dr awB\ndatasets each of size N= 50 with replacement from our training data, the\nsampling unit being the pair zi= (xi,yi). To each bootstrap dataset Z{*}\nwe fit a cubic spline {\\textasciicircum} {\\textmu}{*}(x); the fits from ten such samples are shown in the\nbottom left panel of Figure 8.2. Using B= 200 bootstrap samples, we can\nform a 95{\\%} pointwise confidence band from the percentiles at",
                    " e achx: we\nfind the 2.5{\\%}{\\texttimes}200 = fifth largest and smallest values at each x. These are\nplotted in the bottom right panel of Figure 8.2. The bands loo k similar to\nthose in the top right, being a little wider at the endpoints.\nThere is actually a close connection between the least squar es estimates\n(8.2)and(8.3),thebootstrap,andmaximumlikelihood.Sup posewefurther\nassume that the model errors are Gaussian,\nY={\\textmu}(X)+{\\varepsilon};{\\varepsilon}{\\sim}N(0,{\\sigma}2),\n{\\textmu}(x) =7{\\sum}\nj=1{\\beta}jhj(x). (8.5)\nThe bootstrap method described above, in which we sample wit h re-\nplacement from the training data, is called the nonparametric bootstrap .\nThis really means that the method is {\\textquotedblleft}model-free,{\\textquotedblright} since it u ses the raw\ndata, not a specific parametric model, to generate new datase ts. Consider\na variation of the bootstrap, called the parametric bootstrap , in which we\nsimulate new responses by adding Gaussian noise to the predi cted values:\ny{*}\ni= {\\textasciicircum}{\\textmu}(xi)+{\\varepsilon}{*}\ni;{\\varepsilon}{*}\ni{\\sim}N(0,{\\textasciicircum}{\\sigma}2);i= 1,2,...,N. (8.6)\nThis process is repeated Btimes, where B= 200 say. The resulting boot-\nstrap datasets have the form ( x1,y{*}\n1),...,(xN,y{*}\nN) and we recompute the\nB-spline smooth on each. The confidence bands from this method will ex-\nactly equal the least squares bands in the top right panel, as the number of\nbootstrap samples goes to infinity. A function estimated fro m a bootstrap\nsampley{*}is given by {\\textasciicircum} {\\textmu}{*}(x) =h(x)T",
                    "(HTH){-}1HTy{*}, and has distribution\n{\\textasciicircum}{\\textmu}{*}(x){\\sim}N({\\textasciicircum}{\\textmu}(x),h(x)T(HTH){-}1h(x){\\textasciicircum}{\\sigma}2). (8.7)\nNotice that the mean of this distribution is the least square s estimate, and\nthe standard deviation is the same as the approximate formul a (8.4).\n8.2 The Bootstrap and Maximum Likelihood Methods 265\n8.2.2 Maximum Likelihood Inference\nIt turns out that the parametric bootstrap agrees with least squares in the\nprevious example because the model (8.5) has additive Gauss ian errors. In\ngeneral, the parametric bootstrap agrees not with least squ ares but with\nmaximum likelihood, which we now review.\nWe begin by specifying a probability density or probability mass function\nfor our observations\nzi{\\sim}g{\\theta}(z). (8.8)\nIn this expression {\\theta}represents one or more unknown parameters that gov-\nern the distribution of Z. This is called a parametric model forZ. As an\nexample, if Zhas a normal distribution with mean {\\textmu}and variance {\\sigma}2, then\n{\\theta}= ({\\textmu},{\\sigma}2), (8.9)\nand\ng{\\theta}(z) =1{\\sqrt{}}\n2{\\pi}{\\sigma}e{-}1\n2(z{-}{\\textmu})2/{\\sigma}2. (8.10)\nMaximum likelihood is based on the likelihood function , given by\nL({\\theta};Z) =N{\\prod}\ni=1g{\\theta}(zi), (8.11)\nthe probability of the observed data under the model g{\\theta}. The likelihood is\ndefined only up to a positive multiplier, which we have taken t o be one.\nWe think of L({\\theta};Z) as a function of {\\theta}, with our data Zfixed.\nDenote the logarithm of L({\\theta};Z) by\n{\\ell}({\\theta};Z) =N{\\",
                    "sum}\ni=1{\\ell}({\\theta};zi)\n=N{\\sum}\ni=1logg{\\theta}(zi), (8.12)\nwhich we will sometimes abbreviate as {\\ell}({\\theta}). This expression is called the\nlog-likelihood, and each value {\\ell}({\\theta};zi) = logg{\\theta}(zi) is called a log-likelihood\ncomponent. The method of maximum likelihood chooses the val ue{\\theta}={\\textasciicircum}{\\theta}\nto maximize {\\ell}({\\theta};Z).\nThe likelihood function can be used to assess the precision o f{\\textasciicircum}{\\theta}. We need\na few more definitions. The score function is defined by\n{\\textperiodcentered}{\\ell}({\\theta};Z) =N{\\sum}\ni=1{\\textperiodcentered}{\\ell}({\\theta};zi), (8.13)\n266 8. Model Inference and Averaging\nwhere{\\textperiodcentered}{\\ell}({\\theta};zi) ={\\partial}{\\ell}({\\theta};zi)/{\\partial}{\\theta}. Assuming that the likelihood takes its maxi-\nmum in the interior of the parameter space, {\\textperiodcentered}{\\ell}({\\textasciicircum}{\\theta};Z) = 0. The information\nmatrixis\nI({\\theta}) ={-}N{\\sum}\ni=1{\\partial}2{\\ell}({\\theta};zi)\n{\\partial}{\\theta}{\\partial}{\\theta}T. (8.14)\nWhenI({\\theta}) is evaluated at {\\theta}={\\textasciicircum}{\\theta}, it is often called the observed information .\nTheFisher information (or expected information) is\ni({\\theta}) = E{\\theta}[I({\\theta})]. (8.15)\nFinally, let {\\theta}0denote the true value of {\\theta}.\nA standard result says that the sampling distribution of the maximum\nlikelihood estimator has a limiting normal distribution\n{\\textasciicircum}{\\theta}{\\textrightarrow}N({\\theta}0,",
                    "i({\\theta}0){-}1), (8.16)\nasN{\\textrightarrow}{\\infty}. Here we are independently sampling from g{\\theta}0(z). This suggests\nthat the sampling distribution of {\\textasciicircum}{\\theta}may be approximated by\nN({\\textasciicircum}{\\theta},i({\\textasciicircum}{\\theta}){-}1) orN({\\textasciicircum}{\\theta},I({\\textasciicircum}{\\theta}){-}1), (8.17)\nwhere{\\textasciicircum}{\\theta}represents the maximum likelihood estimate from the observ ed\ndata.\nThe corresponding estimates for the standard errors of {\\textasciicircum}{\\theta}jare obtained\nfrom\n{\\sqrt{}}\ni({\\textasciicircum}{\\theta}){-}1\njjand{\\sqrt{}}\nI({\\textasciicircum}{\\theta}){-}1\njj. (8.18)\nConfidence points for {\\theta}jcan be constructed from either approximation\nin (8.17). Such a confidence point has the form\n{\\textasciicircum}{\\theta}j{-}z(1{-}{\\alpha}){\\textperiodcentered}{\\sqrt{}}\ni({\\textasciicircum}{\\theta}){-}1\njjor{\\textasciicircum}{\\theta}j{-}z(1{-}{\\alpha}){\\textperiodcentered}{\\sqrt{}}\nI({\\textasciicircum}{\\theta}){-}1\njj,\nrespectively, where z(1{-}{\\alpha})is the 1{-}{\\alpha}percentile of the standard normal\ndistribution. More accurate confidence intervals can be der ived from the\nlikelihood function, by using the chi-squared approximati on\n2[{\\ell}({\\textasciicircum}{\\theta}){-}{\\ell}({\\theta}0)]{\\sim}{\\chi}2\np, (8.19)\nwherepis the number of components in {\\theta",
                    "}. The resulting 1 {-}2{\\alpha}confi-\ndence interval is the set of all {\\theta}0such that 2[ {\\ell}({\\textasciicircum}{\\theta}){-}{\\ell}({\\theta}0)]{\\leq}{\\chi}2\np(1{-}2{\\alpha}),\nwhere{\\chi}2\np(1{-}2{\\alpha})is the 1{-}2{\\alpha}percentile of the chi-squared distribution with\npdegrees of freedom.\n8.3 Bayesian Methods 267\nLet`s return to our smoothing example to see what maximum lik elihood\nyields. The parameters are {\\theta}= ({\\beta},{\\sigma}2). The log-likelihood is\n{\\ell}({\\theta}) ={-}N\n2log{\\sigma}22{\\pi}{-}1\n2{\\sigma}2N{\\sum}\ni=1(yi{-}h(xi)T{\\beta})2. (8.20)\nThe maximum likelihood estimate is obtained by setting {\\partial}{\\ell}/{\\partial}{\\beta}= 0 and\n{\\partial}{\\ell}/{\\partial}{\\sigma}2= 0, giving\n{\\textasciicircum}{\\beta}= (HTH){-}1HTy,\n{\\textasciicircum}{\\sigma}2=1\nN{\\sum}\n(yi{-}{\\textasciicircum}{\\textmu}(xi))2,(8.21)\nwhich are the same as the usual estimates given in (8.2) and be low (8.3).\nThe information matrix for {\\theta}= ({\\beta},{\\sigma}2) is block-diagonal, and the block\ncorresponding to {\\beta}is\nI({\\beta}) = (HTH)/{\\sigma}2, (8.22)\nso that the estimated variance ( HTH){-}1{\\textasciicircum}{\\sigma}2agrees with the least squares\nestimate (8.3).\n8.2.3 Bootstrap versus Maximum Likelihood\nIn essence the bootstrap is a computer implementation of non parametric or\nparametric maximum likelihood. The advantage of the",
                    " bootst rap over the\nmaximum likelihood formula is that it allows us to compute ma ximum like-\nlihood estimates of standard errors and other quantities in settings where\nno formulas are available.\nIn our example, suppose that we adaptively choose by cross-v alidation\nthe number and position of the knots that define the B-splines, rather\nthan fix them in advance. Denote by {\\lambda}the collection of knots and their\npositions. Then the standard errors and confidence bands sho uld account\nfor the adaptive choice of {\\lambda}, but there is no way to do this analytically.\nWith the bootstrap, we compute the B-spline smooth with an adaptive\nchoice of knots for each bootstrap sample. The percentiles o f the resulting\ncurves capture the variability from both the noise in the tar gets as well as\nthat from {\\textasciicircum}{\\lambda}. In this particular example the confidence bands (not shown)\ndon`t look much different than the fixed {\\lambda}bands. But in other problems,\nwhere more adaptation is used, this can be an important effect to capture.\n8.3 Bayesian Methods\nIn the Bayesian approach to inference, we specify a sampling model Pr( Z|{\\theta})\n(density or probability mass function) for our data given th e parameters,"
                ]
            },
            {
                "8.3": [
                    "8.3 Bayesian Methods 267\nLet`s return to our smoothing example to see what maximum lik elihood\nyields. The parameters are {\\theta}= ({\\beta},{\\sigma}2). The log-likelihood is\n{\\ell}({\\theta}) ={-}N\n2log{\\sigma}22{\\pi}{-}1\n2{\\sigma}2N{\\sum}\ni=1(yi{-}h(xi)T{\\beta})2. (8.20)\nThe maximum likelihood estimate is obtained by setting {\\partial}{\\ell}/{\\partial}{\\beta}= 0 and\n{\\partial}{\\ell}/{\\partial}{\\sigma}2= 0, giving\n{\\textasciicircum}{\\beta}= (HTH){-}1HTy,\n{\\textasciicircum}{\\sigma}2=1\nN{\\sum}\n(yi{-}{\\textasciicircum}{\\textmu}(xi))2,(8.21)\nwhich are the same as the usual estimates given in (8.2) and be low (8.3).\nThe information matrix for {\\theta}= ({\\beta},{\\sigma}2) is block-diagonal, and the block\ncorresponding to {\\beta}is\nI({\\beta}) = (HTH)/{\\sigma}2, (8.22)\nso that the estimated variance ( HTH){-}1{\\textasciicircum}{\\sigma}2agrees with the least squares\nestimate (8.3).\n8.2.3 Bootstrap versus Maximum Likelihood\nIn essence the bootstrap is a computer implementation of non parametric or\nparametric maximum likelihood. The advantage of the bootst rap over the\nmaximum likelihood formula is that it allows us to compute ma ximum like-\nlihood estimates of standard errors and other quantities in settings where\nno formulas are available.\nIn our example, suppose that we adaptively choose by cross-v alidation\nthe number and position of the knots that define the B-splines, rather\nthan fix them in advance. Denote by {\\lambda}the collection of knots and their\npositions. Then the standard errors and confidence bands sho uld account\nfor the adaptive choice of {\\lambda}, but there is no way to do this analyt",
                    "ically.\nWith the bootstrap, we compute the B-spline smooth with an adaptive\nchoice of knots for each bootstrap sample. The percentiles o f the resulting\ncurves capture the variability from both the noise in the tar gets as well as\nthat from {\\textasciicircum}{\\lambda}. In this particular example the confidence bands (not shown)\ndon`t look much different than the fixed {\\lambda}bands. But in other problems,\nwhere more adaptation is used, this can be an important effect to capture.\n8.3 Bayesian Methods\nIn the Bayesian approach to inference, we specify a sampling model Pr( Z|{\\theta})\n(density or probability mass function) for our data given th e parameters,\n268 8. Model Inference and Averaging\nand a prior distribution for the parameters Pr( {\\theta}) re{fl}ecting our knowledge\nabout{\\theta}before we see the data. We then compute the posterior distrib ution\nPr({\\theta}|Z) =Pr(Z|{\\theta}){\\textperiodcentered}Pr({\\theta}){\\int}\nPr(Z|{\\theta}){\\textperiodcentered}Pr({\\theta})d{\\theta}, (8.23)\nwhich represents our updated knowledge about {\\theta}after we see the data. To\nunderstand this posterior distribution, one might draw sam ples from it or\nsummarize by computing its mean or mode. The Bayesian approa ch differs\nfrom the standard ({\\textquotedblleft}frequentist{\\textquotedblright}) method for inference in i ts use of a prior\ndistribution to express the uncertainty present before see ing the data, and\nto allow the uncertainty remaining after seeing the data to b e expressed in\nthe form of a posterior distribution.\nTheposteriordistributionalsoprovidesthebasisforpred ictingthevalues\nof a future observation znew, via the predictive distribution :\nPr(znew|Z) ={\\int}\nPr(znew|{\\theta}){\\textperiodcentered}Pr({\\theta}|Z)d{\\theta}. (8.24)\nIn contrast, the maximum likelihood approach would use Pr( znew|{\\textasciicircum}{\\theta}),\nthe data density evaluated at the maximum likelihood",
                    " estima te, to predict\nfuture data. Unlike the predictive distribution (8.24), th is does not account\nfor the uncertainty in estimating {\\theta}.\nLet`s walk through the Bayesian approach in our smoothing ex ample.\nWe start with the parametric model given by equation (8.5), a nd assume\nfor the moment that {\\sigma}2is known. We assume that the observed feature\nvaluesx1,x2,...,x Nare fixed, so that the randomness in the data comes\nsolely from yvarying around its mean {\\textmu}(x).\nThe second ingredient we need is a prior distribution. Distr ibutions on\nfunctions are fairly complex entities: one approach is to us e a Gaussian\nprocess prior in which we specify the prior covariance betwe en any two\nfunction values {\\textmu}(x) and{\\textmu}(x{'}) (Wahba, 1990; Neal, 1996).\nHere we take a simpler route: by considering a finite B-spline basis for\n{\\textmu}(x),wecaninsteadprovideapriorforthecoefficients {\\beta},andthisimplicitly\ndefines a prior for {\\textmu}(x). We choose a Gaussian prior centered at zero\n{\\beta}{\\sim}N(0,{\\tau}{\\Sigma}) (8.25)\nwith the choices of the prior correlation matrix {\\Sigma}and variance {\\tau}to be\ndiscussed below. The implicit process prior for {\\textmu}(x) is hence Gaussian,\nwith covariance kernel\nK(x,x{'}) = cov[ {\\textmu}(x),{\\textmu}(x{'})]\n={\\tau}{\\textperiodcentered}h(x)T{\\Sigma}h(x{'}). (8.26)\n8.3 Bayesian Methods 269\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-3 -2 -1 0 1 2 3{\\textmu}(x)\nx\nFIGURE 8.3. Smoothing example: Ten draws from the Gaussian prior distri-\nbution for the function {\\textmu}(x).\nThe posterior distribution for {\\beta}is also Gaussian, with mean and covariance\nE({\\beta}|Z",
                    ") =(\nHTH+{\\sigma}2\n{\\tau}{\\Sigma}{-}1){-}1\nHTy,\ncov({\\beta}|Z) =(\nHTH+{\\sigma}2\n{\\tau}{\\Sigma}{-}1){-}1\n{\\sigma}2,(8.27)\nwith the corresponding posterior values for {\\textmu}(x),\nE({\\textmu}(x)|Z) =h(x)T(\nHTH+{\\sigma}2\n{\\tau}{\\Sigma}{-}1){-}1\nHTy,\ncov[{\\textmu}(x),{\\textmu}(x{'})|Z] =h(x)T(\nHTH+{\\sigma}2\n{\\tau}{\\Sigma}{-}1){-}1\nh(x{'}){\\sigma}2.(8.28)\nHow do we choose the prior correlation matrix {\\Sigma}? In some settings the\nprior can be chosen from subject matter knowledge about the p arameters.\nHere we are willing to say the function {\\textmu}(x) should be smooth, and have\nguaranteed this by expressing {\\textmu}in a smooth low-dimensional basis of B-\nsplines. Hence we can take the prior correlation matrix to be the identity\n{\\Sigma}=I. When the number of basis functions is large, this might not b e suf-\nficient, and additional smoothness can be enforced by imposi ng restrictions\non{\\Sigma}; this is exactly the case with smoothing splines (Section 5. 8.1).\nFigure 8.3 shows ten draws from the corresponding prior for {\\textmu}(x). To\ngenerateposteriorvaluesofthefunction {\\textmu}(x),wegeneratevalues {\\beta}{'}fromits\nposterior (8.27), giving corresponding posterior value {\\textmu}{'}(x) ={\\sum}7\n1{\\beta}{'}\njhj(x).\nTen such posterior curves are shown in Figure 8.4. Two differe nt values\nwere used for the prior variance {\\tau}, 1 and 1000. Notice how similar the\nright panel looks to the bootstrap distribution in the botto m left panel\n270 8",
                    ". Model Inference and Averaging\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\n\n\n\n\n\n\n\n\n\n\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\n\n\n\n\n\n\n\n\n\n{\\textmu}(x){\\textmu}(x)\nx x{\\tau}= 1 {\\tau}= 1000\nFIGURE 8.4. Smoothing example: Ten draws from the posterior distributio n\nfor the function {\\textmu}(x), for two different values of the prior variance {\\tau}. The purple\ncurves are the posterior means.\nof Figure 8.2 on page 263. This similarity is no accident. As {\\tau}{\\textrightarrow}{\\infty}, the\nposterior distribution (8.27) and the bootstrap distribut ion (8.7) coincide.\nOn the other hand, for {\\tau}= 1, the posterior curves {\\textmu}(x) in the left panel\nof Figure 8.4 are smoother than the bootstrap curves, becaus e we have\nimposed more prior weight on smoothness.\nThe distribution (8.25) with {\\tau}{\\textrightarrow}{\\infty}is called a noninformative prior for\n{\\theta}.InGaussianmodels,maximumlikelihoodandparametricboo tstrapanal-\nyses tend to agree with Bayesian analyses that use a noninfor mative prior\nfor the free parameters. These tend to agree, because with a c onstant prior,\nthe posterior distribution is proportional to the likeliho od. This correspon-\ndence also extends to the nonparametric case, where the nonp arametric\nbootstrap approximates a noninformative Bayes analysis; S ection 8.4 has\nthe details.\nWe have, however, done some things that are not proper from a B ayesian\npoint of view. We have used a noninformative (constant) prio r for{\\sigma}2and\nreplaced it with the maximum likelihood estimate {\\textasciicircum} {\\sigma}2in the posterior. A\nmore standard Bayesian analysis would also put a prior on {\\sigma}(typically\ng({\\",
                    "sigma}){\\propto}1/{\\sigma}), calculate a joint posterior for {\\textmu}(x) and{\\sigma}, and then integrate\nout{\\sigma}, rather than just extract the maximum of the posterior distr ibution\n({\\textquotedblleft}MAP{\\textquotedblright} estimate)."
                ]
            },
            {
                "12": [
                    "This is page 417\nPrinter: Opaque this\n12\nSupport Vector Machines and\nFlexible Discriminants\n12.1 Introduction\nIn this chapter we describe generalizations of linear decis ion boundaries\nfor classification. Optimal separating hyperplanes are int roduced in Chap-\nter 4 for the case when two classes are linearly separable. He re we cover\nextensions to the nonseparable case, where the classes over lap. These tech-\nniquesarethengeneralizedtowhatisknownasthe support vector machine ,\nwhich produces nonlinear boundaries by constructing a line ar boundary in\nalarge, transformed versionof the featurespace. Thesecon dsetof methods\ngeneralize Fisher`s linear discriminant analysis (LDA). T he generalizations\ninclude{fl}exible discriminant analysis which facilitates construction of non-\nlinear boundaries in a manner very similar to the support vec tor machines,\npenalized discriminant analysis for problems such as signal and image clas-\nsification where the large number of features are highly corr elated, and\nmixture discriminant analysis for irregularly shaped classes.\n12.2 The Support Vector Classifier\nIn Chapter 4 we discussed a technique for constructing an optimalseparat-\ning hyperplane between two perfectly separated classes. We review this and\ngeneralize to the nonseparable case, where the classes may n ot be separable\nby a linear boundary.\n418 12. Flexible Discriminants\n\n\n\n\n\n\n\n\n\n\n\nmarginM=1\n{\\parallel}{\\beta}{\\parallel}\nM=1\n{\\parallel}{\\beta}{\\parallel}xT{\\beta}+{\\beta}0= 0\n\n\n\n\n\n\n\n\n\n\n\n \nmargin{\\xi}{*}\n1{\\xi}{*}\n1{\\xi}{*}\n1\n{\\xi}{*}\n2{\\xi}{*}\n2{\\xi}{*}\n2{\\xi}{*}\n3{\\xi}{*}\n3{\\xi}{*}\n4{\\xi}{*}\n4{\\xi}{*}\n4{\\xi}{*}\n5\nM=1\n{\\parallel}{\\beta}{\\parallel}\nM=1\n{\\parallel}{\\beta}{\\parallel}xT{\\beta}+{\\beta}0= 0\nFIGURE 12.1. Support vector classifiers. The",
                    " left panel shows the separable\ncase. The decision boundary is the solid line, while broken lines bound the shaded\nmaximal margin of width 2M= 2/{\\parallel}{\\beta}{\\parallel}. The right panel shows the nonseparable\n(overlap) case. The points labeled {\\xi}{*}\njare on the wrong side of their margin by\nan amount {\\xi}{*}\nj=M{\\xi}j; points on the correct side have {\\xi}{*}\nj= 0. The margin is\nmaximized subject to a total budget{\\sum}{\\xi}i{\\leq}constant. Hence{\\sum}{\\xi}{*}\njis the total\ndistance of points on the wrong side of their margin.\nOur training data consists of Npairs (x1,y1),(x2,y2),...,(xN,yN), with\nxi{\\in}IRpandyi{\\in} {\\{}{-}1,1{\\}}. Define a hyperplane by\n{\\{}x:f(x) =xT{\\beta}+{\\beta}0= 0{\\}}, (12.1)\nwhere{\\beta}is a unit vector:{\\parallel}{\\beta}{\\parallel}= 1. A classification rule induced by f(x) is\nG(x) = sign[xT{\\beta}+{\\beta}0]. (12.2)\nThegeometryofhyperplanesisreviewedinSection4.5,wher eweshowthat\nf(x) in (12.1) gives the signed distance from a point xto the hyperplane\nf(x) =xT{\\beta}+{\\beta}0= 0. Since the classes are separable, we can find a function\nf(x) =xT{\\beta}+{\\beta}0withyif(xi){>}0{\\forall}i. Hence we are able to find the\nhyperplane that creates the biggest marginbetween the training points for\nclass 1 and{-}1 (see Figure 12.1). The optimization problem\nmax\n{\\beta},{\\beta}0,{\\parallel}{\\beta}{\\parallel}=1M\nsubject toyi(xT\ni{\\beta}+{\\beta}0){\\geq}M, i= 1,...,N,(12.",
                    "3)\ncaptures this concept. The band in the figure is Munits away from the\nhyperplane on either side, and hence 2 Munits wide. It is called the margin.\nWe showed that this problem can be more conveniently rephras ed as\nmin\n{\\beta},{\\beta}0{\\parallel}{\\beta}{\\parallel}\nsubject toyi(xT\ni{\\beta}+{\\beta}0){\\geq}1, i= 1,...,N,(12.4)\n12.2 The Support Vector Classifier 419\nwhere we have dropped the norm constraint on {\\beta}. Note that M= 1/{\\parallel}{\\beta}{\\parallel}.\nExpression (12.4) is the usual way of writing the support vec tor criterion\nfor separated data. This is a convex optimization problem (q uadratic cri-\nterion, linear inequality constraints), and the solution i s characterized in\nSection 4.5.2.\nSuppose now that the classes overlap in feature space. One wa y to deal\nwith the overlap is to still maximize M, but allow for some points to be on\nthe wrong side of the margin. Define the slack variables {\\xi}= ({\\xi}1,{\\xi}2,...,{\\xi} N).\nThere are two natural ways to modify the constraint in (12.3) :\nyi(xT\ni{\\beta}+{\\beta}0){\\geq}M{-}{\\xi}i, (12.5)\nor\nyi(xT\ni{\\beta}+{\\beta}0){\\geq}M(1{-}{\\xi}i), (12.6)\n{\\forall}i, {\\xi}i{\\geq}0,{\\sum}N\ni=1{\\xi}i{\\leq}constant. The two choices lead to different solutions.\nThe first choice seems more natural, since it measures overla p in actual\ndistancefromthemargin;thesecondchoicemeasurestheove rlapinrelative\ndistance, which changes with the width of the margin M. However, the first\nchoice results in a nonconvex optimization problem, while t he second is\nconvex; thus (12.6) leads to the {\\textquotedblleft}standard{\\textquotedblright} support vector c lassifier, which",
                    "\nwe use from here on.\nHereistheideaoftheformulation.Thevalue {\\xi}iintheconstraint yi(xT\ni{\\beta}+\n{\\beta}0){\\geq}M(1{-}{\\xi}i) is the proportional amount by which the prediction\nf(xi) =xT\ni{\\beta}+{\\beta}0is on the wrong side of its margin. Hence by bounding the\nsum{\\sum}{\\xi}i, we bound the total proportional amount by which prediction s\nfall on the wrong side of their margin. Misclassifications oc cur when{\\xi}i{>}1,\nso bounding{\\sum}{\\xi}iat a valueKsay, bounds the total number of training\nmisclassifications at K.\nAs in (4.48) in Section 4.5.2, we can drop the norm constraint on{\\beta},\ndefineM= 1/{\\parallel}{\\beta}{\\parallel}, and write (12.4) in the equivalent form\nmin{\\parallel}{\\beta}{\\parallel}subject to{\\{}\nyi(xT\ni{\\beta}+{\\beta}0){\\geq}1{-}{\\xi}i{\\forall}i,\n{\\xi}i{\\geq}0,{\\sum}{\\xi}i{\\leq}constant.(12.7)\nThis is the usual way the support vector classifier is defined f or the non-\nseparable case. However we find confusing the presence of the fixed scale\n{\\textquotedblleft}1{\\textquotedblright} in the constraint yi(xT\ni{\\beta}+{\\beta}0){\\geq}1{-}{\\xi}i, and prefer to start with (12.6).\nThe right panel of Figure 12.1 illustrates this overlapping case.\nBy the nature of the criterion (12.7), we see that points well inside their\nclass boundary do not play a big role in shaping the boundary. This seems\nlike an attractive property, and one that differentiates it f rom linear dis-\ncriminant analysis (Section 4.3). In LDA, the decision boun dary is deter-\nmined by the covariance of the class distributions and the po sitions of the\nclass centroids. We will see in Section 12.",
                    "3.3 that logistic regression is more\nsimilar to the support vector classifier in this regard.\n420 12. Flexible Discriminants\n12.2.1 Computing the Support Vector Classifier\nThe problem (12.7) is quadratic with linear inequality cons traints, hence it\nis a convex optimization problem. We describe a quadratic pr ogramming\nsolution using Lagrange multipliers. Computationally it i s convenient to\nre-express (12.7) in the equivalent form\nmin\n{\\beta},{\\beta}01\n2{\\parallel}{\\beta}{\\parallel}2+CN{\\sum}\ni=1{\\xi}i\nsubject to {\\xi}i{\\geq}0, yi(xT\ni{\\beta}+{\\beta}0){\\geq}1{-}{\\xi}i{\\forall}i,(12.8)\nwhere the {\\textquotedblleft}cost{\\textquotedblright} parameter Creplaces the constant in (12.7); the separable\ncase corresponds to C={\\infty}.\nThe Lagrange (primal) function is\nLP=1\n2{\\parallel}{\\beta}{\\parallel}2+CN{\\sum}\ni=1{\\xi}i{-}N{\\sum}\ni=1{\\alpha}i[yi(xT\ni{\\beta}+{\\beta}0){-}(1{-}{\\xi}i)]{-}N{\\sum}\ni=1{\\textmu}i{\\xi}i,(12.9)\nwhich we minimize w.r.t {\\beta},{\\beta}0and{\\xi}i. Setting the respective derivatives to\nzero, we get\n{\\beta}=N{\\sum}\ni=1{\\alpha}iyixi, (12.10)\n0 =N{\\sum}\ni=1{\\alpha}iyi, (12.11)\n{\\alpha}i=C{-}{\\textmu}i,{\\forall}i, (12.12)\nas well as the positivity constraints {\\alpha}i, {\\textmu}i, {\\xi}i{\\geq}0{\\forall}i. By substituting\n(12.10){\\textendash}(12.12) into (12.9), we obtain the Lag",
                    "rangian (Wol fe) dual objec-\ntive function\nLD=N{\\sum}\ni=1{\\alpha}i{-}1\n2N{\\sum}\ni=1N{\\sum}\ni{'}=1{\\alpha}i{\\alpha}i{'}yiyi{'}xT\nixi{'}, (12.13)\nwhich gives a lower bound on the objective function (12.8) fo r any feasible\npoint. We maximize LDsubject to 0{\\leq}{\\alpha}i{\\leq}Cand{\\sum}N\ni=1{\\alpha}iyi= 0. In\naddition to (12.10){\\textendash}(12.12), the Karush{\\textendash}Kuhn{\\textendash}Tucker cond itions include\nthe constraints\n{\\alpha}i[yi(xT\ni{\\beta}+{\\beta}0){-}(1{-}{\\xi}i)] = 0, (12.14)\n{\\textmu}i{\\xi}i= 0, (12.15)\nyi(xT\ni{\\beta}+{\\beta}0){-}(1{-}{\\xi}i){\\geq}0, (12.16)\nfori= 1,...,N. Together these equations (12.10){\\textendash}(12.16) uniquely char-\nacterize the solution to the primal and dual problem.\n12.2 The Support Vector Classifier 421\nFrom (12.10) we see that the solution for {\\beta}has the form\n{\\textasciicircum}{\\beta}=N{\\sum}\ni=1{\\textasciicircum}{\\alpha}iyixi, (12.17)\nwith nonzero coefficients {\\textasciicircum} {\\alpha}ionly for those observations ifor which the\nconstraints in (12.16) are exactly met (due to (12.14)). The se observations\nare called the support vectors , since{\\textasciicircum}{\\beta}is represented in terms of them\nalone. Among these support points, some will lie on the edge o f the margin\n({\\textasciicircum}{\\xi}i= 0), and hence from (12.15) and (12.12) will",
                    " be characterize d by\n0{<}{\\textasciicircum}{\\alpha}i{<} C; the remainder ( {\\textasciicircum}{\\xi}i{>}0) have {\\textasciicircum}{\\alpha}i=C. From (12.14) we can\nsee that any of these margin points (0 {<}{\\textasciicircum}{\\alpha}i,{\\textasciicircum}{\\xi}i= 0) can be used to solve\nfor{\\beta}0, and we typically use an average of all the solutions for nume rical\nstability.\nMaximizing the dual (12.13) is a simpler convex quadratic pr ogramming\nproblemthantheprimal(12.9),andcanbesolvedwithstanda rdtechniques\n(Murray et al., 1981, for example).\nGiven the solutions {\\textasciicircum}{\\beta}0and{\\textasciicircum}{\\beta}, the decision function can be written as\n{\\textasciicircum}G(x) = sign[ {\\textasciicircum}f(x)]\n= sign[xT{\\textasciicircum}{\\beta}+{\\textasciicircum}{\\beta}0]. (12.18)\nThe tuning parameter of this procedure is the cost parameter C.\n12.2.2 Mixture Example (Continued)\nFigure 12.2 shows the support vector boundary for the mixtur e example\nof Figure 2.5 on page 21, with two overlapping classes, for tw o different\nvalues of the cost parameter C. The classifiers are rather similar in their\nperformance. Points on the wrong side of the boundary are sup port vectors.\nIn addition, points on the correct side of the boundary but cl ose to it (in\nthe margin), are also support vectors. The margin is larger f orC= 0.01\nthan it is for C= 10,000. Hence larger values of Cfocus attention more\non (correctly classified) points near the decision boundary , while smaller\nvalues involve data further away. Either way, misclassified points are given\nweight, no matter how far away. In this example the procedure is not very\nsensitive to choices of C, because of the rigidity of a linear boundary.\nThe optimal value for C",
                    "can be estimated by cross-validation, as dis-\ncussed in Chapter 7. Interestingly, the leave-one-out cros s-validation error\ncan be bounded above by the proportion of support points in th e data. The\nreason is that leaving out an observation that is not a suppor t vector will\nnot change the solution. Hence these observations, being cl assified correctly\nby the original boundary, will be classified correctly in the cross-validation\nprocess. However this bound tends to be too high, and not gene rally useful\nfor choosing C(62{\\%} and 85{\\%}, respectively, in our examples).\n422 12. Flexible Discriminants\n\n\nFIGURE 12.2. The linear support vector boundary for the mixture data exam-\nple with two overlapping classes, for two different values of C. The broken lines\nindicate the margins, where f(x) ={\\pm}1. The support points ( {\\alpha}i{>}0) are all the\npoints on the wrong side of their margin. The black solid dots ar e those support\npoints falling exactly on the margin ( {\\xi}i= 0, {\\alpha}i{>}0). In the upper panel 62{\\%}of\nthe observations are support points, while in the lower panel 85{\\%}are. The broken\npurple curve in the background is the Bayes decision boundary .\n12.3 Support Vector Machines and Kernels 423\n12.3 Support Vector Machines and Kernels\nThe support vector classifier described so far finds linear bo undaries in the\ninput feature space. As with other linear methods, we can mak e the pro-\ncedure more {fl}exible by enlarging the feature space using bas is expansions\nsuch as polynomials or splines (Chapter 5). Generally linea r boundaries\nin the enlarged space achieve better training-class separa tion, and trans-\nlate to nonlinear boundaries in the original space. Once the basis functions\nhm(x), m= 1,...,Mare selected, the procedure is the same as before. We\nfittheSVclassifierusinginputfeatures h(xi) = (h1(xi),h2(xi),...,h M(xi)),\ni= 1,...,N, and produce the (nonlinear) function {\\textasciicircum}f(x) =h(x)T{\\textasci",
                    "icircum}{\\beta}+{\\textasciicircum}{\\beta}0.\nThe classifier is {\\textasciicircum}G(x) = sign( {\\textasciicircum}f(x)) as before.\nThesupport vector machine classifier is an extension of this idea, where\nthe dimension of the enlarged space is allowed to get very lar ge, infinite\nin some cases. It might seem that the computations would beco me pro-\nhibitive. It would also seem that with sufficient basis functi ons, the data\nwould be separable, and overfitting would occur. We first show how the\nSVM technology deals with these issues. We then see that in fa ct the SVM\nclassifier is solving a function-fitting problem using a part icular criterion\nand form of regularization, and is part of a much bigger class of problems\nthat includes the smoothing splines of Chapter 5. The reader may wish\nto consult Section 5.8, which provides background material and overlaps\nsomewhat with the next two sections.\n12.3.1 Computing the SVM for Classification\nWe can represent the optimization problem (12.9) and its sol ution in a\nspecial way that only involves the input features via inner p roducts. We do\nthis directly for the transformed feature vectors h(xi). We then see that for\nparticular choices of h, these inner products can be computed very cheaply.\nThe Lagrange dual function (12.13) has the form\nLD=N{\\sum}\ni=1{\\alpha}i{-}1\n2N{\\sum}\ni=1N{\\sum}\ni{'}=1{\\alpha}i{\\alpha}i{'}yiyi{'}{\\langle}h(xi),h(xi{'}){\\rangle}. (12.19)\nFrom (12.10) we see that the solution function f(x) can be written\nf(x) =h(x)T{\\beta}+{\\beta}0\n=N{\\sum}\ni=1{\\alpha}iyi{\\langle}h(x),h(xi){\\rangle}+{\\beta}0. (12.20)\nAs before, given {\\alpha}i,{\\beta}0can be determined by solving yif(xi) = 1 in",
                    " (12.20)\nfor any (or all) xifor which 0 {<}{\\alpha}i{<}C.\n424 12. Flexible Discriminants\nSo both (12.19) and (12.20) involve h(x) only through inner products. In\nfact, we need not specify the transformation h(x) at all, but require only\nknowledge of the kernel function\nK(x,x{'}) ={\\langle}h(x),h(x{'}){\\rangle} (12.21)\nthat computes inner products in the transformed space. Kshould be a\nsymmetric positive (semi-) definite function; see Section 5 .8.1.\nThree popular choices for Kin the SVM literature are\ndth-Degree polynomial: K(x,x{'}) = (1+{\\langle}x,x{'}{\\rangle})d,\nRadial basis: K(x,x{'}) = exp({-}{\\gamma}{\\parallel}x{-}x{'}{\\parallel}2),\nNeural network: K(x,x{'}) = tanh({\\kappa}1{\\langle}x,x{'}{\\rangle}+{\\kappa}2).(12.22)\nConsider for example a feature space with two inputs X1andX2, and a\npolynomial kernel of degree 2. Then\nK(X,X{'}) = (1+{\\langle}X,X{'}{\\rangle})2\n= (1+X1X{'}\n1+X2X{'}\n2)2\n= 1+2X1X{'}\n1+2X2X{'}\n2+(X1X{'}\n1)2+(X2X{'}\n2)2+2X1X{'}\n1X2X{'}\n2.\n(12.23)\nThenM= 6, and if we choose h1(X) = 1,h2(X) ={\\sqrt{}}\n2X1,h3(X) ={\\sqrt{}}\n2X2,h4(X) =X2\n1,h5(X) =X2\n2,andh6",
                    "(X) ={\\sqrt{}}\n2X1X2,thenK(X,X{'}) =\n{\\langle}h(X),h(X{'}){\\rangle}. From (12.20) we see that the solution can be written\n{\\textasciicircum}f(x) =N{\\sum}\ni=1{\\textasciicircum}{\\alpha}iyiK(x,xi)+{\\textasciicircum}{\\beta}0. (12.24)\nThe role of the parameter Cis clearer in an enlarged feature space,\nsince perfect separation is often achievable there. A large value ofCwill\ndiscourage any positive {\\xi}i, and lead to an overfit wiggly boundary in the\noriginal feature space; a small value of Cwill encourage a small value of\n{\\parallel}{\\beta}{\\parallel}, which in turn causes f(x) and hence the boundary to be smoother.\nFigure 12.3 show two nonlinear support vector machines appl ied to the\nmixture example of Chapter 2. The regularization parameter was chosen\nin both cases to achieve good test error. The radial basis ker nel produces\na boundary quite similar to the Bayes optimal boundary for th is example;\ncompare Figure 2.5.\nIn the early literature on support vectors, there were claim s that the\nkernel property of the support vector machine is unique to it and allows\none to finesse the curse of dimensionality. Neither of these c laims is true,\nand we go into both of these issues in the next three subsectio ns.\n12.3 Support Vector Machines and Kernels 425\nSVM - Degree-4 Polynomial in Feature Space\n\nFIGURE 12.3. Two nonlinear SVMs for the mixture data. The upper plot uses\na4th degree polynomial kernel, the lower a radial basis kernel (wi th{\\gamma}= 1). In\neach caseCwas tuned to approximately achieve the best test error perfor mance,\nandC= 1worked well in both cases. The radial basis kernel performs t he best\n(close to Bayes optimal), as might be expected given the data ar ise from mixtures\nof Gaussians. The broken purple curve in the background is the B ayes decision\nboundary.\n426 12. Flexible Discriminants\n{",
                    "-}3 {-}2 {-}1 0 1 2 30.0 0.5 1.0 1.5 2.0 2.5 3.0Hinge Loss\nBinomial Deviance\nSquared Error\nClass HuberLoss\nyf\nFIGURE 12.4. The support vector loss function (hinge loss), compared to the\nnegative log-likelihood loss (binomial deviance) for logistic r egression, squared-er-\nror loss, and a {\\textquotedblleft}Huberized{\\textquotedblright} version of the squared hinge loss. A ll are shown as a\nfunction of yfrather than f, because of the symmetry between the y= +1and\ny={-}1case. The deviance and Huber have the same asymptotes as the S VM\nloss, but are rounded in the interior. All are scaled to have the limiting left-tail\nslope of{-}1.\n12.3.2 The SVM as a Penalization Method\nWithf(x) =h(x)T{\\beta}+{\\beta}0, consider the optimization problem\nmin\n{\\beta}0, {\\beta}N{\\sum}\ni=1[1{-}yif(xi)]++{\\lambda}\n2{\\parallel}{\\beta}{\\parallel}2(12.25)\nwhere the subscript {\\textquotedblleft}+{\\textquotedblright} indicates positive part. This has th e formloss+\npenalty, which is a familiar paradigm in function estimation. It is e asy to\nshow (Exercise 12.1) that the solution to (12.25), with {\\lambda}= 1/C, is the\nsame as that for (12.8).\nExamination of the {\\textquotedblleft}hinge{\\textquotedblright} loss function L(y,f) = [1{-}yf]+shows that\nit is reasonable for two-class classification, when compare d to other more\ntraditional loss functions. Figure 12.4 compares it to the l og-likelihood loss\nfor logistic regression, as well as squared-error loss and a variant thereof.\nThe (negative) log-likelihood or binomial deviance has sim ilar tails as the\nSVM loss, giving zero penalty to points well inside their mar gin,",
                    " and a\n12.3 Support Vector Machines and Kernels 427\nTABLE 12.1. The population minimizers for the different loss functions in F ig-\nure 12.4. Logistic regression uses the binomial log-likelihoo d or deviance. Linear\ndiscriminant analysis (Exercise 4.2) uses squared-error los s. The SVM hinge loss\nestimates the mode of the posterior class probabilities, wher eas the others estimate\na linear transformation of these probabilities.\nLoss Function L[y,f(x)] Minimizing Function\nBinomial\nDeviance log[1+e{-}yf(x)]f(x) = logPr(Y= +1|x)\nPr(Y= -1|x)\nSVM Hinge\nLoss[1{-}yf(x)]+ f(x) = sign[Pr( Y= +1|x){-}1\n2]\nSquared\nError[y{-}f(x)]2= [1{-}yf(x)]2f(x) = 2Pr(Y= +1|x){-}1\n{\\textquotedblleft}Huberised{\\textquotedblright}\nSquare\nHinge Loss{-}4yf(x), yf (x){<}-1\n[1{-}yf(x)]2\n+otherwisef(x) = 2Pr(Y= +1|x){-}1\nlinear penalty to points on the wrong side and far away. Squar ed-error, on\nthe other hand gives a quadratic penalty, and points well ins ide their own\nmargin have a strong in{fl}uence on the model as well. The square d hinge\nlossL(y,f) = [1{-}yf]2\n+is like the quadratic, except it is zero for points\ninside their margin. It still rises quadratically in the lef t tail, and will be\nless robust than hinge or deviance to misclassified observat ions. Recently\nRossetandZhu(2007)proposeda{\\textquotedblleft}Huberized{\\textquotedblright}versionofthes quaredhinge\nloss, which converts smoothly to a linear loss at yf={-}1.\nWe can characterize these loss functions in terms",
                    " of what the y are es-\ntimating at the population level. We consider minimizing E L(Y,f(X)).\nTable 12.1 summarizes the results. Whereas the hinge loss es timates the\nclassifierG(x) itself, all the others estimate a transformation of the cla ss\nposterior probabilities. The {\\textquotedblleft}Huberized{\\textquotedblright} square hinge los s shares attractive\nproperties of logistic regression (smooth loss function, e stimates probabili-\nties), as well as the SVM hinge loss (support points).\nFormulation (12.25) casts the SVM as a regularized function estimation\nproblem, where the coefficients of the linear expansion f(x) ={\\beta}0+h(x)T{\\beta}\nareshrunktowardzero(excludingtheconstant).If h(x)representsahierar-\nchical basis having some ordered structure (such as ordered in roughness),\n428 12. Flexible Discriminants\nthen the uniform shrinkage makes more sense if the rougher el ementshjin\nthe vectorhhave smaller norm.\nAll the loss-functions in Table 12.1 except squared-error a re so called\n{\\textquotedblleft}margin maximizing loss-functions{\\textquotedblright} (Rosset et al., 2004b) . This means that\nif the data are separable, then the limit of {\\textasciicircum}{\\beta}{\\lambda}in (12.25) as {\\lambda}{\\textrightarrow}0 defines\nthe optimal separating hyperplane1.\n12.3.3 Function Estimation and Reproducing Kernels\nHere we describe SVMs in terms of function estimation in repr oducing\nkernel Hilbert spaces, where the kernel property abounds. T his material is\ndiscussed in some detail in Section 5.8. This provides anoth er view of the\nsupport vector classifier, and helps to clarify how it works.\nSuppose the basis harises from the (possibly finite) eigen-expansion of\na positive definite kernel K,\nK(x,x{'}) ={\\infty}{\\sum}\nm=1{\\varphi}m(x){\\varphi}m(x{'}){\\delta}m (12.26)\nandhm(x) ={\\sq",
                    "rt{}}{\\delta}m{\\varphi}m(x). Then with {\\theta}m={\\sqrt{}}{\\delta}m{\\beta}m, we can write (12.25)\nas\nmin\n{\\beta}0, {\\theta}N{\\sum}\ni=1[\n1{-}yi({\\beta}0+{\\infty}{\\sum}\nm=1{\\theta}m{\\varphi}m(xi))]\n++{\\lambda}\n2{\\infty}{\\sum}\nm=1{\\theta}2\nm\n{\\delta}m.(12.27)\nNow (12.27) is identical in form to (5.49) on page 169 in Secti on 5.8, and\nthe theory of reproducing kernel Hilbert spaces described t here guarantees\na finite-dimensional solution of the form\nf(x) ={\\beta}0+N{\\sum}\ni=1{\\alpha}iK(x,xi). (12.28)\nIn particular we see there an equivalent version of the optim ization crite-\nrion (12.19) [Equation (5.67) in Section 5.8.2; see also Wah ba et al. (2000)],\nmin\n{\\beta}0,{\\alpha}N{\\sum}\ni=1(1{-}yif(xi))++{\\lambda}\n2{\\alpha}TK{\\alpha}, (12.29)\nwhereKis theN{\\texttimes}Nmatrix of kernel evaluations for all pairs of training\nfeatures (Exercise 12.2).\nThese models are quite general, and include, for example, th e entire fam-\nily of smoothing splines, additive and interaction spline m odels discussed\n1For logistic regression with separable data, {\\textasciicircum}{\\beta}{\\lambda}diverges, but {\\textasciicircum}{\\beta}{\\lambda}/{\\parallel}{\\textasciicircum}{\\beta}{\\lambda}{\\parallel}converges to\nthe optimal separating direction.\n12.3 Support Vector Machines and Kernels 429\nin Chapters 5 and 9, and in more detail in Wahba (1990) and Hast ie and\nTibshirani (1990). They can be expressed more generally as\nmin\n",
                    "f{\\in}HN{\\sum}\ni=1[1{-}yif(xi)]++{\\lambda}J(f), (12.30)\nwhereHis the structured space of functions, and J(f) an appropriate reg-\nularizer on that space. For example, suppose His the space of additive\nfunctionsf(x) ={\\sum}p\nj=1fj(xj), andJ(f) ={\\sum}\nj{\\int}\n{\\{}f{'}{'}\nj(xj){\\}}2dxj. Then the\nsolution to (12.30) is an additive cubic spline, and has a ker nel representa-\ntion (12.28) with K(x,x{'}) ={\\sum}p\nj=1Kj(xj,x{'}\nj). Each of the Kjis the kernel\nappropriate for the univariate smoothing spline in xj(Wahba, 1990).\nConverselythisdiscussionalsoshowsthat,forexample, anyofthekernels\ndescribed in (12.22) above can be used with anyconvex loss function, and\nwill also lead to a finite-dimensional representation of the form (12.28).\nFigure 12.5 uses the same kernel functions as in Figure 12.3, except using\nthe binomial log-likelihood as a loss function2. The fitted function is hence\nan estimate of the log-odds,\n{\\textasciicircum}f(x) = log{\\textasciicircum}Pr(Y= +1|x)\n{\\textasciicircum}Pr(Y={-}1|x)\n={\\textasciicircum}{\\beta}0+N{\\sum}\ni=1{\\textasciicircum}{\\alpha}iK(x,xi), (12.31)\nor conversely we get an estimate of the class probabilities\n{\\textasciicircum}Pr(Y= +1|x) =1\n1+e{-}{\\textasciicircum}{\\beta}0{-}{\\sum}N\ni=1{\\textasciicircum}{\\alpha}iK(x,xi). (12.32)\nThe fitted models are quite similar in shape and",
                    " performance. Examples\nand more details are given in Section 5.8.\nIt does happen that for SVMs, a sizable fraction of the Nvalues of{\\alpha}i\ncan be zero (the nonsupport points). In the two examples in Fi gure 12.3,\nthese fractions are 42{\\%} and 45{\\%}, respectively. This is a cons equence of the\npiecewise linear nature of the first part of the criterion (12 .25). The lower\nthe class overlap (on the training data), the greater this fr action will be.\nReducing{\\lambda}will generally reduce the overlap (allowing a more {fl}exible f).\nA small number of support points means that {\\textasciicircum}f(x) can be evaluated more\nquickly, which is important at lookup time. Of course, reduc ing the overlap\ntoo much can lead to poor generalization.\n2Ji Zhu assisted in the preparation of these examples.\n430 12. Flexible Discriminants\nLR - Degree-4 Polynomial in Feature Space\n\nFIGURE 12.5. The logistic regression versions of the SVM models in Fig-\nure 12.3, using the identical kernels and hence penalties, but the log-likelihood\nloss instead of the SVM loss function. The two broken contours c orrespond to\nposterior probabilities of 0.75and0.25for the +1 class (or vice versa). The bro-\nken purple curve in the background is the Bayes decision bound ary.\n12.3 Support Vector Machines and Kernels 431\nTABLE 12.2. Skin of the orange: Shown are mean (standard error of the mean )\nof the test error over 50simulations. BRUTO fits an additive spline model adap-\ntively, while MARS fits a low-order interaction model adaptively .\nTest Error (SE)\nMethod No Noise Features Six Noise Features\n1 SV Classifier 0.450 (0.003) 0.472 (0.003)\n2 SVM/poly 2 0.078 (0.003) 0.152 (0.004)\n3 SVM/poly 5 0.180 (0.004) 0.370 (0.004)\n4 SVM/poly 10 0.230 (0.003) 0.434 (0.002)\n5 BRUTO 0.084 (",
                    "0.003) 0.090 (0.003)\n6 MARS 0.156 (0.004) 0.173 (0.005)\nBayes 0.029 0.029\n12.3.4 SVMs and the Curse of Dimensionality\nIn this section, we address the question of whether SVMs have some edge\non the curse of dimensionality. Notice that in expression (1 2.23) we are not\nallowed a fully general inner product in the space of powers a nd products.\nFor example, all terms of the form 2 XjX{'}\njare given equal weight, and the\nkernel cannot adapt itself to concentrate on subspaces. If t he number of\nfeaturespwere large, but the class separation occurred only in the lin ear\nsubspace spanned by say X1andX2, this kernel would not easily find the\nstructure and would suffer from having many dimensions to sea rch over.\nOne would have to build knowledge about the subspace into the kernel;\nthat is, tell it to ignore all but the first two inputs. If such k nowledge were\navailable a priori, much of statistical learning would be ma de much easier.\nA major goal of adaptive methods is to discover such structur e.\nWe support these statements with an illustrative example. W e generated\n100 observations in each of two classes. The first class has fo ur standard\nnormal independent features X1,X2,X3,X4. The second class also has four\nstandard normal independent features, but conditioned on 9 {\\leq}{\\sum}X2\nj{\\leq}16.\nThis is a relatively easy problem. As a second harder problem , we aug-\nmented the features with an additional six standard Gaussia n noise fea-\ntures. Hence the second class almost completely surrounds t he first, like the\nskin surrounding the orange, in a four-dimensional subspac e. The Bayes er-\nror rate for this problem is 0 .029 (irrespective of dimension). We generated\n1000 test observations to compare different procedures. The average test\nerrors over 50 simulations, with and without noise features , are shown in\nTable 12.2.\nLine 1 uses the support vector classifier in the original feat ure space.\nLines2{\\textendash}4refertothesupportvectormachinewitha2-,5-and 10-dimension-",
                    "\nal polynomial kernel. For all support vector procedures, we chose the cost\nparameterCto minimize the test error, to be as fair as possible to the\n432 12. Flexible Discriminants\n1e{-}01 1e+01 1e+030.20 0.25 0.30 0.35\n1e{-}01 1e+01 1e+03 1e{-}01 1e+01 1e+03 1e{-}01 1e+01 1e+03Test Error\nCTest Error Curves {-} SVM with Radial Kernel\n{\\gamma}= 5 {\\gamma}= 1 {\\gamma}= 0.5 {\\gamma}= 0.1\nFIGURE 12.6. Test-error curves as a function of the cost parameter Cfor\nthe radial-kernel SVM classifier on the mixture data. At the top of each plot is\nthe scale parameter {\\gamma}for the radial kernel: K{\\gamma}(x,y) = exp( {-}{\\gamma}||x{-}y||2). The\noptimal value for Cdepends quite strongly on the scale of the kernel. The Bayes\nerror rate is indicated by the broken horizontal lines.\nmethod. Line 5 fits an additive spline model to the ( {-}1,+1) response by\nleast squares, using the BRUTO algorithm for additive model s, described\nin Hastie and Tibshirani (1990). Line 6 uses MARS (multivari ate adaptive\nregression splines) allowing interaction of all orders, as described in Chap-\nter 9; as such it is comparable with the SVM/poly 10. Both BRUT O and\nMARS have the ability to ignore redundant variables. Test er ror was not\nused to choose the smoothing parameters in either of lines 5 o r 6.\nIn the original feature space, a hyperplane cannot separate the classes,\nand the support vector classifier (line 1) does poorly. The po lynomial sup-\nport vector machine makes a substantial improvement in test error rate,\nbut is adversely affected by the six noise features. It is also very sensitive to\nthe choice of kernel: the second degree polynomial kernel (l ine 2) does best,\nsince the true decision boundary is a second-degree polynom ial. However,\nhigher-degree",
                    " polynomial kernels (lines 3 and 4) do much wor se. BRUTO\nperforms well, since the boundary is additive. BRUTO and MAR S adapt\nwell: their performance does not deteriorate much in the pre sence of noise.\n12.3.5 A Path Algorithm for the SVM Classifier\nThe regularization parameter for the SVM classifier is the co st parameter\nC, or its inverse {\\lambda}in (12.25). Common usage is to set Chigh, leading often\nto somewhat overfit classifiers.\nFigure 12.6 shows the test error on the mixture data as a funct ion of\nC, using different radial-kernel parameters {\\gamma}. When{\\gamma}= 5 (narrow peaked\nkernels), the heaviest regularization (small C) is called for. With {\\gamma}= 1\n12.3 Support Vector Machines and Kernels 433\n{-}0.5 0.0 0.5 1.0 1.5 2.0{-}1.0 {-}0.5 0.0 0.5 1.0 1.5789\n1011\n12\n123\n45\n61/||{\\beta}|| f(x) = 0f(x) = +1\nf(x) ={-}1\n0.0 0.2 0.4 0.6 0.8 1.00 2 4 6 8 101\n2\n34\n56\n789\n1011\n12\n{\\alpha}i({\\lambda}){\\lambda}\nFIGURE 12.7. A simple example illustrates the SVM path algorithm. (left\npanel:) This plot illustrates the state of the model at {\\lambda}= 1/2. The {\\textquoteleft}{\\textquoteleft}+1{\\textquotedblright} points\nare orange, the {\\textquotedblleft} {-}1{\\textquotedblright} blue. The width of the soft margin is 2/||{\\beta}||= 2{\\texttimes}0.587.\nTwo blue points {\\{}3,5{\\}}are misclassified, while the two orange points {\\{}10,12{\\}}are\ncorrectly classified, but on the wrong side of their margin f(x) = +1; each of\nthese hasyif(xi){<}1. The three square shaped points {\\{}2,6,7",
                    "{\\}}are exactly on\ntheir margins. (right panel:) This plot shows the piecewise lin ear profiles {\\alpha}i({\\lambda}).\nThe horizontal broken line at {\\lambda}= 1/2indicates the state of the {\\alpha}ifor the model\nin the left plot.\n(the value used in Figure 12.3), an intermediate value of Cis required.\nClearly in situations such as these, we need to determine a go od choice\nforC, perhaps by cross-validation. Here we describe a path algor ithm (in\nthe spirit of Section 3.8) for efficiently fitting the entire se quence of SVM\nmodels obtained by varying C.\nIt is convenient to use the loss+penalty formulation (12.25 ), along with\nFigure 12.4. This leads to a solution for {\\beta}at a given value of {\\lambda}:\n{\\beta}{\\lambda}=1\n{\\lambda}N{\\sum}\ni=1{\\alpha}iyixi. (12.33)\nThe{\\alpha}iare again Lagrange multipliers, but in this case they all lie in [0,1].\nFigure 12.7 illustrates the setup. It can be shown that the KK T optimal-\nity conditions imply that the labeled points ( xi,yi) fall into three distinct\ngroups:\n434 12. Flexible Discriminants\nObservationscorrectlyclassifiedandoutsidetheirmargin s.Theyhave\nyif(xi){>}1, and Lagrange multipliers {\\alpha}i= 0. Examples are the\norange points 8, 9 and 11, and the blue points 1 and 4.\nObservationssittingontheirmarginswith yif(xi) = 1,withLagrange\nmultipliers {\\alpha}i{\\in}[0,1]. Examples are the orange 7 and the blue 2 and\n6.\nObservations inside their margins have yif(xi){<}1, with{\\alpha}i= 1.\nExamples are the blue 3 and 5, and the orange 10 and 12.\nThe idea for the path algorithm is as follows. Initially {\\lambda}is large, the\nmargin 1/||{\\beta}{\\lambda}||is wide, and all points are inside their margin and have\n{\\alpha}i= 1. As{\\lambda}decreases, 1 /||{\\beta}{\\lambda}",
                    "||decreases, and the margin gets narrower.\nSome points will move from inside their margins to outside th eir margins,\nandtheir{\\alpha}iwillchangefrom1to0.Bycontinuity ofthe {\\alpha}i({\\lambda}),thesepoints\nwilllingeron the margin during this transition. From (12.33) we see tha t\nthe points with {\\alpha}i= 1 make fixed contributions to {\\beta}({\\lambda}), and those with\n{\\alpha}i= 0 make no contribution. So all that changes as {\\lambda}decreases are the\n{\\alpha}i{\\in}[0,1] of those (small number) of points on the margin. Since all t hese\npoints have yif(xi) = 1, this results in a small set of linear equations that\nprescribe how {\\alpha}i({\\lambda}) and hence {\\beta}{\\lambda}changes during these transitions. This\nresults in piecewise linear paths for each of the {\\alpha}i({\\lambda}). The breaks occur\nwhen points cross the margin. Figure 12.7 (right panel) show s the{\\alpha}i({\\lambda})\nprofiles for the small example in the left panel.\nAlthough we have described this for linear SVMs, exactly the same idea\nworks for nonlinear models, in which (12.33) is replaced by\nf{\\lambda}(x) =1\n{\\lambda}N{\\sum}\ni=1{\\alpha}iyiK(x,xi). (12.34)\nDetails can be found in Hastie et al. (2004). An Rpackagesvmpath is\navailable on CRAN for fitting these models.\n12.3.6 Support Vector Machines for Regression\nIn this section we show how SVMs can be adapted for regression with a\nquantitative response, in ways that inherit some of the prop erties of the\nSVM classifier. We first discuss the linear regression model\nf(x) =xT{\\beta}+{\\beta}0, (12.35)\nand then handle nonlinear generalizations. To estimate {\\beta}, we consider min-\nimization of\nH({\\beta},{\\beta}0) =N{\\sum}\ni=1V(yi{-}f(xi))+{\\lambda}\n2{\\parallel}{\\beta}{\\parallel}2, (12.",
                    "36)\n12.3 Support Vector Machines and Kernels 435\n-4 -2 0 2 4-1 0 1 2 3 4\n-4 -2 0 2 40 2 4 6 8 10 12{-} c {-}cVH(r)V(r)\nr r\nFIGURE 12.8. The left panel shows the -insensitive error function used by the\nsupport vector regression machine. The right panel shows th e error function used\nin Huber`s robust regression (blue curve). Beyond |c|, the function changes from\nquadratic to linear.\nwhere\nV(r) ={\\{}\n0 if|r|{<},\n|r|{-},otherwise.(12.37)\nThis is an {\\textquotedblleft} -insensitive{\\textquotedblright} error measure, ignoring errors of size less t han\n(left panel of Figure 12.8). There is a rough analogy with the support\nvector classification setup, where points on the correct sid e of the deci-\nsion boundary and far away from it, are ignored in the optimiz ation. In\nregression, these {\\textquotedblleft}low error{\\textquotedblright} points are the ones with small residuals.\nIt is interesting to contrast this with error measures used i n robust re-\ngression in statistics. The most popular, due to Huber (1964 ), has the form\nVH(r) ={\\{}\nr2/2 if|r|{\\leq}c,\nc|r|{-}c2/2,|r|{>}c,(12.38)\nshownintherightpanelofFigure12.8.Thisfunctionreduce sfromquadratic\nto linear the contributions of observations with absolute r esidual greater\nthan a prechosen constant c. This makes the fitting less sensitive to out-\nliers. The support vector error measure (12.37) also has lin ear tails (beyond\n), but in addition it {fl}attens the contributions of those case s with small\nresiduals.\nIf{\\textasciicircum}{\\beta},{\\textasciicircum}{\\beta}0are the minimizers of H, the solution function can be shown to\nhave the form\n{\\textasciicircum}{\\beta}=N{\\sum",
                    "}\ni=1({\\textasciicircum}{\\alpha}{*}\ni{-}{\\textasciicircum}{\\alpha}i)xi, (12.39)\n{\\textasciicircum}f(x) =N{\\sum}\ni=1({\\textasciicircum}{\\alpha}{*}\ni{-}{\\textasciicircum}{\\alpha}i){\\langle}x,xi{\\rangle}+{\\beta}0, (12.40)\n436 12. Flexible Discriminants\nwhere {\\textasciicircum}{\\alpha}i,{\\textasciicircum}{\\alpha}{*}\niare positive and solve the quadratic programming problem\nmin\n{\\alpha}i,{\\alpha}{*}\niN{\\sum}\ni=1({\\alpha}{*}\ni+{\\alpha}i){-}N{\\sum}\ni=1yi({\\alpha}{*}\ni{-}{\\alpha}i)+1\n2N{\\sum}\ni,i{'}=1({\\alpha}{*}\ni{-}{\\alpha}i)({\\alpha}{*}\ni{'}{-}{\\alpha}i{'}){\\langle}xi,xi{'}{\\rangle}\nsubject to the constraints\n0{\\leq}{\\alpha}i, {\\alpha}{*}\ni{\\leq}1/{\\lambda},\nN{\\sum}\ni=1({\\alpha}{*}\ni{-}{\\alpha}i) = 0, (12.41)\n{\\alpha}i{\\alpha}{*}\ni= 0.\nDuetothenatureoftheseconstraints,typicallyonlyasubs etofthesolution\nvalues ({\\textasciicircum}{\\alpha}{*}\ni{-}{\\textasciicircum}{\\alpha}i) are nonzero, and the associated data values are called the\nsupport vectors. As was the case in the classification settin g, the solution\ndepends on the input values only through the inner products {\\langle}xi,xi{'}{\\rangle}. Thus\nwe can generalize the methods to richer spaces by defining an a ppropriate\ninner product, for example, one of those defined",
                    " in (12.22).\nNote that there are parameters, and{\\lambda}, associated with the criterion\n(12.36). These seem to play different roles. is a parameter of the loss\nfunctionV, just likecis forVH. Note that both VandVHdepend on the\nscale ofyand hencer. If we scale our response (and hence use VH(r/{\\sigma}) and\nV(r/{\\sigma})instead),thenwemightconsiderusingpresetvaluesfor cand(the\nvaluec= 1.345 achieves 95{\\%} efficiency for the Gaussian). The quantity {\\lambda}\nis a more traditional regularization parameter, and can be e stimated for\nexample by cross-validation.\n12.3.7 Regression and Kernels\nAs discussed in Section 12.3.3, this kernel property is not u nique to sup-\nport vector machines. Suppose we consider approximation of the regression\nfunction in terms of a set of basis functions {\\{}hm(x){\\}},m= 1,2,...,M:\nf(x) =M{\\sum}\nm=1{\\beta}mhm(x)+{\\beta}0. (12.42)\nTo estimate {\\beta}and{\\beta}0we minimize\nH({\\beta},{\\beta}0) =N{\\sum}\ni=1V(yi{-}f(xi))+{\\lambda}\n2{\\sum}\n{\\beta}2\nm (12.43)\nfor some general error measure V(r). For any choice of V(r), the solution\n{\\textasciicircum}f(x) ={\\sum}{\\textasciicircum}{\\beta}mhm(x)+{\\textasciicircum}{\\beta}0has the form\n{\\textasciicircum}f(x) =N{\\sum}\ni=1{\\textasciicircum}aiK(x,xi) (12.44)\n12.3 Support Vector Machines and Kernels 437\nwithK(x,y) ={\\sum}M\nm=1hm(x)hm(y). Notice that this has the same form\nas both the radial basis function expansion and a regulariza tion estimate,\ndiscussed in Chapters 5 and 6.\nFor concreteness, let`s work out the case V",
                    "(r) =r2. LetHbe theN{\\texttimes}M\nbasis matrix with imth element hm(xi), and suppose that M {>}Nis large.\nFor simplicity we assume that {\\beta}0= 0, or that the constant is absorbed in\nh; see Exercise 12.3 for an alternative.\nWe estimate {\\beta}by minimizing the penalized least squares criterion\nH({\\beta}) = (y{-}H{\\beta})T(y{-}H{\\beta})+{\\lambda}{\\parallel}{\\beta}{\\parallel}2. (12.45)\nThe solution is\n{\\textasciicircum}y=H{\\textasciicircum}{\\beta} (12.46)\nwith{\\textasciicircum}{\\beta}determined by\n{-}HT(y{-}H{\\textasciicircum}{\\beta})+{\\lambda}{\\textasciicircum}{\\beta}= 0. (12.47)\nFrom this it appears that we need to evaluate the M{\\texttimes}Mmatrix of inner\nproducts in the transformed space. However, we can premulti ply byHto\ngive\nH{\\textasciicircum}{\\beta}= (HHT+{\\lambda}I){-}1HHTy. (12.48)\nTheN{\\texttimes}NmatrixHHTconsists of inner products between pairs of obser-\nvationsi,i{'}; that is, the evaluation of an inner product kernel {\\{}HHT{\\}}i,i{'}=\nK(xi,xi{'}). It is easy to show (12.44) directly in this case, that the pr edicted\nvalues at an arbitrary xsatisfy\n{\\textasciicircum}f(x) =h(x)T{\\textasciicircum}{\\beta}\n=N{\\sum}\ni=1{\\textasciicircum}{\\alpha}iK(x,xi), (12.49)\nwhere {\\textasciicircum}{\\alpha}= (HHT+{\\lambda}I){-}1y. As in the support vector machine, we need not\nspecify or evaluate the large set of functions h1(x),h2(x),...,",
                    "h M(x). Only\nthe inner product kernel K(xi,xi{'}) need be evaluated, at the Ntraining\npoints for each i,i{'}and at points xfor predictions there. Careful choice\nofhm(such as the eigenfunctions of particular, easy-to-evalua te kernels\nK) means, for example, that HHTcan be computed at a cost of N2/2\nevaluations of K, rather than the direct cost N2M.\nNote, however, that this property depends on the choice of sq uared norm\n{\\parallel}{\\beta}{\\parallel}2in the penalty. It does not hold, for example, for the L1norm|{\\beta}|,\nwhich may lead to a superior model."
                ]
            }
        ]
    },
    "An Introduction to Statistical Learning": {
        "authors": [
            "Gareth James",
            "Daniela Witten",
            "Trevor Hastie",
            "Robert Tibshirani"
        ],
        "year": 2023,
        "chapters": [
            {
                "6.2": [
                    "240 6. Linear Model Selection and Regularization\nrepeated the validation set approach using a different split of the data into\na training set and a validation set, or if we repeated cross-validation using\na different set of cross-validation folds, then the precise model with the\nlowest estimated test error would surely change. In this setting, we can\nselect a model using the one-standard-error rule . We first calculate theone-\nstandard-\nerror\nrulestandard error of the estimated test MSE for each model size, and then\nselect the smallest model for which the estimated test error is within one\nstandard error of the lowest point on the curve. The rationale here is that\nif a set of models appear to be more or less equally good, then we might\nas well choose the simplest model{\\textemdash}that is, the model with the smallest\nnumber of predictors. In this case, applying the one-standard-error rule\nto the validation set or cross-validation approach leads to selection of the\nthree-variable model.\n6.2 Shrinkage Methods\nThe subset selection methods described in Section 6.1involve using least\nsquares to fit a linear model that contains a subset of the predictors. As an\nalternative, we can fit a model containing all ppredictors using a technique\nthatconstrains orregularizes the coefficient estimates, or equivalently, that\nshrinksthe coefficient estimates towards zero. It may not be immediately\nobvious why such a constraint should improve the fit, but it turns out that\nshrinking the coefficient estimates can significantly reduce their variance.\nThe two best-known techniques for shrinking the regression coefficients\ntowards zero are ridge regression and thelasso.\n6.2.1 Ridge Regression\nRecall from Chapter 3that the least squares fitting procedure estimates\n{\\beta}0,{\\beta}1,...,{\\beta}pusing the values that minimize\nRSS =n{\\sum}\ni=1\nyi{-}{\\beta}0{-}p{\\sum}\nj=1{\\beta}jxij\n2\n.\nRidge regression is very similar to least squares, except that the coefficientsridge\nregressionare estimated by minimizing a slightly different quantity. In particular, the\nridge regression coefficient estimates {\\textasciicircum}{\\beta}Rare the values that minimize\nn{\\sum}\ni=1\n",
                    "yi{-}{\\beta}0{-}p{\\sum}\nj=1{\\beta}jxij\n2\n+{\\lambda}p{\\sum}\nj=1{\\beta}2\nj= RSS + {\\lambda}p{\\sum}\nj=1{\\beta}2\nj, (6.5)\nwhere{\\lambda}{\\geq}0is atuning parameter , to be determined separately. Equa-tuning\nparametertion6.5trades off two different criteria. As with least squares, ridge regres-\nsion seeks coefficient estimates that fit the data well, by making the RSS\nsmall. However, the second term, {\\lambda}{\\sum}\nj{\\beta}2\nj, called a shrinkage penalty , isshrinkage\npenaltysmall when {\\beta}1,...,{\\beta}pare close to zero, and so it has the effect of shrinking\nthe estimates of {\\beta}jtowards zero. The tuning parameter {\\lambda}serves to control\n6.2 Shrinkage Methods 241\n1e{-}02 1e+00 1e+02 1e+04{-}300 {-}100 0 100 200 300 400Standardized CoefficientsIncomeLimitRatingStudent\n0.0 0.2 0.4 0.6 0.8 1.0{-}300 {-}100 0 100 200 300 400Standardized Coefficients{\\lambda}{\\parallel}{\\textasciicircum}{\\beta}R{\\lambda}{\\parallel}2/{\\parallel}{\\textasciicircum}{\\beta}{\\parallel}2FIGURE 6.4. The standardized ridge regression coefficients are displayed for\ntheCreditdata set, as a function of {\\lambda}and{\\parallel}{\\textasciicircum}{\\beta}R\n{\\lambda}{\\parallel}2/{\\parallel}{\\textasciicircum}{\\beta}{\\parallel}2.\nthe relative impact of these two terms on the regression coefficient esti-\nmates. When {\\lambda}=0, the penalty term has no effect, and ridge regression\nwill produce the least squares estimates. However, as {\\lambda}{\\textrightarrow}{\\infty}, the impact of\nthe shrinkage penalty grows, and the ridge regression coefficient estimates\nwill approach zero. Unlike least squares, which generates only one set of",
                    " co-\nefficient estimates, ridge regression will produce a different set of coefficient\nestimates, {\\textasciicircum}{\\beta}R\n{\\lambda}, for each value of {\\lambda}. Selecting a good value for {\\lambda}is critical;\nwe defer this discussion to Section 6.2.3, where we use cross-validation.\nNote that in ( 6.5), the shrinkage penalty is applied to {\\beta}1,...,{\\beta}p, but\nnot to the intercept {\\beta}0. We want to shrink the estimated association of\neach variable with the response; however, we do not want to shrink the\nintercept, which is simply a measure of the mean value of the response\nwhenxi1=xi2=...=xip=0. If we assume that the variables{\\textemdash}that is,\nthe columns of the data matrix X{\\textemdash}have been centered to have mean zero\nbefore ridge regression is performed, then the estimated intercept will take\nthe form {\\textasciicircum}{\\beta}0={\\textasciimacron}y={\\sum}n\ni=1yi/n.\nAn Application to the Credit Data\nIn Figure 6.4, the ridge regression coefficient estimates for the Creditdata\nset are displayed. In the left-hand panel, each curve corresponds to the\nridge regression coefficient estimate for one of the ten variables, plotted\nas a function of {\\lambda}. For example, the black solid line represents the ridge\nregression estimate for the incomecoefficient, as {\\lambda}is varied. At the extreme\nleft-hand side of the plot, {\\lambda}is essentially zero, and so the corresponding\nridge coefficient estimates are the same as the usual least squares esti-\nmates. But as {\\lambda}increases, the ridge coefficient estimates shrink towards\nzero. When {\\lambda}is extremely large, then all of the ridge coefficient estimates\nare basically zero; this corresponds to the null model that contains no pre-\ndictors. In this plot, the income,limit,rating, andstudentvariables are\ndisplayed in distinct colors, since these variables tend to have by far the\nlargest coefficient estimates. While the ridge coefficient estimates tend to\ndecrease in aggregate as {\\lambda}increases, individual coefficients, such as rating\nandincome, may occasionally increase as {\\lambda}increases.\n242 6. Linear Model Selection and Regularization",
                    "\nThe right-hand panel of Figure 6.4displays the same ridge coefficient\nestimates as the left-hand panel, but instead of displaying {\\lambda}on thex-axis,\nwe now display {\\parallel}{\\textasciicircum}{\\beta}R\n{\\lambda}{\\parallel}2/{\\parallel}{\\textasciicircum}{\\beta}{\\parallel}2, where {\\textasciicircum}{\\beta}denotes the vector of least squares\ncoefficient estimates. The notation {\\parallel}{\\beta}{\\parallel}2denotes the {\\ell}2norm(pronounced{\\ell}2norm\n{\\textquotedblleft}ell 2{\\textquotedblright}) of a vector, and is defined as {\\parallel}{\\beta}{\\parallel}2={\\sqrt{}}{\\sum}p\nj=1{\\beta}j2. It measures the\ndistance of {\\beta}from zero. As {\\lambda}increases, the {\\ell}2norm of {\\textasciicircum}{\\beta}R\n{\\lambda}willalways\ndecrease, and so will {\\parallel}{\\textasciicircum}{\\beta}R\n{\\lambda}{\\parallel}2/{\\parallel}{\\textasciicircum}{\\beta}{\\parallel}2. The latter quantity ranges from 1 (when\n{\\lambda}=0, in which case the ridge regression coefficient estimate is the same\nas the least squares estimate, and so their {\\ell}2norms are the same) to 0\n(when{\\lambda}={\\infty}, in which case the ridge regression coefficient estimate is a\nvector of zeros, with {\\ell}2norm equal to zero). Therefore, we can think of the\nx-axis in the right-hand panel of Figure 6.4as the amount that the ridge\nregression coefficient estimates have been shrunken towards zero; a small\nvalue indicates that they have been shrunken very close to zero.\nThe standard least squares coefficient estimates discussed in Chapter 3\narescale equivariant : multiplying Xjby a constant csimply leads to ascale\nequivariantscaling of the least squares coefficient estimates by a factor of 1/c. In other\nwords, regardless of how the jth predictor is scaled, Xj{\\textasciicircum}{\\beta}",
                    "jwill remain the\nsame. In contrast, the ridge regression coefficient estimates can change sub-\nstantially when multiplying a given predictor by a constant. For instance,\nconsider the incomevariable, which is measured in dollars. One could rea-\nsonably have measured income in thousands of dollars, which would result\ninareductionintheobservedvaluesof incomebyafactorof1,000.Nowdue\nto the sum of squared coefficients term in the ridge regression formulation\n(6.5), such a change in scale will not simply cause the ridge regression co-\nefficient estimate for incometo change by a factor of 1,000. In other words,\nXj{\\textasciicircum}{\\beta}R\nj,{\\lambda}will depend not only on the value of {\\lambda}, but also on the scaling of the\njth predictor. In fact, the value of Xj{\\textasciicircum}{\\beta}R\nj,{\\lambda}may even depend on the scaling\nof theotherpredictors! Therefore, it is best to apply ridge regression after\nstandardizing the predictors , using the formula\n{\\textasciitilde}xij=xij{\\sqrt{}}\n1\nn{\\sum}n\ni=1(xij{-}xj)2, (6.6)\nso that they are all on the same scale. In ( 6.6), the denominator is the\nestimated standard deviation of the jth predictor. Consequently, all of the\nstandardized predictors will have a standard deviation of one. As a re-\nsult the final fit will not depend on the scale on which the predictors are\nmeasured. In Figure 6.4, they-axis displays the standardized ridge regres-\nsion coefficient estimates{\\textemdash}that is, the coefficient estimates that result from\nperforming ridge regression using standardized predictors.\nWhy Does Ridge Regression Improve Over Least Squares?\nRidgeregression`sadvantageoverleastsquaresisrootedinthe bias-variance\ntrade-off. As{\\lambda}increases, the flexibility of the ridge regression fit decreases,\nleading to decreased variance but increased bias. This is illustrated in the\nleft-hand panel of Figure 6.5, using a simulated data set containing p= 45\npredictors and n= 50 observations. The green curve in the left-hand panel\n",
                    "6.2 Shrinkage Methods 243\n1e{-}01 1e+01 1e+030 10 20 30 40 50 60Mean Squared Error0.0 0.2 0.4 0.6 0.8 1.00 10 20 30 40 50 60Mean Squared Error{\\lambda}{\\parallel}{\\textasciicircum}{\\beta}R{\\lambda}{\\parallel}2/{\\parallel}{\\textasciicircum}{\\beta}{\\parallel}2FIGURE 6.5. Squared bias (black), variance (green), and test mean squared\nerror (purple) for the ridge regression predictions on a simulated data set, as a\nfunction of {\\lambda}and{\\parallel}{\\textasciicircum}{\\beta}R\n{\\lambda}{\\parallel}2/{\\parallel}{\\textasciicircum}{\\beta}{\\parallel}2. The horizontal dashed lines indicate the minimum\npossible MSE. The purple crosses indicate the ridge regression models for which\nthe MSE is smallest.\nof Figure 6.5displays the variance of the ridge regression predictions as a\nfunction of {\\lambda}. At the least squares coefficient estimates, which correspond\nto ridge regression with {\\lambda}=0, the variance is high but there is no bias. But\nas{\\lambda}increases, the shrinkage of the ridge coefficient estimates leads to a\nsubstantial reduction in the variance of the predictions, at the expense of a\nslight increase in bias. Recall that the test mean squared error (MSE), plot-\nted in purple, is closely related to the variance plus the squared bias. For\nvalues of {\\lambda}up to about 10, the variance decreases rapidly, with very little\nincrease in bias, plotted in black. Consequently, the MSE drops consider-\nably as{\\lambda}increases from 0to10. Beyond this point, the decrease in variance\ndue to increasing {\\lambda}slows, and the shrinkage on the coefficients causes them\nto be significantly underestimated, resulting in a large increase in the bias.\nThe minimum MSE is achieved at approximately {\\lambda}= 30 . Interestingly,\nbecause of its high variance, the MSE associated with the least squares\nfit, when {\\lambda}=0, is almost as high as that of the null model for which all\ncoefficient estimates are zero, when {\\lambda}={\\",
                    "infty}. However, for an intermediate\nvalue of{\\lambda}, the MSE is considerably lower.\nThe right-hand panel of Figure 6.5displays the same curves as the left-\nhand panel, this time plotted against the {\\ell}2norm of the ridge regression\ncoefficient estimates divided by the {\\ell}2norm of the least squares estimates.\nNow as we move from left to right, the fits become more flexible, and so\nthe bias decreases and the variance increases.\nIn general, in situations where the relationship between the response\nand the predictors is close to linear, the least squares estimates will have\nlow bias but may have high variance. This means that a small change in\nthe training data can cause a large change in the least squares coefficient\nestimates. In particular, when the number of variables pis almost as large\nas the number of observations n, as in the example in Figure 6.5, the\nleast squares estimates will be extremely variable. And if p{>}n , then the\nleast squares estimates do not even have a unique solution, whereas ridge\nregression can still perform well by trading off a small increase in bias for a\n244 6. Linear Model Selection and Regularization\nlarge decrease in variance. Hence, ridge regression works best in situations\nwhere the least squares estimates have high variance.\nRidge regression also has substantial computational advantages over best\nsubset selection, which requires searching through 2pmodels. As we dis-\ncussed previously, even for moderate values of p, such a search can be\ncomputationally infeasible. In contrast, for any fixed value of {\\lambda}, ridge re-\ngression only fits a single model, and the model-fitting procedure can be\nperformed quite quickly. In fact, one can show that the computations re-\nquired to solve ( 6.5),simultaneously for all values of {\\lambda}, are almost identical\nto those for fitting a model using least squares.\n6.2.2 The Lasso\nRidge regression does have one obvious disadvantage. Unlike best subset,\nforward stepwise, and backward stepwise selection, which will generally\nselect models that involve just a subset of the variables, ridge regression\nwill include all ppredictors in the final model. The penalty {\\lambda}{\\sum}{\\beta}2\njin (6.5)\nwillshrinkallofthecoefficientstowardszero,butitwillnotset",
                    "anyofthem\nexactly to zero (unless {\\lambda}={\\infty}). This may not be a problem for prediction\naccuracy, but it can create a challenge in model interpretation in settings in\nwhich the number of variables pis quite large. For example, in the Credit\ndata set, it appears that the most important variables are income,limit,\nrating, andstudent. So we might wish to build a model including just\nthese predictors. However, ridge regression will always generate a model\ninvolving all ten predictors. Increasing the value of {\\lambda}will tend to reduce\nthe magnitudes of the coefficients, but will not result in exclusion of any of\nthe variables.\nThelassois a relatively recent alternative to ridge regression that over-lassocomes this disadvantage. The lasso coefficients, {\\textasciicircum}{\\beta}L\n{\\lambda}, minimize the quantity\nn{\\sum}\ni=1\nyi{-}{\\beta}0{-}p{\\sum}\nj=1{\\beta}jxij\n2\n+{\\lambda}p{\\sum}\nj=1|{\\beta}j|= RSS + {\\lambda}p{\\sum}\nj=1|{\\beta}j|. (6.7)\nComparing ( 6.7) to (6.5), we see that the lasso and ridge regression have\nsimilar formulations. The only difference is that the {\\beta}2\njterm in the ridge\nregression penalty ( 6.5) has been replaced by |{\\beta}j|in the lasso penalty ( 6.7).\nIn statistical parlance, the lasso uses an {\\ell}1(pronounced {\\textquotedblleft}ell 1{\\textquotedblright}) penalty\ninstead of an {\\ell}2penalty. The {\\ell}1norm of a coefficient vector {\\beta}is given by\n{\\parallel}{\\beta}{\\parallel}1={\\sum}|{\\beta}j|.\nAs with ridge regression, the lasso shrinks the coefficient estimates to-\nwards zero. However, in the case of the lasso, the {\\ell}1penalty has the effect\nof forcing some of the coefficient estimates to be exactly equal to zero when\nthetuningparameter {\\lambda}issufficientlylarge.Hence,muchlikebestsubsetse-\nlection, the lasso performs variable selection",
                    " . As a result, models generated\nfrom the lasso are generally much easier to interpret than those produced\nby ridge regression. We say that the lasso yields sparsemodels{\\textemdash}that is,sparse\nmodels that involve only a subset of the variables. As in ridge regression,\nselecting a good value of {\\lambda}for the lasso is critical; we defer this discussion\nto Section 6.2.3, where we use cross-validation.\n6.2 Shrinkage Methods 245\n20 50 100 200 500 2000 5000{-}200 0 100 200 300 400Standardized Coefficients0.0 0.2 0.4 0.6 0.8 1.0{-}300 {-}100 0 100 200 300 400Standardized CoefficientsIncomeLimitRatingStudent{\\lambda}{\\parallel}{\\textasciicircum}{\\beta}L{\\lambda}{\\parallel}1/{\\parallel}{\\textasciicircum}{\\beta}{\\parallel}1FIGURE 6.6. The standardized lasso coefficients on the Creditdata set are\nshown as a function of {\\lambda}and{\\parallel}{\\textasciicircum}{\\beta}L\n{\\lambda}{\\parallel}1/{\\parallel}{\\textasciicircum}{\\beta}{\\parallel}1.\nAs an example, consider the coefficient plots in Figure 6.6, which are gen-\nerated from applying the lasso to the Creditdata set. When {\\lambda}=0, then\nthe lasso simply gives the least squares fit, and when {\\lambda}becomes sufficiently\nlarge, the lasso gives the null model in which all coefficient estimates equal\nzero. However, in between these two extremes, the ridge regression and\nlasso models are quite different from each other. Moving from left to right\nin the right-hand panel of Figure 6.6, we observe that at first the lasso re-\nsults in a model that contains only the ratingpredictor. Then studentand\nlimitenter the model almost simultaneously, shortly followed by income.\nEventually, the remaining variables enter the model. Hence, depending on\nthe value of {\\lambda}, the lasso can produce a model involving any number of vari-\nables. In contrast, ridge regression will always include all of the variables in\nthe model, although the magnitude of the coefficient estimates will depend\non{\\lambda}.",
                    "\nAnother Formulation for Ridge Regression and the Lasso\nOne can show that the lasso and ridge regression coefficient estimates solve\nthe problems\nminimize\n{\\beta}\n\nn{\\sum}\ni=1\nyi{-}{\\beta}0{-}p{\\sum}\nj=1{\\beta}jxij\n2\n\nsubject top{\\sum}\nj=1|{\\beta}j|{\\leq}s\n(6.8)\nand\nminimize\n{\\beta}\n\nn{\\sum}\ni=1\nyi{-}{\\beta}0{-}p{\\sum}\nj=1{\\beta}jxij\n2\n\nsubject top{\\sum}\nj=1{\\beta}2\nj{\\leq}s,\n(6.9)\nrespectively. In other words, for every value of {\\lambda}, there is some ssuch that\nthe Equations ( 6.7) and (6.8) will give the same lasso coefficient estimates.\nSimilarly, for every value of {\\lambda}there is a corresponding ssuch that Equa-\ntions(6.5)and(6.9)willgivethesameridgeregressioncoefficientestimates.\n246 6. Linear Model Selection and Regularization\nWhenp=2, then (6.8) indicates that the lasso coefficient estimates have\nthe smallest RSS out of all points that lie within the diamond defined by\n|{\\beta}1|+|{\\beta}2|{\\leq}s. Similarly, the ridge regression estimates have the smallest\nRSS out of all points that lie within the circle defined by {\\beta}2\n1+{\\beta}2\n2{\\leq}s.\nWecanthinkof( 6.8)asfollows.Whenweperformthelassowearetrying\nto find the set of coefficient estimates that lead to the smallest RSS, subject\nto the constraint that there is a budgetsfor how large{\\sum}p\nj=1|{\\beta}j|can be.\nWhensis extremely large, then this budget is not very restrictive, and so\nthe coefficient estimates can be large. In fact, if sis large enough that the\nleast squares solution falls within the budget, then ( 6.8) will simply yield\nthe least squares solution. In contrast, if sis small, then{\\sum}p\nj=1",
                    "|{\\beta}j|must be\nsmall in order to avoid violating the budget. Similarly, ( 6.9) indicates that\nwhen we perform ridge regression, we seek a set of coefficient estimates\nsuch that the RSS is as small as possible, subject to the requirement that{\\sum}p\nj=1{\\beta}2\njnot exceed the budget s.\nThe formulations ( 6.8) and (6.9) reveal a close connection between the\nlasso, ridge regression, and best subset selection. Consider the problem\nminimize\n{\\beta}\n\nn{\\sum}\ni=1\nyi{-}{\\beta}0{-}p{\\sum}\nj=1{\\beta}jxij\n2\n\nsubject top{\\sum}\nj=1I({\\beta}j= 0) {\\leq}s.\n(6.10)\nHereI({\\beta}j= 0) isanindicatorvariable:ittakesonavalueof1if {\\beta}j=0,and\nequals zero otherwise. Then ( 6.10) amounts to finding a set of coefficient\nestimates such that RSS is as small as possible, subject to the constraint\nthat no more than scoefficients can be nonzero. The problem ( 6.10) is\nequivalent to best subset selection. Unfortunately, solving ( 6.10) is com-\nputationally infeasible when pis large, since it requires considering all(p\ns)\nmodels containing spredictors. Therefore, we can interpret ridge regression\nand the lasso as computationally feasible alternatives to best subset selec-\ntion that replace the intractable form of the budget in ( 6.10) with forms\nthat are much easier to solve. Of course, the lasso is much more closely\nrelated to best subset selection, since the lasso performs feature selection\nforssufficiently small in ( 6.8), while ridge regression does not.\nThe Variable Selection Property of the Lasso\nWhy is it that the lasso, unlike ridge regression, results in coefficient esti-\nmates that are exactly equal to zero? The formulations ( 6.8) and (6.9) can\nbe used to shed light on the issue. Figure 6.7illustrates the situation. The\nleast squares solution is marked as {\\textasciicircum}{\\beta}, while the blue diamond and circle\nrepresent the l",
                    "asso and ridge regression constraints in ( 6.8) and (6.9), re-\nspectively. If sis sufficiently large, then the constraint regions will contain\n{\\textasciicircum}{\\beta}, and so the ridge regression and lasso estimates will be the same as the\nleast squares estimates. (Such a large value of scorresponds to {\\lambda}=0 in\n(6.5) and (6.7).) However, in Figure 6.7the least squares estimates lie out-\nside of the diamond and the circle, and so the least squares estimates are\nnot the same as the lasso and ridge regression estimates.\nEach of the ellipses centered around {\\textasciicircum}{\\beta}represents a contour: this meanscontourthat all of the points on a particular ellipse have the same RSS value. As\n6.2 Shrinkage Methods 247\nFIGURE 6.7. Contours of the error and constraint functions for the lasso\n(left)and ridge regression (right). The solid blue areas are the constraint regions,\n|{\\beta}1|+|{\\beta}2|{\\leq}sand{\\beta}2\n1+{\\beta}2\n2{\\leq}s, while the red ellipses are the contours of the RSS.\nthe ellipses expand away from the least squares coefficient estimates, the\nRSS increases. Equations ( 6.8) and (6.9) indicate that the lasso and ridge\nregression coefficient estimates are given by the first point at which an\nellipse contacts the constraint region. Since ridge regression has a circular\nconstraintwithnosharppoints,thisintersectionwillnotgenerallyoccuron\nan axis, and so the ridge regression coefficient estimates will be exclusively\nnon-zero. However, the lasso constraint has cornersat each of the axes, and\nso the ellipse will often intersect the constraint region at an axis. When this\noccurs, one of the coefficients will equal zero. In higher dimensions, many of\nthe coefficient estimates may equal zero simultaneously. In Figure 6.7, the\nintersection occurs at {\\beta}1=0, and so the resulting model will only include\n{\\beta}2.\nIn Figure 6.7, we considered the simple case of p=2. When p=3,\nthen the constraint region for ridge regression becomes",
                    " a sphere, and the\nconstraint region for the lasso becomes a polyhedron. When p{>}3, the\nconstraint for ridge regression becomes a hypersphere, and the constraint\nfor the lasso becomes a polytope. However, the key ideas depicted in Fig-\nure6.7still hold. In particular, the lasso leads to feature selection when\np{>}2due to the sharp corners of the polyhedron or polytope.\nComparing the Lasso and Ridge Regression\nIt is clear that the lasso has a major advantage over ridge regression, in\nthat it produces simpler and more interpretable models that involve only a\nsubset of the predictors. However, which method leads to better prediction\naccuracy? Figure 6.8displays the variance, squared bias, and test MSE of\nthe lasso applied to the same simulated data as in Figure 6.5. Clearly the\nlasso leads to qualitatively similar behavior to ridge regression, in that as {\\lambda}\nincreases, the variance decreases and the bias increases. In the right-hand\n248 6. Linear Model Selection and Regularization\n0.02 0.10 0.50 2.00 10.00 50.000 10 20 30 40 50 60Mean Squared Error0.0 0.2 0.4 0.6 0.8 1.00 10 20 30 40 50 60R2 on Training DataMean Squared Error{\\lambda}FIGURE 6.8. Left:Plots of squared bias (black), variance (green), and test\nMSE (purple) for the lasso on a simulated data set. Right:Comparison of squared\nbias, variance, and test MSE between lasso (solid) and ridge (dotted). Both are\nplotted against their R2on the training data, as a common form of indexing. The\ncrosses in both plots indicate the lasso model for which the MSE is smallest.\npanel of Figure 6.8, the dotted lines represent the ridge regression fits.\nHere we plot both against their R2on the training data. This is another\nuseful way to index models, and can be used to compare models with\ndifferent types of regularization, as is the case here. In this example, the\nlasso and ridge regression result in almost identical biases. However, the\nvariance of ridge regression is slightly lower than the variance of the l",
                    "asso.\nConsequently,theminimumMSEofridgeregressionisslightlysmallerthan\nthat of the lasso.\nHowever, the data in Figure 6.8were generated in such a way that all 45\npredictorswererelatedtotheresponse{\\textemdash}thatis,noneofthetruecoefficients\n{\\beta}1,...,{\\beta}45equaled zero. The lasso implicitly assumes that a number of the\ncoefficients truly equal zero. Consequently, it is not surprising that ridge\nregression outperforms the lasso in terms of prediction error in this setting.\nFigure6.9illustrates a similar situation, except that now the response is a\nfunction of only 2 out of 45predictors. Now the lasso tends to outperform\nridge regression in terms of bias, variance, and MSE.\nThese two examples illustrate that neither ridge regression nor the lasso\nwill universally dominate the other. In general, one might expect the lasso\nto perform better in a setting where a relatively small number of predictors\nhave substantial coefficients, and the remaining predictors have coefficients\nthat are very small or that equal zero. Ridge regression will perform better\nwhen the response is a function of many predictors, all with coefficients of\nroughly equal size. However, the number of predictors that is related to the\nresponse is never known a priori for real data sets. A technique such as\ncross-validation can be used in order to determine which approach is better\non a particular data set.\nAs with ridge regression, when the least squares estimates have exces-\nsively high variance, the lasso solution can yield a reduction in variance\nat the expense of a small increase in bias, and consequently can gener-\nate more accurate predictions. Unlike ridge regression, the lasso performs\nvariable selection, and hence results in models that are easier to interpret.\n6.2 Shrinkage Methods 249\n0.02 0.10 0.50 2.00 10.00 50.000 20 40 60 80 100Mean Squared Error0.4 0.5 0.6 0.7 0.8 0.9 1.00 20 40 60 80 100R2 on Training DataMean Squared Error{\\lambda}FIGURE 6.9. Left:Plots of squared bias (black), variance (green), and test\nMSE (purple) for the lasso. The simulated data is similar to that in Figure",
                    " 6.8,\nexcept that now only two predictors are related to the response. Right:Comparison\nof squared bias, variance, and test MSE between lasso (solid) and ridge (dotted).\nBoth are plotted against their R2on the training data, as a common form of\nindexing. The crosses in both plots indicate the lasso model for which the MSE is\nsmallest.\nThereareveryefficientalgorithmsforfittingbothridgeandlassomodels;\nin both cases the entire coefficient paths can be computed with about the\nsame amount of work as a single least squares fit. We will explore this\nfurther in the lab at the end of this chapter.\nA Simple Special Case for Ridge Regression and the Lasso\nIn order to obtain a better intuition about the behavior of ridge regression\nand the lasso, consider a simple special case with n=p, andXa diag-\nonal matrix with 1`s on the diagonal and 0`s in all off-diagonal elements.\nTo simplify the problem further, assume also that we are performing regres-\nsion without an intercept. With these assumptions, the usual least squares\nproblem simplifies to finding {\\beta}1,...,{\\beta}pthat minimize\np{\\sum}\nj=1(yj{-}{\\beta}j)2. (6.11)\nIn this case, the least squares solution is given by\n{\\textasciicircum}{\\beta}j=yj.\nAnd in this setting, ridge regression amounts to finding {\\beta}1,...,{\\beta}psuch that\np{\\sum}\nj=1(yj{-}{\\beta}j)2+{\\lambda}p{\\sum}\nj=1{\\beta}2\nj (6.12)\nis minimized, and the lasso amounts to finding the coefficients such that\np{\\sum}\nj=1(yj{-}{\\beta}j)2+{\\lambda}p{\\sum}\nj=1|{\\beta}j| (6.13)\n250 6. Linear Model Selection and Regularization\n{-}1.5 {-}0.5 0.0 0.5 1.0 1.5{-}1.5 {-}0.5 0.5 1.5Coefficient EstimateRidgeLeast Squares\n{-}1.5",
                    " {-}0.5 0.0 0.5 1.0 1.5{-}1.5 {-}0.5 0.5 1.5Coefficient EstimateLassoLeast Squares\nyjyjFIGURE 6.10. The ridge regression and lasso coefficient estimates for a simple\nsetting with n=pandXa diagonal matrix with 1`s on the diagonal. Left:The\nridge regression coefficient estimates are shrunken proportionally towards zero,\nrelative to the least squares estimates. Right:The lasso coefficient estimates are\nsoft-thresholded towards zero.\nis minimized. One can show that in this setting, the ridge regression esti-\nmates take the form\n{\\textasciicircum}{\\beta}R\nj=yj/(1 + {\\lambda}), (6.14)\nand the lasso estimates take the form\n{\\textasciicircum}{\\beta}L\nj=\n\nyj{-}{\\lambda}/2ifyj{>}{\\lambda}/2;\nyj+{\\lambda}/2ifyj{<}{-}{\\lambda}/2;\n0 if|yj|{\\leq}{\\lambda}/2.(6.15)\nFigure6.10displays the situation. We can see that ridge regression and\nthe lasso perform two very different types of shrinkage. In ridge regression,\neach least squares coefficient estimate is shrunken by the same proportion.\nIn contrast, the lasso shrinks each least squares coefficient towards zero by\na constant amount, {\\lambda}/2; the least squares coefficients that are less than\n{\\lambda}/2in absolute value are shrunken entirely to zero. The type of shrink-\nage performed by the lasso in this simple setting ( 6.15) is known as soft-\nthresholding . The fact that some lasso coefficients are shrunken entirely tosoft-\nthresholdingzero explains why the lasso performs feature selection.\nIn the case of a more general data matrix X, the story is a little more\ncomplicated than what is depicted in Figure 6.10, but the main ideas still\nhold approximately: ridge regression more or less shrinks every dimension\nof the data by the same proportion, whereas the lasso more or less shrinks\nall coefficients toward zero by a similar amount, and sufficiently small co-\nefficients are shrunken all the way",
                    " to zero.\nBayesian Interpretation of Ridge Regression and the Lasso\nWe now show that one can view ridge regression and the lasso through\na Bayesian lens. A Bayesian viewpoint for regression assumes that the\ncoefficient vector {\\beta}has some priordistribution, say p({\\beta}), where {\\beta}=\n({\\beta}0,{\\beta}1,...,{\\beta}p)T. The likelihood of the data can be written as f(Y|X,{\\beta}),\n6.2 Shrinkage Methods 251\n{-}3 {-}2 {-}1 0 1 2 30.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7{-}3 {-}2 {-}1 0 1 2 30.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7{\\beta}j{\\beta}j\ng({\\beta}j)g({\\beta}j)\nFIGURE 6.11. Left:Ridge regression is the posterior mode for {\\beta}under a Gaus-\nsian prior. Right:The lasso is the posterior mode for {\\beta}under a double-exponential\nprior.\nwhereX=(X1,...,X p). Multiplying the prior distribution by the likeli-\nhood gives us (up to a proportionality constant) the posterior distribution ,posterior\ndistributionwhich takes the form\np({\\beta}|X,Y){\\propto}f(Y|X,{\\beta})p({\\beta}|X)=f(Y|X,{\\beta})p({\\beta}),\nwhere the proportionality above follows from Bayes` theorem, and the\nequality above follows from the assumption that Xis fixed.\nWe assume the usual linear model,\nY={\\beta}0+X1{\\beta}1+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+Xp{\\beta}p+{\\epsilon},\nand suppose that the errors are independent and drawn from a normal dis-\ntribution. Furthermore, assume that p({\\beta})={\\prod}p\nj=1g({\\beta}j), for some density\nfunction g. It turns out that ridge regression and the lasso follow naturally\nfrom two special cases of g",
                    ":\n Ifgis a Gaussian distribution with mean zero and standard deviation\na function of {\\lambda}, then it follows that the posterior mode for{\\beta}{\\textemdash}thatposterior\nmodeis, the most likely value for {\\beta}, given the data{\\textemdash}is given by the ridge\nregression solution. (In fact, the ridge regression solution is also the\nposterior mean.)\n Ifgis a double-exponential (Laplace) distribution with mean zero\nand scale parameter a function of {\\lambda}, then it follows that the posterior\nmode for {\\beta}is the lasso solution. (However, the lasso solution is not\nthe posterior mean, and in fact, the posterior mean does not yield a\nsparse coefficient vector.)\nThe Gaussian and double-exponential priors are displayed in Figure 6.11.\nTherefore, from a Bayesian viewpoint, ridge regression and the lasso follow\ndirectly from assuming the usual linear model with normal errors, together\nwith a simple prior distribution for {\\beta}. Notice that the lasso prior is steeply\npeaked at zero, while the Gaussian is flatter and fatter at zero. Hence, the\nlasso expects a priori that many of the coefficients are (exactly) zero, while\nridge assumes the coefficients are randomly distributed about zero.\n252 6. Linear Model Selection and Regularization\n5e{-}03 5e{-}02 5e{-}01 5e+0025.0 25.2 25.4 25.6Cross{-}Validation Error5e{-}03 5e{-}02 5e{-}01 5e+00{-}300 {-}100 0 100 300Standardized Coefficients{\\lambda}{\\lambda}FIGURE 6.12. Left:Cross-validation errors that result from applying ridge\nregression to the Creditdata set with various values of {\\lambda}.Right:The coefficient\nestimates as a function of {\\lambda}. The vertical dashed lines indicate the value of {\\lambda}\nselected by cross-validation.\n6.2.3 Selecting the Tuning Parameter\nJust as the subset selection approaches considered in Section 6.1require\na method to determine which of the models under consideration is best,\nimplementing ridge regression and the lasso requires a method for selecting\na value for the tuning parameter {\\lambda}in (6.5) and (",
                    "6.7), or equivalently, the\nvalue of the constraint sin (6.9) and (6.8). Cross-validation provides a sim-\nple way to tackle this problem. We choose a grid of {\\lambda}values, and compute\nthe cross-validation error for each value of {\\lambda}, as described in Chapter 5.W e\nthen select the tuning parameter value for which the cross-validation error\nis smallest. Finally, the model is re-fit using all of the available observations\nand the selected value of the tuning parameter.\nFigure6.12displays the choice of {\\lambda}that results from performing leave-\none-out cross-validation on the ridge regression fits from the Creditdata\nset. The dashed vertical lines indicate the selected value of {\\lambda}. In this case\nthe value is relatively small, indicating that the optimal fit only involves a\nsmallamountofshrinkagerelativetotheleastsquaressolution.Inaddition,\nthe dip is not very pronounced, so there is rather a wide range of values\nthat would give a very similar error. In a case like this we might simply use\nthe least squares solution.\nFigure6.13provides an illustration of ten-fold cross-validation applied to\nthe lasso fits on the sparse simulated data from Figure 6.9. The left-hand\npanelofFigure 6.13displaysthecross-validationerror,whiletheright-hand\npanel displays the coefficient estimates. The vertical dashed lines indicate\nthe point at which the cross-validation error is smallest. The two colored\nlines in the right-hand panel of Figure 6.13represent the two predictors\nthat are related to the response, while the grey lines represent the unre-\nlated predictors; these are often referred to as signalandnoisevariables,signalrespectively. Not only has the lasso correctly given much larger coeffi-\ncient estimates to the two signal predictors, but also the minimum cross-\nvalidation error corresponds to a set of coefficient estimates for which only\nthe signal variables are non-zero. Hence cross-validation together with the\nlasso has correctly identified the two signal variables in the model, even\nthough this is a challenging setting, with p= 45 variables and only n= 50"
                ]
            },
            {
                "2.2.2": [
                    "2.2 Assessing Model Accuracy 31\n0 20 40 60 80 1002 4 6 8 10 12XY\n2 5 10 200.0 0.5 1.0 1.5 2.0 2.5FlexibilityMean Squared Error\nFIGURE 2.10. Details are as in Figure 2.9, using a different true fthat is\nmuch closer to linear. In this setting, linear regression provides a very good fit to\nthe data.\npatterns that the method found in the training data simply don`t exist\nin the test data. Note that regardless of whether or not overfitting has\noccurred, we almost always expect the training MSE to be smaller than\nthe test MSE because most statistical learning methods either directly or\nindirectly seek to minimize the training MSE. Overfitting refers specifically\nto the case in which a less flexible model would have yielded a smaller\ntest MSE.\nFigure2.10provides another example in which the true fis approxi-\nmately linear. Again we observe that the training MSE decreases mono-\ntonically as the model flexibility increases, and that there is a U-shape in\nthe test MSE. However, because the truth is close to linear, the test MSE\nonly decreases slightly before increasing again, so that the orange least\nsquares fit is substantially better than the highly flexible green curve. Fi-\nnally, Figure 2.11displays an example in which fis highly non-linear. The\ntraining and test MSE curves still exhibit the same general patterns, but\nnow there is a rapid decrease in both curves before the test MSE starts to\nincrease slowly.\nIn practice, one can usually compute the training MSE with relative\nease, but estimating the test MSE is considerably more difficult because\nusuallynotestdata areavailable.Asthe previousthreeexamples illustrate,\nthe flexibility level corresponding to the model with the minimal test MSE\ncan vary considerably among data sets. Throughout this book, we discuss a\nvarietyofapproachesthatcanbeusedinpracticetoestimatethisminimum\npoint. One important method is cross-validation (Chapter 5), which is across-\nvalidation method for estimating the test MSE using the training data.\n2.2.2 The Bias-Variance Trade-Off\nThe U-shape observed in the test MSE curves (Figures 2.9{\\",
                    "textendash}2.11) turns out\nto be the result of two competing properties of statistical learning methods.\n32 2. Statistical Learning\n0 20 40 60 80 100{-}10 0 10 20XY\n2 5 10 200 5 10 15 20FlexibilityMean Squared Error\nFIGURE 2.11. Details are as in Figure 2.9, using a different fthat is far from\nlinear. In this setting, linear regression provides a very poor fit to the data.\nThough the mathematical proof is beyond the scope of this book, it is\npossible to show that the expected test MSE, for a given value x0, can\nalways be decomposed into the sum of three fundamental quantities: the\nvariance of{\\textasciicircum}f(x0), the squared biasof{\\textasciicircum}f(x0)and the variance of the errorvariance\nbiasterms{\\epsilon}. That is,\nE(\ny0{-}{\\textasciicircum}f(x0))2\n=Var({\\textasciicircum}f(x0)) + [ Bias({\\textasciicircum}f(x0))]2+Var({\\epsilon}). (2.7)\nHere the notation E(\ny0{-}{\\textasciicircum}f(x0))2\ndefines the expected test MSE atx0,expected\ntest MSE and refers to the average test MSE that we would obtain if we repeatedly\nestimated fusingalargenumberoftrainingsets,andtestedeachat x0.The\noverall expected test MSE can be computed by averaging E(\ny0{-}{\\textasciicircum}f(x0))2\nover all possible values of x0in the test set.\nEquation 2.7tells us that in order to minimize the expected test error,\nwe need to select a statistical learning method that simultaneously achieves\nlow variance andlow bias. Note that variance is inherently a nonnegative\nquantity, and squared bias is also nonnegative. Hence, we see that the\nexpected test MSE can never lie below Var ({\\epsilon}), the irreducible error from\n(2.3).\nWhat do we mean by the variance andbiasof a statistical learning\nmethod? Vari",
                    "ance refers to the amount by which {\\textasciicircum}fwould change if we\nestimated it using a different training data set. Since the training data\nare used to fit the statistical learning method, different training data sets\nwill result in a different {\\textasciicircum}f. But ideally the estimate for fshould not vary\ntoo much between training sets. However, if a method has high variance\nthen small changes in the training data can result in large changes in {\\textasciicircum}f. In\ngeneral,moreflexiblestatisticalmethodshavehighervariance.Considerthe\ngreen and orange curves in Figure 2.9. The flexible green curve is following\nthe observations very closely. It has high variance because changing any\none of these data points may cause the estimate {\\textasciicircum}fto change considerably.\n2.2 Assessing Model Accuracy 33\n2 5 10 200.0 0.5 1.0 1.5 2.0 2.5Flexibility2 5 10 200.0 0.5 1.0 1.5 2.0 2.5Flexibility2 5 10 2005 10 15 20\nFlexibilityMSEBiasVar\nFIGURE 2.12. Squared bias (blue curve), variance (orange curve), Var ({\\epsilon})\n(dashed line), and test MSE (red curve) for the three data sets in Figures 2.9{\\textendash}2.11.\nThe vertical dotted line indicates the flexibility level corresponding to the smallest\ntest MSE.\nIn contrast, the orange least squares line is relatively inflexible and has low\nvariance, because moving any single observation will likely cause only a\nsmall shift in the position of the line.\nOn the other hand, biasrefers to the error that is introduced by approxi-\nmatingareal-lifeproblem,whichmaybeextremelycomplicated,byamuch\nsimpler model. For example, linear regression assumes that there is a linear\nrelationship between YandX1,X2,...,X p. It is unlikely that any real-life\nproblem truly has such a simple linear relationship, and so performing lin-\near regression will undoubtedly result in some bias in the estimate of f. In\nFigure2.11, the true fis substantially non-linear, so no matter how many\ntraining observations we are given, it will not be possible",
                    " to produce an\naccurate estimate using linear regression. In other words, linear regression\nresults in high bias in this example. However, in Figure 2.10the true f\nis very close to linear, and so given enough data, it should be possible for\nlinear regression to produce an accurate estimate. Generally, more flexible\nmethods result in less bias.\nAs a general rule, as we use more flexible methods, the variance will\nincrease and the bias will decrease. The relative rate of change of these\ntwo quantities determines whether the test MSE increases or decreases. As\nwe increase the flexibility of a class of methods, the bias tends to initially\ndecrease faster than the variance increases. Consequently, the expected\ntest MSE declines. However, at some point increasing flexibility has little\nimpact on the bias but starts to significantly increase the variance. When\nthis happens the test MSE increases. Note that we observed this pattern\nof decreasing test MSE followed by increasing test MSE in the right-hand\npanels of Figures 2.9{\\textendash}2.11.\nThe three plots in Figure 2.12illustrate Equation 2.7for the examples in\nFigures2.9{\\textendash}2.11. In each case the blue solid curve represents the squared\nbias, for different levels of flexibility, while the orange curve corresponds to\nthe variance. The horizontal dashed line represents Var ({\\epsilon}), the irreducible\nerror. Finally, the red curve, corresponding to the test set MSE, is the sum\n34 2. Statistical Learning\nof these three quantities. In all three cases, the variance increases and the\nbias decreases as the method`s flexibility increases. However, the flexibility\nlevel corresponding to the optimal test MSE differs considerably among the\nthree data sets, because the squared bias and variance change at different\nrates in each of the data sets. In the left-hand panel of Figure 2.12, the\nbias initially decreases rapidly, resulting in an initial sharp decrease in the\nexpected test MSE. On the other hand, in the center panel of Figure 2.12\nthe truefis close to linear, so there is only a small decrease in bias as flex-\nibility increases, and the test MSE only declines slightly before increasing\nrapidly as the variance increases. Finally, in the right-hand panel of Fig-\nure2.12, as flexibility increases, there",
                    " is a dramatic decline in bias because\nthe true fis very non-linear. There is also very little increase in variance\nas flexibility increases. Consequently, the test MSE declines substantially\nbefore experiencing a small increase as model flexibility increases.\nThe relationship between bias, variance, and test set MSE given in Equa-\ntion2.7and displayed in Figure 2.12is referred to as the bias-variance\ntrade-off. Good test set performance of a statistical learning method re-bias-variance\ntrade-offquires low variance as well as low squared bias. This is referred to as a\ntrade-off because it is easy to obtain a method with extremely low bias but\nhigh variance (for instance, by drawing a curve that passes through every\nsingle training observation) or a method with very low variance but high\nbias (by fitting a horizontal line to the data). The challenge lies in finding\na method for which both the variance and the squared bias are low. This\ntrade-off is one of the most important recurring themes in this book.\nIn a real-life situation in which fis unobserved, it is generally not pos-\nsible to explicitly compute the test MSE, bias, or variance for a statistical\nlearning method. Nevertheless, one should always keep the bias-variance\ntrade-off in mind. In this book we explore methods that are extremely\nflexible and hence can essentially eliminate bias. However, this does not\nguarantee that they will outperform a much simpler method such as linear\nregression. To take an extreme example, suppose that the true fis linear.\nIn this situation linear regression will have no bias, making it very hard\nfor a more flexible method to compete. In contrast, if the true fis highly\nnon-linear and we have an ample number of training observations, then\nwe may do better using a highly flexible approach, as in Figure 2.11. In\nChapter 5we discuss cross-validation, which is a way to estimate the test\nMSE using the training data.\n2.2.3 The Classification Setting\nThus far, our discussion of model accuracy has been focused on the regres-\nsion setting. But many of the concepts that we have encountered, such\nas the bias-variance trade-off, transfer over to the classification setting\nwith only some modifications due to the fact that yiis no longer quan-\ntitative. Suppose that we",
                    " seek to estimate fon the basis of training obser-\nvations{\\{}(x1,y1),...,(xn,yn){\\}}, where now y1,...,y nare qualitative. The\nmost common approach for quantifying the accuracy of our estimate {\\textasciicircum}fis\nthetraining error rate ,theproportionofmistakesthataremadeifweapplyerror rate"
                ]
            },
            {
                "9": [
                    "9\nSupport Vector Machines\nIn this chapter, we discuss the support vector machine (SVM), an approach\nfor classification that was developed in the computer science community in\nthe 1990s and that has grown in popularity since then. SVMs have been\nshown to perform well in a variety of settings, and are often considered one\nof the best {\\textquotedblleft}out of the box{\\textquotedblright} classifiers.\nThe support vector machine is a generalization of a simple and intu-\nitive classifier called the maximal margin classifier , which we introduce in\nSection9.1. Though it is elegant and simple, we will see that this classifier\nunfortunately cannot be applied to most data sets, since it requires that\nthe classes be separable by a linear boundary. In Section 9.2, we introduce\nthesupport vector classifier , an extension of the maximal margin classifier\nthat can be applied in a broader range of cases. Section 9.3introduces the\nsupport vector machine , which is a further extension of the support vec-\ntor classifier in order to accommodate non-linear class boundaries. Support\nvector machines are intended for the binary classification setting in which\nthere are two classes; in Section 9.4we discuss extensions of support vector\nmachines to the case of more than two classes. In Section 9.5we discuss\nthecloseconnectionsbetweensupportvectormachinesandotherstatistical\nmethods such as logistic regression.\nPeople often loosely refer to the maximal margin classifier, the support\nvector classifier, and the support vector machine as {\\textquotedblleft}support vector\nmachines{\\textquotedblright}. To avoid confusion, we will carefully distinguish between these\nthree notions in this chapter.\n9.1 Maximal Margin Classifier\nIn this section, we define a hyperplane and introduce the concept of an\noptimal separating hyperplane.\n{\\textcopyright} Springer Nature Switzerland AG 2023 \nG. James et al., An Introduction to Statistical Learning , Springer Texts in Statistics, \nhttps://doi.org/10.1007/978-3-031-38747-0{\\_}9  367\n368 9. Support Vector Machines\n9.1.1 What Is a Hyperplane?\nIn ap-dimensional space, a hyperplane is a flat affine subspace ofhyperplanedimension p{-}1",
                    ".1For instance, in two dimensions, a hyperplane is a flat\none-dimensional subspace{\\textemdash}in other words, a line. In three dimensions, a\nhyperplane is a flat two-dimensional subspace{\\textemdash}that is, a plane. In p{>}3\ndimensions, it can be hard to visualize a hyperplane, but the notion of a\n(p{-}1)-dimensional flat subspace still applies.\nThe mathematical definition of a hyperplane is quite simple. In two di-\nmensions, a hyperplane is defined by the equation\n{\\beta}0+{\\beta}1X1+{\\beta}2X2=0 (9.1)\nfor parameters {\\beta}0,{\\beta}1, and{\\beta}2. When we say that ( 9.1) {\\textquotedblleft}defines{\\textquotedblright} the hyper-\nplane, we mean that any X=(X1,X2)Tfor which ( 9.1) holds is a point\non the hyperplane. Note that ( 9.1) is simply the equation of a line, since\nindeed in two dimensions a hyperplane is a line.\nEquation 9.1can be easily extended to the p-dimensional setting:\n{\\beta}0+{\\beta}1X1+{\\beta}2X2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pXp=0 (9.2)\ndefines a p-dimensional hyperplane, again in the sense that if a point X=\n(X1,X2,...,X p)Tinp-dimensional space (i.e. a vector of length p) satisfies\n(9.2), thenXlies on the hyperplane.\nNow, suppose that Xdoes not satisfy ( 9.2); rather,\n{\\beta}0+{\\beta}1X1+{\\beta}2X2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pXp{>}0. (9.3)\nThen this tells us that Xlies to one side of the hyperplane. On the other\nhand, if\n{\\beta}0+{\\beta}1X1+{\\beta}2X2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\",
                    "beta}pXp{<}0, (9.4)\nthenXlies on the other side of the hyperplane. So we can think of the\nhyperplane as dividing p-dimensional space into two halves. One can easily\ndetermineonwhichsideofthehyperplaneapointliesbysimplycalculating\nthe sign of the left-hand side of ( 9.2). A hyperplane in two-dimensional\nspace is shown in Figure 9.1.\n9.1.2 Classification Using a Separating Hyperplane\nNow suppose that we have an n{\\texttimes}pdata matrix Xthat consists of n\ntraining observations in p-dimensional space,\nx1=\nx11\n...\nx1p\n,...,x n=\nxn1\n...\nxnp\n, (9.5)\nand that these observations fall into two classes{\\textemdash}that is, y1,...,y n{\\in}\n{\\{}{-}1,1{\\}}where{-}1represents one class and 1the other class. We also have a\n1The word affineindicates that the subspace need not pass through the origin.\n9.1 Maximal Margin Classifier 369\n{-}1.5 {-}1.0 {-}0.5 0.0 0.5 1.0 1.5{-}1.5 {-}1.0 {-}0.5 0.0 0.5 1.0 1.5X1X2\nFIGURE 9.1. The hyperplane 1+2 X1+3X2=0is shown. The blue region is\nthe set of points for which 1+2 X1+3X2{>}0, and the purple region is the set of\npoints for which 1+2 X1+3X2{<}0.\ntest observation, a p-vector of observed features x{*}=(x{*}\n1... x{*}\np)T. Our\ngoal is to develop a classifier based on the training data that will correctly\nclassify the test observation using its feature measurements. We have seen\na number of approaches for this task, such as linear discriminant analysis\nand logistic regression in Chapter 4, and classification trees, bagging, and\nboosting in Chapter 8. We will now see a new approach that is based upon\nthe",
                    " concept of a separating hyperplane .separating\nhyperplaneSuppose that it is possible to construct a hyperplane that separates the\ntraining observations perfectly according to their class labels. Examples\nof three such separating hyperplanes are shown in the left-hand panel of\nFigure9.2. We can label the observations from the blue class as yi=1and\nthose from the purple class as yi={-}1. Then a separating hyperplane has\nthe property that\n{\\beta}0+{\\beta}1xi1+{\\beta}2xi2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip{>}0 ifyi=1, (9.6)\nand\n{\\beta}0+{\\beta}1xi1+{\\beta}2xi2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip{<}0 ifyi={-}1. (9.7)\nEquivalently, a separating hyperplane has the property that\nyi({\\beta}0+{\\beta}1xi1+{\\beta}2xi2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip){>}0 (9.8)\nfor alli=1,...,n .\nIfaseparatinghyperplaneexists,wecanuseittoconstructaverynatural\nclassifier: a test observation is assigned a class depending on which side of\nthe hyperplane it is located. The right-hand panel of Figure 9.2shows\nan example of such a classifier. That is, we classify the test observation x{*}\nbasedonthesignof f(x{*})={\\beta}0+{\\beta}1x{*}\n1+{\\beta}2x{*}\n2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}px{*}\np.Iff(x{*})ispositive,\nthen we assign the test observation to class 1, and iff(x{*})is negative, then\nwe assign it to class {-}1. We can also make use of the magnitude off(x{*}). If\n370 9. Support Vector Machines\n{-}1 0 1 2 3{-}1 0 1 2 3{-}1 0 1 2 3",
                    "{-}1 0 1 2 3X1X1\nX2X2\nFIGURE 9.2. Left:There are two classes of observations, shown in blue and\nin purple, each of which has measurements on two variables. Three separating\nhyperplanes, out of many possible, are shown in black. Right:A separating hy-\nperplane is shown in black. The blue and purple grid indicates the decision rule\nmade by a classifier based on this separating hyperplane: a test observation that\nfalls in the blue portion of the grid will be assigned to the blue class, and a test\nobservation that falls into the purple portion of the grid will be assigned to the\npurple class.\nf(x{*})is far from zero, then this means that x{*}lies far from the hyperplane,\nand so we can be confident about our class assignment for x{*}. On the other\nhand,iff(x{*})isclosetozero,then x{*}islocatednearthehyperplane,andso\nwe are less certain about the class assignment for x{*}. Not surprisingly, and\nas we see in Figure 9.2, a classifier that is based on a separating hyperplane\nleads to a linear decision boundary.\n9.1.3 The Maximal Margin Classifier\nIn general, if our data can be perfectly separated using a hyperplane, then\nthere will in fact exist an infinite number of such hyperplanes. This is\nbecauseagivenseparatinghyperplanecanusuallybeshiftedatinybitupor\ndown,orrotated,withoutcomingintocontactwithanyoftheobservations.\nThree possible separating hyperplanes are shown in the left-hand panel\nof Figure 9.2. In order to construct a classifier based upon a separating\nhyperplane, we must have a reasonable way to decide which of the infinite\npossible separating hyperplanes to use.\nA natural choice is the maximal margin hyperplane (also known as themaximal\nmargin\nhyperplaneoptimal separating hyperplane ), which is the separating hyperplane that\noptimal\nseparating\nhyperplaneis farthest from the training observations. That is, we can compute the\n(perpendicular) distance from each training observation to a given separat-\ning hyperplane; the smallest such distance is the minimal distance from the\nobservations to the hyperplane, and is known as",
                    " the margin. The maximal\nmargin margin hyperplane is the separating hyperplane for which the margin is\nlargest{\\textemdash}that is, it is the hyperplane that has the farthest minimum dis-\ntance to the training observations. We can then classify a test observation\nbasedonwhichsideofthemaximalmarginhyperplaneitlies.Thisisknown\n9.1 Maximal Margin Classifier 371\n{-}10123{-}10123\nX1X2\nFIGURE 9.3. There are two classes of observations, shown in blue and in\npurple. The maximal margin hyperplane is shown as a solid line. The margin\nis the distance from the solid line to either of the dashed lines. The two blue\npoints and the purple point that lie on the dashed lines are the support vectors,\nand the distance from those points to the hyperplane is indicated by arrows. The\npurple and blue grid indicates the decision rule made by a classifier based on this\nseparating hyperplane.\nas themaximal margin classifier . We hope that a classifier that has a largemaximal\nmargin\nclassifiermargin on the training data will also have a large margin on the test data,\nand hence will classify the test observations correctly. Although the maxi-\nmal margin classifier is often successful, it can also lead to overfitting when\npis large.\nIf{\\beta}0,{\\beta}1,...,{\\beta}pare the coefficients of the maximal margin hyperplane,\nthen the maximal margin classifier classifies the test observation x{*}based\non the sign of f(x{*})={\\beta}0+{\\beta}1x{*}\n1+{\\beta}2x{*}\n2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}px{*}\np.\nFigure9.3shows the maximal margin hyperplane on the data set of\nFigure9.2. Comparing the right-hand panel of Figure 9.2to Figure 9.3,\nwe see that the maximal margin hyperplane shown in Figure 9.3does in-\ndeed result in a greater minimal distance between the observations and the\nseparating hyperplane{\\textemdash}that is, a larger margin. In a sense, the maximal\nmargin hyperplane represents the mid-line of the widest {\\textquotedblleft}slab",
                    "{\\textquotedblright} that we can\ninsert between the two classes.\nExaminingFigure 9.3,weseethatthreetrainingobservationsareequidis-\ntant from the maximal margin hyperplane and lie along the dashed lines\nindicating the width of the margin. These three observations are known as\nsupport vectors ,sincetheyarevectorsin p-dimensionalspace(inFigure 9.3,support\nvector p=2) and they {\\textquotedblleft}support{\\textquotedblright} the maximal margin hyperplane in the sense\nthat if these points were moved slightly then the maximal margin hyper-\nplane would move as well. Interestingly, the maximal margin hyperplane\ndepends directly on the support vectors, but not on the other observations:\namovementtoanyoftheotherobservationswouldnotaffecttheseparating\nhyperplane, provided that the observation`s movement does not cause it to\n372 9. Support Vector Machines\ncross the boundary set by the margin. The fact that the maximal margin\nhyperplane depends directly on only a small subset of the observations is\nan important property that will arise later in this chapter when we discuss\nthe support vector classifier and support vector machines.\n9.1.4 Construction of the Maximal Margin Classifier\nWe now consider the task of constructing the maximal margin hyperplane\nbased on a set of ntraining observations x1,...,x n{\\in}Rpand associated\nclass labels y1,...,y n{\\in}{\\{}{-}1,1{\\}}. Briefly, the maximal margin hyperplane\nis the solution to the optimization problem\nmaximize\n{\\beta}0,{\\beta}1,...,{\\beta}p,MM (9.9)\nsubject top{\\sum}\nj=1{\\beta}2\nj=1, (9.10)\nyi({\\beta}0+{\\beta}1xi1+{\\beta}2xi2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip){\\geq}M{\\forall}i=1, . . . , n. (9.11)\nThis optimization problem ( 9.9){\\textendash}(9.11) is actually simpler than it looks.\nFirst of all, the constraint in ( 9.11) that\nyi({\\beta}0",
                    "+{\\beta}1xi1+{\\beta}2xi2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip){\\geq}M{\\forall}i=1,...,n\nguarantees that each observation will be on the correct side of the hyper-\nplane, provided that Mis positive. (Actually, for each observation to be\non the correct side of the hyperplane we would simply need yi({\\beta}0+{\\beta}1xi1+\n{\\beta}2xi2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip){>}0, so the constraint in ( 9.11) in fact requires that each\nobservation be on the correct side of the hyperplane, with some cushion,\nprovided that Mis positive.)\nSecond,notethat( 9.10)isnotreallyaconstraintonthehyperplane,since\nif{\\beta}0+{\\beta}1xi1+{\\beta}2xi2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip=0defines a hyperplane, then so does\nk({\\beta}0+{\\beta}1xi1+{\\beta}2xi2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip)=0 for anyk=0. However, ( 9.10) adds\nmeaningto( 9.11);onecanshowthatwiththisconstrainttheperpendicular\ndistance from the ith observation to the hyperplane is given by\nyi({\\beta}0+{\\beta}1xi1+{\\beta}2xi2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip).\nTherefore, the constraints ( 9.10) and (9.11) ensure that each observation\nis on the correct side of the hyperplane and at least a distance Mfrom the\nhyperplane. Hence, Mrepresents the margin of our hyperplane, and the\noptimization problem chooses {\\beta}0,{\\beta}1,...,{\\beta}pto maximize M. This is exactly\nthe definition of the maximal margin hyperplane! The problem ( 9.9){\\textendash}(9.11)\ncan be solved efficiently, but details of this",
                    " optimization are outside of the\nscope of this book.\n9.1.5 The Non-separable Case\nThe maximal margin classifier is a very natural way to perform classifi-\ncation,if a separating hyperplane exists . However, as we have hinted, in\nmany cases no separating hyperplane exists, and so there is no maximal\n9.2 Support Vector Classifiers 373\n0123{-}1.0 {-}0.5 0.0 0.5 1.0 1.5 2.0X1X2\nFIGURE 9.4. There are two classes of observations, shown in blue and in\npurple. In this case, the two classes are not separable by a hyperplane, and so the\nmaximal margin classifier cannot be used.\nmargin classifier. In this case, the optimization problem ( 9.9){\\textendash}(9.11) has no\nsolution with M{>} 0. An example is shown in Figure 9.4. In this case, we\ncannotexactlyseparate the two classes. However, as we will see in the next\nsection, we can extend the concept of a separating hyperplane in order to\ndevelop a hyperplane that almostseparates the classes, using a so-called\nsoft margin . The generalization of the maximal margin classifier to the\nnon-separable case is known as the support vector classifier .\n9.2 Support Vector Classifiers\n9.2.1 Overview of the Support Vector Classifier\nIn Figure 9.4, we see that observations that belong to two classes are not\nnecessarily separable by a hyperplane. In fact, even if a separating hyper-\nplane does exist, then there are instances in which a classifier based on\na separating hyperplane might not be desirable. A classifier based on a\nseparating hyperplane will necessarily perfectly classify all of the training\nobservations; this can lead to sensitivity to individual observations. An ex-\nample is shown in Figure 9.5. The addition of a single observation in the\nright-hand panel of Figure 9.5leads to a dramatic change in the maxi-\nmal margin hyperplane. The resulting maximal margin hyperplane is not\nsatisfactory{\\textemdash}for one thing, it has only a tiny margin. This is problematic\nbecause as discussed previously, the distance of an observation from the\nhyperplane can be seen as a",
                    " measure of our confidence that the obser-\nvation was correctly classified. Moreover, the fact that the maximal mar-\ngin hyperplane is extremely sensitive to a change in a single observation\nsuggests that it may have overfit the training data.\nIn this case, we might be willing to consider a classifier based on a hy-\nperplane that does notperfectly separate the two classes, in the interest of\n374 9. Support Vector Machines\n{-}1 0 1 2 3{-}1 0 1 2 3{-}1 0 1 2 3{-}1 0 1 2 3X1X1X2X2\nFIGURE 9.5. Left:Two classes of observations are shown in blue and in\npurple, along with the maximal margin hyperplane. Right:An additional blue\nobservation has been added, leading to a dramatic shift in the maximal margin\nhyperplane shown as a solid line. The dashed line indicates the maximal margin\nhyperplane that was obtained in the absence of this additional point.\n Greater robustness to individual observations, and\n Better classification of mostof the training observations.\nThat is, it could be worthwhile to misclassify a few training observations\nin order to do a better job in classifying the remaining observations.\nThesupport vector classifier , sometimes called a soft margin classifier ,support\nvector\nclassifier\nsoft margin\nclassifierdoes exactly this. Rather than seeking the largest possible margin so that\nevery observation is not only on the correct side of the hyperplane but\nalso on the correct side of the margin, we instead allow some observations\nto be on the incorrect side of the margin, or even the incorrect side of\nthe hyperplane. (The margin is softbecause it can be violated by some\nof the training observations.) An example is shown in the left-hand panel\nofFigure 9.6.Mostoftheobservationsareonthecorrectsideofthemargin.\nHowever, a small subset of the observations are on the wrong side of the\nmargin.\nAn observation can be not only on the wrong side of the margin, but also\non the wrong side of the hyperplane. In fact, when there is no separating\nhyperplane,suchasituationisinevitable.Observationsonthewrongsideof\nthehyperplanecorrespondtotrainingobservationsthataremisclassifiedby\nthe support vector classifier. The right-hand panel of Figure 9.6",
                    "illustrates\nsuch a scenario.\n9.2.2 Details of the Support Vector Classifier\nThe support vector classifier classifies a test observation depending on\nwhich side of a hyperplane it lies. The hyperplane is chosen to correctly\nseparate most of the training observations into the two classes, but may\n9.2 Support Vector Classifiers 375\n{-}0.50.0 0.5 1.0 1.5 2.0 2.5{-}1 0 1 2 3 412345678910\n{-}0.50.0 0.5 1.0 1.5 2.0 2.5{-}1 0 1 2 3 4123456789101112X1X1\nX2X2\nFIGURE 9.6. Left:A support vector classifier was fit to a small data set. The\nhyperplane is shown as a solid line and the margins are shown as dashed lines.\nPurple observations: Observations 3,4,5, and6are on the correct side of the\nmargin, observation 2is on the margin, and observation 1 is on the wrong side of\nthe margin. Blue observations: Observations 7and10are on the correct side of\nthe margin, observation 9is on the margin, and observation 8is on the wrong side\nof the margin. No observations are on the wrong side of the hyperplane. Right:\nSame as left panel with two additional points, 11and12. These two observations\nare on the wrong side of the hyperplane and the wrong side of the margin.\nmisclassifyafewobservations.Itisthesolutiontotheoptimizationproblem\nmaximize\n{\\beta}0,{\\beta}1,...,{\\beta}p,{\\epsilon}1,...,{\\epsilon}n,MM (9.12)\nsubject top{\\sum}\nj=1{\\beta}2\nj=1, (9.13)\nyi({\\beta}0+{\\beta}1xi1+{\\beta}2xi2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip){\\geq}M(1{-}{\\epsilon}i),(9.14)\n{\\epsilon}i{\\geq}0,n{\\sum}\ni=1{\\epsilon}i{\\leq}C, (9.15",
                    ")\nwhereCis a nonnegative tuning parameter. As in ( 9.11),Mis the width\nof the margin; we seek to make this quantity as large as possible. In ( 9.14),\n{\\epsilon}1,...,{\\epsilon}nareslack variables that allow individual observations to be onslack\nvariablethe wrong side of the margin or the hyperplane; we will explain them in\ngreater detail momentarily. Once we have solved ( 9.12){\\textendash}(9.15), we classify\na test observation x{*}as before, by simply determining on which side of the\nhyperplane it lies. That is, we classify the test observation based on the\nsign off(x{*})={\\beta}0+{\\beta}1x{*}\n1+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}px{*}\np.\nThe problem ( 9.12){\\textendash}(9.15) seems complex, but insight into its behavior\ncan be made through a series of simple observations presented below. First\nof all, the slack variable {\\epsilon}itells us where the ith observation is located,\nrelative to the hyperplane and relative to the margin. If {\\epsilon}i=0then the ith\nobservation is on the correct side of the margin, as we saw in Section 9.1.4.\nIf{\\epsilon}i{>}0then the ith observation is on the wrong side of the margin, and\nwe say that the ith observation has violatedthe margin. If {\\epsilon}i{>}1then it is\non the wrong side of the hyperplane.\n376 9. Support Vector Machines\nWe now consider the role of the tuning parameter C. In (9.15),Cbounds\nthe sum of the {\\epsilon}i`s, and so it determines the number and severity of the vio-\nlations to the margin (and to the hyperplane) that we will tolerate. We can\nthink of Cas abudgetfor the amount that the margin can be violated\nby thenobservations. If C=0 then there is no budget for violations to\nthe margin, and it must be the case that {\\epsilon}1={\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}",
                    "={\\epsilon}n=0, in which case\n(9.12){\\textendash}(9.15) simply amounts to the maximal margin hyperplane optimiza-\ntion problem ( 9.9){\\textendash}(9.11). (Of course, a maximal margin hyperplane exists\nonly if the two classes are separable.) For C{>}0no more than Cobserva-\ntions can be on the wrong side of the hyperplane, because if an observation\nis on the wrong side of the hyperplane then {\\epsilon}i{>}1, and (9.15) requires\nthat{\\sum}n\ni=1{\\epsilon}i{\\leq}C. As the budget Cincreases, we become more tolerant of\nviolations to the margin, and so the margin will widen. Conversely, as C\ndecreases, we become less tolerant of violations to the margin and so the\nmargin narrows. An example is shown in Figure 9.7.\nInpractice, Cistreatedasatuningparameterthatisgenerallychosenvia\ncross-validation. As with the tuning parameters that we have seen through-\nout this book, Ccontrols the bias-variance trade-off of the statistical learn-\ning technique. When Cis small, we seek narrow margins that are rarely\nviolated; this amounts to a classifier that is highly fit to the data, which\nmay have low bias but high variance. On the other hand, when Cis larger,\nthe margin is wider and we allow more violations to it; this amounts to\nfitting the data less hard and obtaining a classifier that is potentially more\nbiased but may have lower variance.\nThe optimization problem ( 9.12){\\textendash}(9.15) has a very interesting property:\nit turns out that only observations that either lie on the margin or that\nviolate the margin will affect the hyperplane, and hence the classifier ob-\ntained. In other words, an observation that lies strictly on the correct side\nof the margin does not affect the support vector classifier! Changing the\nposition of that observation would not change the classifier at all, provided\nthat its position remains on the correct side of the margin. Observations\nthat lie directly on the margin, or on the wrong side of the margin for\ntheir class, are known as support vectors . These observations do affect",
                    " the\nsupport vector classifier.\nThe fact that only support vectors affect the classifier is in line with our\nprevious assertion that Ccontrols the bias-variance trade-off of the support\nvector classifier. When the tuning parameter Cis large, then the margin is\nwide, many observations violate the margin, and so there are many support\nvectors. In this case, many observations are involved in determining the\nhyperplane. The top left panel in Figure 9.7illustrates this setting: this\nclassifier has low variance (since many observations are support vectors)\nbut potentially high bias. In contrast, if Cis small, then there will be fewer\nsupport vectors and hence the resulting classifier will have low bias but\nhigh variance. The bottom right panel in Figure 9.7illustrates this setting,\nwith only eight support vectors.\nThe fact that the support vector classifier`s decision rule is based only\non a potentially small subset of the training observations (the support vec-\ntors) means that it is quite robust to the behavior of observations that\nare far away from the hyperplane. This property is distinct from some of\n9.3 Support Vector Machines 377\n{-}1012{-}3 {-}2 {-}1 0 1 2 3{-}1012{-}3 {-}2 {-}1 0 1 2 3\n{-}1012{-}3 {-}2 {-}1 0 1 2 3{-}1012{-}3 {-}2 {-}1 0 1 2 3X1X1X1X1X2X2\nX2X2\nFIGURE 9.7. A support vector classifier was fit using four different values\nof the tuning parameter Cin (9.12){\\textendash}(9.15). The largest value of Cwas used\nin the top left panel, and smaller values were used in the top right, bottom left,\nand bottom right panels. When Cis large, then there is a high tolerance for\nobservations being on the wrong side of the margin, and so the margin will be\nlarge. As Cdecreases, the tolerance for observations being on the wrong side of\nthe margin decreases, and the margin narrows.\nthe other classification methods that we have seen in preceding chapters,\nsuch as linear discriminant analysis. Recall that the LDA classification rule\ndepends",
                    " on the mean of allof the observations within each class, as well as\nthe within-class covariance matrix computed using allof the observations.\nIn contrast, logistic regression, unlike LDA, has very low sensitivity to ob-\nservations far from the decision boundary. In fact we will see in Section 9.5\nthat the support vector classifier and logistic regression are closely related.\n9.3 Support Vector Machines\nWe first discuss a general mechanism for converting a linear classifier into\none that produces non-linear decision boundaries. We then introduce the\nsupport vector machine, which does this in an automatic way.\n378 9. Support Vector Machines\n{-}4 {-}2 0 2 4{-}4 {-}2 0 2 4{-}4 {-}2 0 2 4{-}4 {-}2 0 2 4X1X1X2X2\nFIGURE 9.8. Left:The observations fall into two classes, with a non-lin-\near boundary between them. Right:The support vector classifier seeks a linear\nboundary, and consequently performs very poorly.\n9.3.1 Classification with Non-Linear Decision Boundaries\nThe support vector classifier is a natural approach for classification in the\ntwo-class setting, if the boundary between the two classes is linear. How-\never, in practice we are sometimes faced with non-linear class boundaries.\nFor instance, consider the data in the left-hand panel of Figure 9.8. It is\nclear that a support vector classifier or any linear classifier will perform\npoorly here. Indeed, the support vector classifier shown in the right-hand\npanel of Figure 9.8is useless here.\nIn Chapter 7, we are faced with an analogous situation. We see there\nthat the performance of linear regression can suffer when there is a non-\nlinear relationship between the predictors and the outcome. In that case,\nwe consider enlarging the feature space using functions of the predictors,\nsuch as quadratic and cubic terms, in order to address this non-linearity.\nIn the case of the support vector classifier, we could address the prob-\nlem of possibly non-linear boundaries between classes in a similar way, by\nenlarging the feature space using quadratic, cubic, and even higher-order\npolynomial functions of the predictors. For instance, rather than fitting a\nsupport vector class",
                    "ifier using pfeatures\nX1,X2,...,X p,\nwe could instead fit a support vector classifier using 2pfeatures\nX1,X2\n1,X2,X2\n2,...,X p,X2\np.\n9.3 Support Vector Machines 379\nThen (9.12){\\textendash}(9.15) would become\nmaximize\n{\\beta}0,{\\beta}11,{\\beta}12,...,{\\beta}p1,{\\beta}p2,{\\epsilon}1,...,{\\epsilon}n,MM (9.16)\nsubject to yi\n{\\beta}0+p{\\sum}\nj=1{\\beta}j1xij+p{\\sum}\nj=1{\\beta}j2x2\nij\n{\\geq}M(1{-}{\\epsilon}i),\nn{\\sum}\ni=1{\\epsilon}i{\\leq}C,{\\epsilon}i{\\geq}0,p{\\sum}\nj=12{\\sum}\nk=1{\\beta}2\njk=1.\nWhy does this lead to a non-linear decision boundary? In the enlarged\nfeature space, the decision boundary that results from ( 9.16) is in fact lin-\near. But in the original feature space, the decision boundary is of the form\nq(x)=0 , whereqis a quadratic polynomial, and its solutions are gener-\nally non-linear. One might additionally want to enlarge the feature space\nwith higher-order polynomial terms, or with interaction terms of the form\nXjXj{'}forj=j{'}. Alternatively, other functions of the predictors could\nbe considered rather than polynomials. It is not hard to see that there\nare many possible ways to enlarge the feature space, and that unless we\nare careful, we could end up with a huge number of features. Then compu-\ntations would become unmanageable. The support vector machine, which\nwe present next, allows us to enlarge the feature space used by the support\nvector classifier in a way that leads to efficient computations.\n9.3.2 The Support Vector Machine\nThesupport vector machine (SVM) is an extension of the support vectorsupport\nvector\nmachineclassifier that results",
                    " from enlarging the feature space in a specific way,\nusingkernels. We will now discuss this extension, the details of which are\nkernel somewhat complex and beyond the scope of this book. However, the main\nidea is described in Section 9.3.1: we may want to enlarge our feature space\nin order to accommodate a non-linear boundary between the classes. The\nkernel approach that we describe here is simply an efficient computational\napproach for enacting this idea.\nWe have not discussed exactly how the support vector classifier is com-\nputed because the details become somewhat technical. However, it turns\nout that the solution to the support vector classifier problem ( 9.12){\\textendash}(9.15)\ninvolves only the inner products of the observations (as opposed to the\nobservations themselves). The inner product of two r-vectors aandbis\ndefined as {\\langle}a, b{\\rangle}={\\sum}r\ni=1aibi. Thus the inner product of two observations\nxi,xi{'}is given by\n{\\langle}xi,xi{'}{\\rangle}=p{\\sum}\nj=1xijxi{'}j. (9.17)\nIt can be shown that\n The linear support vector classifier can be represented as\nf(x)={\\beta}0+n{\\sum}\ni=1{\\alpha}i{\\langle}x, x i{\\rangle}, (9.18)\n380 9. Support Vector Machines\nwhere there are nparameters {\\alpha}i,i=1,...,n , one per training\nobservation.\n To estimate the parameters {\\alpha}1,...,{\\alpha}nand{\\beta}0, all we need are the(n\n2)\ninner products {\\langle}xi,xi{'}{\\rangle}between all pairs of training observations.\n(The notation(n\n2)\nmeansn(n{-}1)/2, and gives the number of pairs\namong a set of nitems.)\nNotice that in ( 9.18), in order to evaluate the function f(x), we need to\ncomputetheinnerproductbetweenthenewpoint xandeachofthetraining\npointsxi. However, it turns out that {\\alpha}iis nonzero only for the support\nvectors in the solution{\\textemdash",
                    "}that is, if a training observation is not a support\nvector, then its {\\alpha}iequals zero. So if Sis the collection of indices of these\nsupport points, we can rewrite any solution function of the form ( 9.18) as\nf(x)={\\beta}0+{\\sum}\ni{\\in}S{\\alpha}i{\\langle}x, x i{\\rangle}, (9.19)\nwhich typically involves far fewer terms than in ( 9.18).2\nTosummarize,inrepresentingthelinearclassifier f(x),andincomputing\nits coefficients, all we need are inner products.\nNow suppose that every time the inner product ( 9.17) appears in the\nrepresentation ( 9.18), or in a calculation of the solution for the support\nvector classifier, we replace it with a generalization of the inner product of\nthe form\nK(xi,xi{'}), (9.20)\nwhereKis some function that we will refer to as a kernel. A kernel is akernelfunction that quantifies the similarity of two observations. For instance, we\ncould simply take\nK(xi,xi{'})=p{\\sum}\nj=1xijxi{'}j, (9.21)\nwhich would just give us back the support vector classifier. Equation 9.21\nis known as a linearkernel because the support vector classifier is linear\nin the features; the linear kernel essentially quantifies the similarity of a\npair of observations using Pearson (standard) correlation. But one could\ninstead choose another form for ( 9.20). For instance, one could replace\nevery instance of{\\sum}p\nj=1xijxi{'}jwith the quantity\nK(xi,xi{'}) = (1 +p{\\sum}\nj=1xijxi{'}j)d. (9.22)\nThis is known as a polynomial kernel of degree d, wheredis a positivepolynomial\nkernelinteger. Using such a kernel with d{>} 1, instead of the standard linear\nkernel(9.21),inthesupportvectorclassifieralgorithmleadstoamuchmore\nflexibledecision boundary.Itessentiallyamountstofitting a support vector\n2By expanding each of the inner products in ( 9.19), it is easy to see",
                    " that f(x)is\na linear function of the coordinates of x. Doing so also establishes the correspondence\nbetween the {\\alpha}iand the original parameters {\\beta}j.\n9.3 Support Vector Machines 381\n{-}4 {-}2 0 2 4{-}4 {-}2 0 2 4      \n      \n{-}4 {-}2 0 2 4{-}4 {-}2 0 2 4    \n    \nX1X1X2X2\nFIGURE 9.9. Left:An SVM with a polynomial kernel of degree 3 is applied to\nthe non-linear data from Figure 9.8, resulting in a far more appropriate decision\nrule.Right:An SVM with a radial kernel is applied. In this example, either kernel\nis capable of capturing the decision boundary.\nclassifier in a higher-dimensional space involving polynomials of degree d,\nrather than in the original feature space. When the support vector classifier\niscombinedwithanon-linearkernelsuchas( 9.22),theresultingclassifieris\nknown as a support vector machine. Note that in this case the (non-linear)\nfunction has the form\nf(x)={\\beta}0+{\\sum}\ni{\\in}S{\\alpha}iK(x, x i). (9.23)\nThe left-hand panel of Figure 9.9shows an example of an SVM with a\npolynomial kernel applied to the non-linear data from Figure 9.8. The fit is\na substantial improvement over the linear support vector classifier. When\nd=1, then the SVM reduces to the support vector classifier seen earlier in\nthis chapter.\nThe polynomial kernel shown in ( 9.22) is one example of a possible\nnon-linear kernel, but alternatives abound. Another popular choice is the\nradial kernel , which takes the formradial kernel\nK(xi,xi{'}) = exp( {-}{\\gamma}p{\\sum}\nj=1(xij{-}xi{'}j)2). (9.24)\nIn (9.24),{\\gamma}is a positive constant. The right-hand panel of Figure 9.9shows\nan example of an SVM with a radial kernel on this",
                    " non-linear data; it also\ndoes a good job in separating the two classes.\nHow does the radial kernel ( 9.24) actually work? If a given test obser-\nvationx{*}=(x{*}\n1,...,x{*}\np)Tis far from a training observation xiin terms of\nEuclidean distance, then{\\sum}p\nj=1(x{*}\nj{-}xij)2will be large, and so K(x{*},xi)=\nexp({-}{\\gamma}{\\sum}p\nj=1(x{*}\nj{-}xij)2)will be tiny. This means that in ( 9.23),xiwill\nplay virtually no role in f(x{*}). Recall that the predicted class label for the\ntest observation x{*}is based on the sign of f(x{*}). In other words, training\nobservations that are far from x{*}will play essentially no role in the pre-\ndicted class label for x{*}. This means that the radial kernel has very local\n382 9. Support Vector Machines\nFalse positive rateTrue positive rate0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Support Vector ClassifierLDAFalse positive rateTrue positive rate0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Support Vector ClassifierSVM: {\\gamma}=10{-}3SVM: {\\gamma}=10{-}2SVM: {\\gamma}=10{-}1FIGURE 9.10. ROC curves for the Heartdata training set. Left:The support\nvector classifier and LDA are compared. Right:The support vector classifier is\ncompared to an SVM using a radial basis kernel with {\\gamma}= 10{-}3,10{-}2, and10{-}1.\nbehavior, in the sense that only nearby training observations have an effect\non the class label of a test observation.\nWhat is the advantage of using a kernel rather than simply enlarging\nthe feature space using functions of the original features, as in ( 9.16)? One\n",
                    "advantage is computational, and it amounts to the fact that using kernels,\none need only compute K(xi,x{'}\ni)for all(n\n2)\ndistinct pairs i, i{'}. This can be\ndone without explicitly working in the enlarged feature space. This is im-\nportant because in many applications of SVMs, the enlarged feature space\nis so large that computations are intractable. For some kernels, such as the\nradial kernel ( 9.24), the feature space is implicitand infinite-dimensional,\nso we could never do the computations there anyway!\n9.3.3 An Application to the Heart Disease Data\nInChapter 8weapplydecisiontreesandrelatedmethodstothe Heartdata.\nTheaim isto use 13predictorssuchas Age,Sex,andCholinorderto predict\nwhether an individual has heart disease. We now investigate how an SVM\ncompares to LDA on this data. After removing 6 missing observations, the\ndata consist of 297 subjects, which we randomly split into 207 training and\n90 test observations.\nWe first fit LDA and the support vector classifier to the training data.\nNotethatthesupportvectorclassifierisequivalenttoanSVMusingapoly-\nnomial kernel of degree d=1. The left-hand panel of Figure 9.10displays\nROC curves (described in Section 4.4.2) for the training set predictions for\nboth LDA and the support vector classifier. Both classifiers compute scores\nof the form {\\textasciicircum}f(X)={\\textasciicircum}{\\beta}0+{\\textasciicircum}{\\beta}1X1+{\\textasciicircum}{\\beta}2X2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\textasciicircum}{\\beta}pXpfor each observation.\nFor any given cutoff t, we classify observations into the heart disease or\nno heart disease categories depending on whether {\\textasciicircum}f(X){<}tor{\\textasciicircum}f(X){\\geq}t.\nThe ROC curve is obtained by forming these predictions and computing\nthe false positive and true positive rates for a range of values of t. An opti-\nmal classifier will hug the",
                    " top left corner of the ROC plot. In this instance\n9.4 SVMs with More than Two Classes 383\nFalse positive rateTrue positive rate0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Support Vector ClassifierLDAFalse positive rateTrue positive rate0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0Support Vector ClassifierSVM: {\\gamma}=10{-}3SVM: {\\gamma}=10{-}2SVM: {\\gamma}=10{-}1FIGURE 9.11. ROC curves for the test set of the Heartdata.Left:The support\nvector classifier and LDA are compared. Right:The support vector classifier is\ncompared to an SVM using a radial basis kernel with {\\gamma}= 10{-}3,10{-}2, and10{-}1.\nLDA and the support vector classifier both perform well, though there is a\nsuggestion that the support vector classifier may be slightly superior.\nThe right-hand panel of Figure 9.10displays ROC curves for SVMs using\na radial kernel, with various values of {\\gamma}. As{\\gamma}increases and the fit becomes\nmore non-linear, the ROC curves improve. Using {\\gamma}= 10{-}1appears to give\nan almost perfect ROC curve. However, these curves represent training\nerror rates, which can be misleading in terms of performance on new test\ndata. Figure 9.11displays ROC curves computed on the 90test observa-\ntions. We observe some differences from the training ROC curves. In the\nleft-hand panel of Figure 9.11, the support vector classifier appears to have\na small advantage over LDA (although these differences are not statisti-\ncally significant). In the right-hand panel, the SVM using {\\gamma}= 10{-}1, which\nshowed the best results on the training data, produces the worst estimates\non the test data. This is once again evidence that while a more flexible\nmethod will often produce lower training error rates, this does not neces-\nsarily lead to improved performance on test data.",
                    " The SVMs with {\\gamma}= 10{-}2\nand{\\gamma}= 10{-}3perform comparably to the support vector classifier, and all\nthree outperform the SVM with {\\gamma}= 10{-}1.\n9.4 SVMs with More than Two Classes\nSo far, our discussion has been limited to the case of binary classification:\nthat is, classification in the two-class setting. How can we extend SVMs\nto the more general case where we have some arbitrary number of classes?\nIt turns out that the concept of separating hyperplanes upon which SVMs\nare based does not lend itself naturally to more than two classes. Though\na number of proposals for extending SVMs to the K-class case have been\nmade, the two most popular are the one-versus-one andone-versus-all\napproaches. We briefly discuss those two approaches here.\n384 9. Support Vector Machines\n9.4.1 One-Versus-One Classification\nSuppose that we would like to perform classification using SVMs, and there\nareK{>} 2classes. A one-versus-one orall-pairs approach constructs(K\n2)\none-versus-\none SVMs, each of which compares a pair of classes. For example, one such\nSVM might compare the kth class, coded as +1, to thek{'}th class, coded\nas{-}1. We classify a test observation using each of the(K\n2)\nclassifiers, and\nwe tally the number of times that the test observation is assigned to each\nof theKclasses. The final classification is performed by assigning the test\nobservation to the class to which it was most frequently assigned in these(K\n2)\npairwise classifications.\n9.4.2 One-Versus-All Classification\nTheone-versus-all approach (also referred to as one-versus-rest ) is an al-one-versus-\nall\none-versus-\nrestternative procedure for applying SVMs in the case of K{>} 2classes. We\nfitKSVMs, each time comparing one of the Kclasses to the remaining\nK{-}1classes. Let {\\beta}0k,{\\beta}1k,...,{\\beta}pkdenote the parameters that result from\nfitting",
                    " an SVM comparing the kth class (coded as +1) to the others (coded\nas{-}1). Letx{*}denote a test observation. We assign the observation to the\nclass for which {\\beta}0k+{\\beta}1kx{*}\n1+{\\beta}2kx{*}\n2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pkx{*}\npis largest, as this amounts\nto a high level of confidence that the test observation belongs to the kth\nclass rather than to any of the other classes.\n9.5 Relationship to Logistic Regression\nWhen SVMs were first introduced in the mid-1990s, they made quite a\nsplash in the statistical and machine learning communities. This was due\nin part to their good performance, good marketing, and also to the fact\nthat the underlying approach seemed both novel and mysterious. The idea\nof finding a hyperplane that separates the data as well as possible, while al-\nlowing some violations to this separation, seemed distinctly different from\nclassical approaches for classification, such as logistic regression and lin-\near discriminant analysis. Moreover, the idea of using a kernel to expand\nthe feature space in order to accommodate non-linear class boundaries ap-\npeared to be a unique and valuable characteristic.\nHowever, since that time, deep connections between SVMs and other\nmore classical statistical methods have emerged. It turns out that one can\nrewrite the criterion ( 9.12){\\textendash}(9.15) for fitting the support vector classifier\nf(X)={\\beta}0+{\\beta}1X1+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pXpas\nminimize\n{\\beta}0,{\\beta}1,...,{\\beta}p\n\nn{\\sum}\ni=1max [0 ,1{-}yif(xi)] +{\\lambda}p{\\sum}\nj=1{\\beta}2\nj\n\n, (9.25)\nwhere{\\lambda}is a nonnegative tuning parameter. When {\\lambda}is large then {\\beta}1,...,{\\beta}p\nare small, more violations to the margin are tolerated, and a low-variance\nbut high-bias classifier will result. When {\\lambda}is",
                    " small then few violations\nto the margin will occur; this amounts to a high-variance but low-bias\n9.5 Relationship to Logistic Regression 385\nclassifier. Thus, a small value of {\\lambda}in (9.25) amounts to a small value of C\nin (9.15). Note that the {\\lambda}{\\sum}p\nj=1{\\beta}2\njterm in ( 9.25) is the ridge penalty term\nfrom Section 6.2.1, and plays a similar role in controlling the bias-variance\ntrade-off for the support vector classifier.\nNow(9.25)takesthe{\\textquotedblleft}Loss+Penalty{\\textquotedblright}formthatwehaveseenrepeatedly\nthroughout this book:\nminimize\n{\\beta}0,{\\beta}1,...,{\\beta}p{\\{}L(X,y,{\\beta})+{\\lambda}P({\\beta}){\\}}. (9.26)\nIn (9.26),L(X,y,{\\beta})is some loss function quantifying the extent to which\nthe model, parametrized by {\\beta}, fits the data (X,y), andP({\\beta})is a penalty\nfunction on the parameter vector {\\beta}whose effect is controlled by a nonneg-\native tuning parameter {\\lambda}. For instance, ridge regression and the lasso both\ntake this form with\nL(X,y,{\\beta})=n{\\sum}\ni=1\nyi{-}{\\beta}0{-}p{\\sum}\nj=1xij{\\beta}j\n2\nand with P({\\beta})={\\sum}p\nj=1{\\beta}2\njfor ridge regression and P({\\beta})={\\sum}p\nj=1|{\\beta}j|for\nthe lasso. In the case of ( 9.25) the loss function instead takes the form\nL(X,y,{\\beta})=n{\\sum}\ni=1max [0 ,1{-}yi({\\beta}0+{\\beta}1xi1+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip)].\nThis is known as hinge loss , and is depicted in Figure 9.12. However, it",
                    "hinge lossturns out that the hinge loss function is closely related to the loss function\nused in logistic regression, also shown in Figure 9.12.\nAn interesting characteristic of the support vector classifier is that only\nsupport vectors play a role in the classifier obtained; observations on the\ncorrect side of the margin do not affect it. This is due to the fact that the\nloss function shown in Figure 9.12is exactly zero for observations for which\nyi({\\beta}0+{\\beta}1xi1+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip){\\geq}1; these correspond to observations that are\non the correct side of the margin.3In contrast, the loss function for logistic\nregression shown in Figure 9.12is not exactly zero anywhere. But it is very\nsmall for observations that are far from the decision boundary. Due to the\nsimilarities between their loss functions, logistic regression and the support\nvector classifier often give very similar results. When the classes are well\nseparated, SVMs tend to behave better than logistic regression; in more\noverlapping regimes, logistic regression is often preferred.\nWhen the support vector classifier and SVM were first introduced, it was\nthought that the tuning parameter Cin (9.15) was an unimportant {\\textquotedblleft}nui-\nsance{\\textquotedblright} parameter that could be set to some default value, like 1. However,\nthe {\\textquotedblleft}Loss + Penalty{\\textquotedblright} formulation ( 9.25) for the support vector classifier\nindicates that this is not the case. The choice of tuning parameter is very\nimportant and determines the extent to which the model underfits or over-\nfits the data, as illustrated, for example, in Figure 9.7.\n3With this hinge-loss + penalty representation, the margin corresponds to the value\none, and the width of the margin is determined by{\\sum}{\\beta}2\nj.\n386 9. Support Vector Machines\n{-}6 {-}4 {-}2 0 202468LossSVM LossLogistic Regression Loss\nyi({\\beta}0+{\\beta}1xi1+...+{\\beta}pxip)FIGURE 9.12. The SVM and logistic regression loss functions are compared,\n",
                    "as a function of yi({\\beta}0+{\\beta}1xi1+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip). Whenyi({\\beta}0+{\\beta}1xi1+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\beta}pxip)is\ngreater than 1, then the SVM loss is zero, since this corresponds to an observation\nthat is on the correct side of the margin. Overall, the two loss functions have quite\nsimilar behavior.\nWe have established that the support vector classifier is closely related\nto logistic regression and other preexisting statistical methods. Is the SVM\nunique in its use of kernels to enlarge the feature space to accommodate\nnon-linear class boundaries? The answer to this question is {\\textquotedblleft}no{\\textquotedblright}. We could\njust as well perform logistic regression or many of the other classification\nmethods seen in this book using non-linear kernels; this is closely related\nto some of the non-linear approaches seen in Chapter 7. However, for his-\ntorical reasons, the use of non-linear kernels is much more widespread in\nthe context of SVMs than in the context of logistic regression or other\nmethods.\nThough we have not addressed it here, there is in fact an extension\nof the SVM for regression (i.e. for a quantitative rather than a qualita-\ntive response), called support vector regression . In Chapter 3, we saw thatsupport\nvector\nregressionleast squares regression seeks coefficients {\\beta}0,{\\beta}1,...,{\\beta}psuch that the sum\nof squared residuals is as small as possible. (Recall from Chapter 3that\nresiduals are defined as yi{-}{\\beta}0{-}{\\beta}1xi1{-}{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}{-}{\\beta}pxip.) Support vector\nregression instead seeks coefficients that minimize a different type of loss,\nwhere only residuals larger in absolute value than some positive constant\ncontribute to the loss function. This is an extension of the margin used in\nsupport vector classifiers to the regression setting."
                ]
            },
            {
                "10": [
                    "10\nDeep Learning\nThis chapter covers the important topic of deep learning . At the time ofdeep\nlearningwriting(2020),deeplearningisaveryactiveareaofresearchinthemachine\nlearning and artificial intelligence communities. The cornerstone of deep\nlearning is the neural network .neural\nnetworkNeural networks rose to fame in the late 1980s. There was a lot of excite-\nment and a certain amount of hype associated with this approach, and they\nwere the impetus for the popular Neural Information Processing Systems\nmeetings (NeurIPS, formerly NIPS) held every year, typically in exotic\nplaces like ski resorts. This was followed by a synthesis stage, where the\nproperties of neural networks were analyzed by machine learners, math-\nematicians and statisticians; algorithms were improved, and the method-\nology stabilized. Then along came SVMs, boosting, and random forests,\nand neural networks fell somewhat from favor. Part of the reason was that\nneural networks required a lot of tinkering, while the new methods were\nmore automatic. Also, on many problems the new methods outperformed\npoorly-trained neural networks. This was the status quo for the first decade\nin the new millennium.\nAll the while, though, a core group of neural-network enthusiasts were\npushingtheirtechnologyharderonever-largercomputingarchitecturesand\ndata sets. Neural networks resurfaced after 2010 with the new name deep\nlearning, with new architectures, additional bells and whistles, and a string\nof success stories on some niche problems such as image and video classifi-\ncation, speech and text modeling. Many in the field believe that the major\nreason for these successes is the availability of ever-larger training datasets,\nmade possible by the wide-scale use of digitization in science and industry.\nIn this chapter we discuss the basics of neural networks and deep learn-\ning, and then go into some of the specializations for specific problems, such\nas convolutional neural networks (CNNs) for image classification, and re-\ncurrent neural networks (RNNs) for time series and other sequences. We\n{\\textcopyright} Springer Nature Switzerland AG 2023 \nG. James et al., An Introduction to Statistical Learning , Springer Texts in Statistics, \nhttps://doi.org/10.1007/978-3-031-38747-0{\\_}10 399\n400 10. Deep Learning\nX1X2X",
                    "3X4A1A2A3A4A5f(X)YHiddenLayerInputLayerOutputLayer\nFIGURE 10.1. Neural network with a single hidden layer. The hidden layer\ncomputes activations Ak=hk(X)that are nonlinear transformations of linear\ncombinations of the inputs X1,X2,...,X p. Hence these Akare not directly ob-\nserved. The functions hk({\\textperiodcentered})are not fixed in advance, but are learned during the\ntraining of the network. The output layer is a linear model that uses these acti-\nvationsAkas inputs, resulting in a function f(X).\nwill also demonstrate these models using the Python torch package, along\nwith a number of helper packages.\nThe material in this chapter is slightly more challenging than elsewhere\nin this book.\n10.1 Single Layer Neural Networks\nA neural network takes an input vector of pvariables X=(X1,X2,...,X p)\nand builds a nonlinear function f(X)to predict the response Y. We have\nbuilt nonlinear prediction models in earlier chapters, using trees, boosting\nand generalized additive models. What distinguishes neural networks from\nthese methods is the particular structure of the model. Figure 10.1shows\na simple feed-forward neural network for modeling a quantitative responsefeed-forward\nneural\nnetworkusingp=4predictors. In the terminology of neural networks, the four fea-\nturesX1,...,X 4make up the units in the input layer . The arrows indicate\ninput layer that each of the inputs from the input layer feeds into each of the Khidden\nunits(we get to pick K; here we chose 5). The neural network model hashidden unitsthe form\nf(X)= {\\beta}0+{\\sum}K\nk=1{\\beta}khk(X)\n={\\beta}0+{\\sum}K\nk=1{\\beta}kg(wk0+{\\sum}p\nj=1wkjXj).(10.1)\nIt is built up here in two steps. First the Kactivations Ak,k=1,...,K , inactivationsthehiddenlayerarecomputedasfunctionsoftheinputfeatures X1,...,X p,\nAk=hk(X)=g(wk0+{\\sum}p\nj=1wkjXj), (10.",
                    "2)\n10.1 Single Layer Neural Networks 401\n{-}4 {-}2 0240.0 0.2 0.4 0.6 0.8 1.0zg(z)sigmoidReLU\nFIGURE 10.2. Activation functions. The piecewise-linear ReLUfunction is pop-\nular for its efficiency and computability. We have scaled it down by a factor of\nfive for ease of comparison.\nwhereg(z)is a nonlinear activation function that is specified in advance.activation\nfunctionWe can think of each Akas a different transformation hk(X)of the original\nfeatures, much like the basis functions of Chapter 7. These Kactivations\nfrom the hidden layer then feed into the output layer, resulting in\nf(X)={\\beta}0+K{\\sum}\nk=1{\\beta}kAk, (10.3)\na linear regression model in the K=5 activations. All the parameters\n{\\beta}0,...,{\\beta}Kandw10,...,w Kpneed to be estimated from data. In the early\ninstances of neural networks, the sigmoidactivation function was favored,sigmoid\ng(z)=ez\n1+ez=1\n1+e{-}z, (10.4)\nwhich is the same function used in logistic regression to convert a linear\nfunction into probabilities between zero and one (see Figure 10.2). The\npreferred choice in modern neural networks is the ReLU(rectified linearReLUunit) activation function, which takes the formrectified\nlinear unit\ng(z)=( z)+={\\{}0ifz{<}0\nzotherwise .(10.5)\nA ReLU activation can be computed and stored more efficiently than a\nsigmoid activation. Although it thresholds at zero, because we apply it to a\nlinear function ( 10.2) the constant term wk0will shift this inflection point.\nSo in words, the model depicted in Figure 10.1derives five new features\nby computing five different linear combinations of X, and then squashes\neach through an activation function g({\\textperiodcentered})to transform it. The final model\nis linear in these derived variables.\nThename neuralnetwork originallyderivedfromthinkingofthesehidden\nunits as analogous to neurons in the brain {\\textemdash} values of the activations\n",
                    "Ak=hk(X)close to one are firing, while those close to zero are silent\n(using the sigmoid activation function).\nThe nonlinearity in the activation function g({\\textperiodcentered})is essential, since without\nit the model f(X)in (10.1) would collapse into a simple linear model in\n402 10. Deep Learning\nX1,...,X p. Moreover, having a nonlinear activation function allows the\nmodel to capture complex nonlinearities and interaction effects. Consider\na very simple example with p=2 input variables X=(X1,X2), and\nK=2hidden units h1(X)andh2(X)withg(z)=z2. We specify the other\nparameters as\n{\\beta}0=0,{\\beta}1=1\n4,{\\beta}2={-}1\n4,\nw10=0,w 11=1,w 12=1 ,\nw20=0,w 21=1,w 22={-}1.(10.6)\nFrom (10.2), this means that\nh1(X) = (0 + X1+X2)2,\nh2(X) = (0 + X1{-}X2)2.(10.7)\nThen plugging ( 10.7) into (10.1), we get\nf(X)=0 +1\n4{\\textperiodcentered}(0 + X1+X2)2{-}1\n4{\\textperiodcentered}(0 + X1{-}X2)2\n=1\n4[\n(X1+X2)2{-}(X1{-}X2)2]\n=X1X2.(10.8)\nSo the sum of two nonlinear transformations of linear functions can give\nus an interaction! In practice we would not use a quadratic function for\ng(z), since we would always get a second-degree polynomial in the original\ncoordinates X1,...,X p. The sigmoid or ReLU activations do not have such\na limitation.\nFitting a neural network requires estimating the unknown parameters in\n(10.1). For a quantitative response, typically squared-error loss is used, so\nthat the parameters are chosen to minimize\nn{\\sum}\ni=1(yi{-}f(xi",
                    "))2. (10.9)\nDetailsabouthowtoperformthisminimizationareprovidedinSection 10.7.\n10.2 Multilayer Neural Networks\nModern neural networks typically have more than one hidden layer, and\noften many units per layer. In theory a single hidden layer with a large\nnumber of units has the ability to approximate most functions. However,\nthe learning task of discovering a good solution is made much easier with\nmultiple layers each of modest size.\nWe will illustrate a large dense network on the famous and publicly\navailable MNISThandwritten digit dataset.1Figure10.3shows examples of\nthese digits. The idea is to build a model to classify the images into their\ncorrect digit class 0{\\textendash}9. Every image has p= 28 {\\texttimes}28 = 784 pixels, each\nof which is an eight-bit grayscale value between 0 and 255 representing\n1See LeCun, Cortes, and Burges (2010) {\\textquotedblleft}The MNIST database of handwritten digits{\\textquotedblright},\navailable at http://yann.lecun.com/exdb/mnist .\n10.2 Multilayer Neural Networks 403\nFIGURE 10.3. Examples of handwritten digits from the MNISTcorpus. Each\ngrayscale image has 28{\\texttimes}28pixels, each of which is an eight-bit number (0{\\textendash}255)\nwhich represents how dark that pixel is. The first 3, 5, and 8 are enlarged to show\ntheir 784 individual pixel values.\nthe relative amount of the written digit in that tiny square.2These pixels\nare stored in the input vector X(in, say, column order). The output is\nthe class label, represented by a vector Y=(Y0,Y1,...,Y 9)of 10 dummy\nvariables, with a one in the position corresponding to the label, and zeros\nelsewhere. In the machine learning community, this is known as one-hot\nencoding. There are 60,000 training images, and 10,000 test images.one-hot\nencodingOn a historical note, digit recognition problems were the catalyst that\naccelerated the development of neural network technology in the late 1980s\nat AT{\\&}T Bell Laboratories and elsewhere. Pattern recognition tasks of this\nkind are relatively simple for humans. Our visual system occupies",
                    " a large\nfraction of our brains, and good recognition is an evolutionary force for\nsurvival. These tasks are not so simple for machines, and it has taken more\nthan 30 years to refine the neural-network architectures to match human\nperformance.\nFigure10.4shows a multilayer network architecture that works well for\nsolving the digit-classification task. It differs from Figure 10.1in several\nways:\n It has two hidden layers L1(256 units) and L2(128 units) rather\nthan one. Later we will see a network with seven hidden layers.\n It has ten output variables, rather than one. In this case the ten vari-\nables really represent a single qualitative variable and so are quite\ndependent. (We have indexed them by the digit class 0{\\textendash}9 rather than\n1{\\textendash}10, for clarity.) More generally, in multi-task learning one can pre-multi-task\nlearningdict different responses simultaneously with a single network; they\nall have a say in the formation of the hidden layers.\n The loss function used for training the network is tailored for the\nmulticlass classification task.\n2In the analog-to-digital conversion process, only part of the written numeral may\nfall in the square representing a particular pixel.\n404 10. Deep LearningX1X2X3X4X5X6...XpA(1)1A(1)2A(1)3A(1)4...A(1)K1A(2)1A(2)2A(2)3...A(2)K2f0(X)Y0f1(X)Y1......f9(X)Y9HiddenlayerL2HiddenlayerL1InputlayerOutputlayer\nW1W2BFIGURE 10.4. Neural network diagram with two hidden layers and multiple\noutputs, suitable for the MNISThandwritten-digit problem. The input layer has\np= 784 units, the two hidden layers K1= 256 andK2= 128 units respectively,\nand the output layer 10units. Along with intercepts (referred to as biasesin the\ndeep-learning community) this network has 235,146 parameters (referred to as\nweights).\nThe first hidden layer is as in ( 10.2), with\nA(1)\nk=h(1)\nk(X)\n=g",
                    "(w(1)\nk0+{\\sum}p\nj=1w(1)\nkjXj)(10.10)\nfork=1,...,K 1. The second hidden layer treats the activations A(1)\nkof\nthe first hidden layer as inputs and computes new activations\nA(2)\n{\\ell}=h(2)\n{\\ell}(X)\n=g(w(2)\n{\\ell}0+{\\sum}K1\nk=1w(2)\n{\\ell}kA(1)\nk)(10.11)\nfor{\\ell}=1,...,K 2.Notice that each of the activations in the second layer\nA(2)\n{\\ell}=h(2)\n{\\ell}(X)is a function of the input vector X. This is the case because\nwhile they are explicitly a function of the activations A(1)\nkfrom layer L1,\nthese in turn are functions of X. This would also be the case with more\nhidden layers. Thus, through a chain of transformations, the network is\nable to build up fairly complex transformations of Xthat ultimately feed\ninto the output layer as features.\nWe have introduced additional superscript notation such as h(2)\n{\\ell}(X)and\nw(2)\n{\\ell}jin (10.10) and (10.11) to indicate to which layer the activations and\nweights(coefficients) belong, in this case layer 2. The notation W1in Fig-weights\n10.2 Multilayer Neural Networks 405\nure10.4represents the entire matrix of weights that feed from the input\nlayer to the first hidden layer L1. This matrix will have 785{\\texttimes}256 = 200 ,960\nelements; there are 785 rather than 784 because we must account for the\nintercept or biasterm.3\nbiasEach element A(1)\nkfeeds to the second hidden layer L2via the matrix of\nweightsW2of dimension 257{\\texttimes}128 = 32 ,896.\nWe now get to the output layer, where we now have ten responses rather\nthan one. The first step is to compute ten different linear models similar\nto our single model ( 10.1),\nZm={\\beta}m0+{\\sum}K2\n{\\ell}=1{\\",
                    "beta}m{\\ell}h(2)\n{\\ell}(X)\n={\\beta}m0+{\\sum}K2\n{\\ell}=1{\\beta}m{\\ell}A(2)\n{\\ell},(10.12)\nform=0,1,...,9. The matrix Bstores all 129{\\texttimes}10 = 1 ,290of these\nweights.\nIf these were all separate quantitative responses, we would simply set\neachfm(X)=Zmand be done. However, we would like our estimates to\nrepresent class probabilities fm(X) = Pr( Y=m|X), just like in multi-\nnomial logistic regression in Section 4.3.5. So we use the special softmaxsoftmaxactivation function (see ( 4.13) on page 145),\nfm(X) = Pr( Y=m|X)=eZm\n{\\sum}9\n{\\ell}=0eZ{\\ell}, (10.13)\nform=0,1,...,9. This ensures that the 10 numbers behave like proba-\nbilities (non-negative and sum to one). Even though the goal is to build\na classifier, our model actually estimates a probability for each of the 10\nclasses. The classifier then assigns the image to the class with the highest\nprobability.\nTo train this network, since the response is qualitative, we look for coef-\nficient estimates that minimize the negative multinomial log-likelihood\n{-}n{\\sum}\ni=19{\\sum}\nm=0yimlog(fm(xi)), (10.14)\nalso known as the cross-entropy . This is a generalization of the crite-cross-\nentropy rion (4.5) for two-class logistic regression. Details on how to minimize this\nobjective are given in Section 10.7. If the response were quantitative, we\nwould instead minimize squared-error loss as in ( 10.9).\nTable10.1compares the test performance of the neural network with\ntwo simple models presented in Chapter 4that make use of linear decision\nboundaries:multinomiallogisticregressionandlineardiscriminantanalysis.\nThe improvement of neural networks over both of these linear methods is\ndramatic:thenetworkwithdropoutregularizationachievesatesterrorrate\nbelow 2{\\%",
                    "} on the 10,000test images. (We describe dropout regularization in\nSection10.7.3.) In Section 10.9.2of the lab, we present the code for fitting\nthis model, which runs in just over two minutes on a laptop computer.\n3The use of {\\textquotedblleft}weights{\\textquotedblright} for coefficients and {\\textquotedblleft}bias{\\textquotedblright} for the intercepts wk0in (10.2) is\npopular in the machine learning community; this use of bias is not to be confused with\nthe {\\textquotedblleft}bias-variance{\\textquotedblright} usage elsewhere in this book.\n406 10. Deep Learning\nMethod Test Error\nNeural Network + Ridge Regularization 2.3{\\%}\nNeural Network + Dropout Regularization 1.8{\\%}\nMultinomial Logistic Regression 7.2{\\%}\nLinear Discriminant Analysis 12.7{\\%}\nTABLE 10.1. Test error rate on the MNISTdata, for neural networks with two\nforms of regularization, as well as multinomial logistic regression and linear dis-\ncriminant analysis. In this example, the extra complexity of the neural network\nleads to a marked improvement in test error.\nFIGURE 10.5. A sample of images from the CIFAR100 database: a collection of\nnatural images from everyday life, with 100 different classes represented.\nAdding the number of coefficients in W1,W2andB, we get235,146in\nall, more than 33 times the number 785{\\texttimes}9=7,065needed for multinomial\nlogistic regression. Recall that there are 60,000images in the training set.\nWhile this might seem like a large training set, there are almost four times\nasmanycoefficientsintheneuralnetworkmodelasthereareobservationsin\nthe training set! To avoid overfitting, some regularization is needed. In this\nexample, we used two forms of regularization: ridge regularization, which\nis similar to ridge regression from Chapter 6, anddropoutregularization.dropoutWe discuss both forms of regularization in Section 10.7.\n10.3 Convolutional Neural Networks\nNeural networks rebounded around 2010 with big successes in image classi-\nfication. Around that time, massive databases",
                    " of labeled images were being\naccumulated, with ever-increasing numbers of classes. Figure 10.5shows\n75 images drawn from the CIFAR100 database.4This database consists of\n60,000 images labeled according to 20 superclasses (e.g. aquatic mammals),\nwith five classes per superclass (beaver, dolphin, otter, seal, whale). Each\nimage has a resolution of 32{\\texttimes}32pixels, with three eight-bit numbers per\npixel representing red, green and blue. The numbers for each image are\norganized in a three-dimensional array called a feature map . The first twofeature map\n4See Chapter 3 of Krizhevsky (2009) {\\textquotedblleft}Learning multiple layers of fea-\ntures from tiny images{\\textquotedblright}, available at https://www.cs.toronto.edu/{\\textasciitilde}kriz/\nlearning-features-2009-TR.pdf .\n10.3 Convolutional Neural Networks 407\nFIGURE 10.6. Schematic showing how a convolutional neural network classifies\nan image of a tiger. The network takes in the image and identifies local features.\nIt then combines the local features in order to create compound features, which in\nthis example include eyes and ears. These compound features are used to output\nthe label {\\textquotedblleft}tiger{\\textquotedblright}.\naxes are spatial (both are 32-dimensional), and the third is the channelchannelaxis,5representing the three colors. There is a designated training set of\n50,000 images, and a test set of 10,000.\nA special family of convolutional neural networks (CNNs) has evolved forconvolutional\nneural\nnetworksclassifying images such as these, and has shown spectacular success on a\nwide range of problems. CNNs mimic to some degree how humans classify\nimages, by recognizing specific features or patterns anywhere in the image\nthat distinguish each particular object class. In this section we give a brief\noverview of how they work.\nFigure10.6illustrates the idea behind a convolutional neural network on\na cartoon image of a tiger.6\nThe network first identifies low-level features in the input image, such\nas small edges, patches of color, and the like. These low-level features are\nthen combined to form higher-level features, such as",
                    " parts of ears, eyes,\nand so on. Eventually, the presence or absence of these higher-level features\ncontributes to the probability of any given output class.\nHowdoesaconvolutionalneuralnetworkbuildupthishierarchy?Itcom-\nbines two specialized types of hidden layers, called convolution layers and\npoolinglayers. Convolution layers search for instances of small patterns in\nthe image, whereas pooling layers downsample these to select a prominent\nsubset. In order to achieve state-of-the-art results, contemporary neural-\nnetwork architectures make use of many convolution and pooling layers.\nWe describe convolution and pooling layers next.\n10.3.1 Convolution Layers\nAconvolution layer is made up of a large number of convolution filters , eachconvolution\nlayer\nconvolution\nfilter5The term channel is taken from the signal-processing literature. Each channel is a\ndistinct source of information.\n6Thanks to Elena Tuzhilina for producing the diagram and https://www.\ncartooning4kids.com/ for permission to use the cartoon tiger.\n408 10. Deep Learning\nof which is a template that determines whether a particular local feature is\npresent in an image. A convolution filter relies on a very simple operation,\ncalled a convolution , which basically amounts to repeatedly multiplying\nmatrix elements and then adding the results.\nTo understand how a convolution filter works, consider a very simple\nexample of a 4{\\texttimes}3image:\nOriginal Image =\nabc\ndef\nghi\njkl\n.\nNow consider a 2{\\texttimes}2filter of the form\nConvolution Filter =[{\\alpha}{\\beta}\n{\\gamma}{\\delta}]\n.\nWhen we convolve the image with the filter, we get the result7\nConvolved Image =\na{\\alpha}+b{\\beta}+d{\\gamma}+e{\\delta}b{\\alpha}+c{\\beta}+e{\\gamma}+f{\\delta}\nd{\\alpha}+e{\\beta}+g{\\gamma}+h{\\delta}e{\\alpha}+f{\\beta}+h{\\gamma}+i{\\delta}\ng{\\alpha}+h{\\beta}+j{\\gamma}+k{\\delta}h{\\alpha}+i{\\beta",
                    "}+k{\\gamma}+l{\\delta}\n.\nFor instance, the top-left element comes from multiplying each element in\nthe2{\\texttimes}2filter by the corresponding element in the top left 2{\\texttimes}2portion\nof the image, and adding the results. The other elements are obtained in a\nsimilar way: the convolution filter is applied to every 2{\\texttimes}2submatrix of the\noriginal image in order to obtain the convolved image. If a 2{\\texttimes}2submatrix\nof the original image resembles the convolution filter, then it will have a\nlargevalue in the convolved image; otherwise, it will have a smallvalue.\nThus,the convolved image highlights regions of the original image that\nresemble the convolution filter. We have used 2{\\texttimes}2as an example; in\ngeneral convolution filters are small {\\ell}1{\\texttimes}{\\ell}2arrays, with {\\ell}1and{\\ell}2small\npositive integers that are not necessarily equal.\nFigure10.7illustratestheapplicationoftwoconvolutionfilterstoa 192{\\texttimes}\n179image of a tiger, shown on the left-hand side.8Each convolution filter\nis a15{\\texttimes}15image containing mostly zeros (black), with a narrow strip\nof ones (white) oriented either vertically or horizontally within the image.\nWhen each filter is convolved with the image of the tiger, areas of the tiger\nthat resemble the filter (i.e. that have either horizontal or vertical stripes or\nedges) are givenlarge values,and areas of the tiger that do not resemble the\nfeature are given small values. The convolved images are displayed on the\nright-hand side. We see that the horizontal stripe filter picks out horizontal\nstripes and edges in the original image, whereas the vertical stripe filter\npicks out vertical stripes and edges in the original image.\n7The convolved image is smaller than the original image because its dimension is\ngiven by the number of 2{\\texttimes}2submatrices in the original image. Note that 2{\\texttimes}2is the\ndimension of the convolution filter. If we want the convolved image to have the same\ndimension as the original image, then padding can be applied.\n8The tiger image used in Figures 10.7{\\textendash}10.9was",
                    " obtained from the public domain\nimage resource https://www.needpix.com/ .\n10.3 Convolutional Neural Networks 409\nFIGURE 10.7. Convolution filters find local features in an image, such as edges\nand small shapes. We begin with the image of the tiger shown on the left, and\napply the two small convolution filters in the middle. The convolved images high-\nlight areas in the original image where details similar to the filters are found.\nSpecifically, the top convolved image highlights the tiger`s vertical stripes, whereas\nthe bottom convolved image highlights the tiger`s horizontal stripes. We can think\nof the original image as the input layer in a convolutional neural network, and\nthe convolved images as the units in the first hidden layer.\nWe have used a large image and two large filters in Figure 10.7for illus-\ntration. For the CIFAR100 database there are 32{\\texttimes}32color pixels per image,\nand we use 3{\\texttimes}3convolution filters.\nIn a convolution layer, we use a whole bank of filters to pick out a variety\nof differently-oriented edges and shapes in the image. Using predefined\nfilters in this way is standard practice in image processing. By contrast,\nwith CNNs the filters are learnedfor the specific classification task. We can\nthink of the filter weights as the parameters going from an input layer to a\nhidden layer, with one hidden unit for each pixel in the convolved image.\nThis is in fact the case, though the parameters are highly structured and\nconstrained (see Exercise 4for more details). They operate on localized\npatches in the input image (so there are many structural zeros), and the\nsameweightsina givenfilter are reusedfor allpossible patchesin the image\n(so the weights are constrained).9\nWe now give some additional details.\n Since the input image is in color, it has three channels represented\nby a three-dimensional feature map (array). Each channel is a two-\ndimensional ( 32{\\texttimes}32) feature map {\\textemdash} one for red, one for green, and\none for blue. A single convolution filter will also have three channels,\none per color, each of dimension 3{\\texttimes}3, with potentially different filter\nweights. The results of the three convolutions are summed to form\na two-dimensional output",
                    " feature map. Note that at this point the\ncolor information has been used, and is not passed on to subsequent\nlayers except through its role in the convolution.\n9This used to be called weight sharing in the early years of neural networks.\n410 10. Deep Learning\n If we use Kdifferent convolution filters at this first hidden layer,\nwe getKtwo-dimensional output feature maps, which together are\ntreated as a single three-dimensional feature map. We view each of\ntheKoutput feature maps as a separate channel of information, so\nnow we have Kchannels in contrast to the three color channels of\nthe original input feature map. The three-dimensional feature map is\njust like the activations in a hidden layer of a simple neural network,\nexcept organized and produced in a spatially structured way.\n We typically apply the ReLU activation function ( 10.5) to the con-\nvolved image. This step is sometimes viewed as a separate layer in\nthe convolutional neural network, in which case it is referred to as a\ndetector layer .detector\nlayer\n10.3.2 Pooling Layers\nApoolinglayer provides a way to condense a large image into a smallerpoolingsummary image. While there are a number of possible ways to perform\npooling, the max pooling operation summarizes each non-overlapping 2{\\texttimes}2\nblock of pixels in an image using the maximum value in the block. This\nreduces the size of the image by a factor of two in each direction, and it\nalso provides some location invariance : i.e. as long as there is a large value\nin one of the four pixels in the block, the whole block registers as a large\nvalue in the reduced image.\nHere is a simple example of max pooling:\nMax pool\n1253\n3012\n2134\n1120\n{\\textrightarrow}[35\n24]\n.\n10.3.3 Architecture of a Convolutional Neural Network\nSo far we have defined a single convolution layer {\\textemdash} each filter produces a\nnew two-dimensional feature map. The number of convolution filters in a\nconvolution layer is akin to the number of units at a particular hidden layer\nin a fully-connected neural network of the type we saw in Section 10.2.\nThis number also defines the number of channels in the resulting three-\ndimensional feature map. We",
                    " have also described a pooling layer, which\nreduces the first two dimensions of each three-dimensional feature map.\nDeep CNNs havemanysuchlayers.Figure 10.8showsa typicalarchitecture\nfor a CNN for the CIFAR100 image classification task.\nAt the input layer, we see the three-dimensional feature map of a color\nimage, where the channel axis represents each color by a 32{\\texttimes}32two-\ndimensional feature map of pixels. Each convolution filter produces a new\nchannel at the first hidden layer, each of which is a 32{\\texttimes}32feature map\n(after some padding at the edges). After this first round of convolutions, we\nnow have a new {\\textquotedblleft}image{\\textquotedblright}; a feature map with considerably more channels\nthan the three color input channels (six in the figure, since we used six\nconvolution filters).\n3216328164322500100convolveconvolveconvolvepoolpoolpoolflatten8FIGURE 10.8. Architecture of a deep CNN for the CIFAR100 classification task.\nConvolution layers are interspersed with 2{\\texttimes}2max-pool layers, which reduce the\nsize by a factor of 2 in both dimensions.\nThis is followed by a max-pool layer, which reduces the size of the feature\nmap in each channel by a factor of four: two in each dimension.\nThisconvolve-then-poolsequenceisnowrepeatedforthenexttwolayers.\nSome details are as follows:\n Each subsequent convolve layer is similar to the first. It takes as input\nthe three-dimensional feature map from the previous layer and treats\nit like a single multi-channel image. Each convolution filter learned\nhas as many channels as this feature map.\n Since the channel feature maps are reduced in size after each pool\nlayer, we usually increase the number of filters in the next convolve\nlayer to compensate.\n Sometimes we repeat several convolve layers before a pool layer. This\neffectively increases the dimension of the filter.\nThese operations are repeated until the pooling has reduced each channel\nfeature map down to just a few pixels in each dimension. At this point the\nthree-dimensional feature maps are flattened {\\textemdash} the pixels are treated as\nseparate units {\\textemdash} and fed into one or more fully-connected layers before\nreaching the output layer, which is",
                    " a softmax activation for the 100 classes\n(as in (10.13)).\nThere are many tuning parameters to be selected in constructing such a\nnetwork, apart from the number, nature, and sizes of each layer. Dropout\nlearning can be used at each layer, as well as lasso or ridge regularization\n(see Section 10.7). The details of constructing a convolutional neural net-\nwork can seem daunting. Fortunately, terrific software is available, with\nextensive examples and vignettes that provide guidance on sensible choices\nfor the parameters. For the CIFAR100 official test set, the best accuracy as\nof this writing is just above 75{\\%}, but undoubtedly this performance will\ncontinue to improve.\n10.3.4 Data Augmentation\nAn additional important trick used with image modeling is data augment-data aug-\nmentationation. Essentially, each training image is replicated many times, with each\nreplicate randomly distorted in a natural way such that human recognition\nis unaffected. Figure 10.9shows some examples. Typical distortions are10.3 Convolutional Neural Networks 411\n412 10. Deep Learning\nFIGURE 10.9. Data augmentation. The original image (leftmost) is distorted\nin natural ways to produce different images with the same class label. These\ndistortions do not fool humans, and act as a form of regularization when fitting\nthe CNN.\nzoom, horizontal and vertical shift, shear, small rotations, and in this case\nhorizontal flips. At face value this is a way of increasing the training set\nconsiderably with somewhat different examples, and thus protects against\noverfitting. In fact we can see this as a form of regularization: we build a\ncloud of images around each original image, all with the same label. This\nkind of fattening of the data is similar in spirit to ridge regularization.\nWe will see in Section 10.7.2that the stochastic gradient descent al-\ngorithms for fitting deep learning models repeatedly process randomly-\nselected batches of, say, 128 training images at a time. This works hand-in-\nglove with augmentation, because we can distort each image in the batch\non the fly, and hence do not have to store all the new images.\n10.3.5 Results Using a Pretrained Classifier\nHere we use an industry-level pretrained classifier to predict the class of\nsome new images. The resnet",
                    "50 classifier is a convolutional neural network\nthat was trained using the imagenet data set, which consists of millions of\nimages that belong to an ever-growing number of categories.10Figure10.10\ndemonstrates the performance of resnet50 on six photographs (private col-\nlection of one of the authors).11The CNN does a reasonable job classifying\nthe hawk in the second image. If we zoom out as in the third image, it\ngets confused and chooses the fountain rather than the hawk. In the final\nimage a {\\textquotedblleft}jacamar{\\textquotedblright} is a tropical bird from South and Central America with\nsimilar coloring to the South African Cape Weaver. We give more details\non this example in Section 10.9.4.\nMuch of the work in fitting a CNN is in learning the convolution filters\nat the hidden layers; these are the coefficients of a CNN. For models fit to\nmassive corpora such as imagenet with many classes, the output of these\nfilters can serve as features for general natural-image classification prob-\nlems. One can use these pretrained hidden layers for new problems with\nmuch smaller training sets (a process referred to as weight freezing ), andweight\nfreezingjust train the last few layers of the network, which requires much less data.\n10For more information about resnet50 , see He, Zhang, Ren, and Sun (2015) {\\textquotedblleft}Deep\nresidual learning for image recognition{\\textquotedblright}, https://arxiv.org/abs/1512.03385 . For de-\ntails about imagenet , see Russakovsky, Deng, et al. (2015) {\\textquotedblleft}ImageNet Large Scale\nVisual Recognition Challenge{\\textquotedblright}, in International Journal of Computer Vision .\n11Theseresnet results can change with time, since the publicly-trained model gets\nupdated periodically.\n10.4 Document Classification 413\nflamingo Cooper`s hawk Cooper`s hawk\nflamingo 0.83 kite 0.60fountain 0.35\nspoonbill 0.17 great grey owl 0.09 nail 0.12\nwhite stork 0.00 robin 0.06hook 0.07\nLhasa Apso cat Cape weaver\nTibetan terrier 0.56 Old English sheepdog 0.82 jacamar 0.",
                    "28\nLhasa 0.32Shih-Tzu 0.04macaw 0.12\ncocker spaniel 0.03 Persian cat 0.04robin 0.12\nFIGURE 10.10. Classification of six photographs using the resnet50 CNN\ntrained on the imagenet corpus. The table below the images displays the true\n(intended) label at the top of each panel, and the top three choices of the classifier\n(out of 100). The numbers are the estimated probabilities for each choice. (A kite\nis a raptor, but not a hawk.)\nThe vignettes and book12that accompany the keraspackage give more\ndetails on such applications.\n10.4 Document Classification\nIn this section we introduce a new type of example that has important\napplications in industry and science: predicting attributes of documents.\nExamples of documents include articles in medical journals, Reuters news\nfeeds, emails, tweets, and so on. Our example will be IMDb(Internet Movie\nDatabase) ratings {\\textemdash} short documents where viewers have written critiques\nof movies.13The response in this case is the sentiment of the review, which\nwill bepositiveornegative.\n12Deep Learning with R by F. Chollet and J.J. Allaire, 2018, Manning Publications.\n13For details, see Maas et al. (2011) {\\textquotedblleft}Learning word vectors for sentiment analysis{\\textquotedblright},\ninProceedings of the 49th Annual Meeting of the Association for Computational Lin-\nguistics: Human Language Technologies , pages 142{\\textendash}150.\n414 10. Deep Learning\nHere is the beginning of a rather amusing negative review:\nThis has to be one of the worst films of the 1990s. When my\nfriends {\\&} I were watching this film (being the target audience it\nwas aimed at) we just sat {\\&} watched the first half an hour with\nour jaws touching the floor at how bad it really was. The rest\nof the time, everyone else in the theater just started talking to\neach other, leaving or generally crying into their popcorn ...\nEach review can be a different length, include slang or non-words, have\nspelling errors, etc. We need to find a way to featurize such a document.featurizeThis is modern parlance for defining a set of predictors.\nThe simplest and",
                    " most common featurization is the bag-of-words model.bag-of-wordsWe score each document for the presence or absence of each of the words in\na language dictionary {\\textemdash} in this case an English dictionary. If the dictionary\ncontains Mwords,thatmeansforeachdocumentwecreateabinaryfeature\nvector of length M, and score a 1for every word present, and 0otherwise.\nThat can be a very wide feature vector, so we limit the dictionary {\\textemdash} in\nthis case to the 10,000 most frequently occurring words in the training\ncorpus of 25,000 reviews. Fortunately there are nice tools for doing this\nautomatically. Here is the beginning of a positive review that has been\nredacted in this way:\n{\\langle}START{\\rangle}this film was just brilliant casting location scenery\nstory direction everyone`s really suited the part they played and\nyou could just imagine being there robert {\\langle}UNK{\\rangle}is an amazing\nactor and now the same being director {\\langle}UNK{\\rangle}father came from\nthe same scottish island as myself so i loved ...\nHere we can see many words have been omitted, and some unknown words\n(UNK) have been marked as such. With this reduction the binary feature\nvector has length 10,000, and consists mostly of 0`s and a smattering of 1`s\nin the positions corresponding to words that are present in the document.\nWe have a training set and test set, each with 25,000 examples, and each\nbalanced with regard to sentiment . The resulting training feature matrix X\nhas dimension 25,000{\\texttimes}10,000, but only 1.3{\\%} of the binary entries are non-\nzero. We call such a matrix sparse, because most of the values are the same\n(zero in this case); it can be stored efficiently in sparse matrix format .14\nsparse\nmatrix\nformatThere are a variety of ways to account for the document length; here we\nonly score a word as in or out of the document, but for example one could\ninstead record the relative frequency of words. We split off a validation set\nof size 2,000 from the 25,000 training observations (for model tuning), and\nfit two model sequences:\n A lasso logistic regression using the glmnetpackage;\n A two",
                    "-class neural network with two hidden layers, each with 16\nReLU units.\n14Rather than store the whole matrix, we can store instead the location and values for\nthe nonzero entries. In this case, since the nonzero entries are all 1, just the locations\nare stored.\n10.4 Document Classification 415\nFIGURE 10.11. Accuracy of the lasso and a two-hidden-layer neural network\non theIMDbdata. For the lasso, the x-axis displays {-}log({\\lambda}), while for the neural\nnetwork it displays epochs (number of times the fitting algorithm passes through\nthe training set). Both show a tendency to overfit, and achieve approximately the\nsame test accuracy.\nBoth methods produce a sequence of solutions. The lasso sequence is in-\ndexed by the regularization parameter {\\lambda}. The neural-net sequence is in-\ndexed by the number of gradient-descent iterations used in the fitting,\nas measured by training epochs or passes through the training set (Sec-\ntion10.7). Notice that the training accuracy in Figure 10.11(black points)\nincreases monotonically in both cases. We can use the validation error to\npick a good solution from each sequence (blue points in the plots), which\nwould then be used to make predictions on the test data set.\nNote that a two-class neural network amounts to a nonlinear logistic\nregression model. From ( 10.12) and (10.13) we can see that\nlog(Pr(Y=1|X)\nPr(Y=0|X))\n=Z1{-}Z0 (10.15)\n=( {\\beta}10{-}{\\beta}00)+K2{\\sum}\n{\\ell}=1({\\beta}1{\\ell}{-}{\\beta}0{\\ell})A(2)\n{\\ell}.\n(This shows the redundancy in the softmax function; for Kclasses we\nreally only need to estimate K{-}1sets of coefficients. See Section 4.3.5.) In\nFigure10.11we show accuracy (fraction correct) rather than classificationaccuracy\nerror (fraction incorrect), the former being more popular in the machine\nlearning community. Both models achieve a test-set accuracy of about 88{\\%}.\nThe bag-of-words model summarizes a document by the words present,",
                    "\nand ignores their context. There are at least two popular ways to take the\ncontext into account:\n Thebag-of-n-gramsmodel. For example, a bag of 2-grams recordsbag-of-n-\ngrams/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n468 1 0 1 20.6 0.7 0.8 0.9 1.0Lasso\n{-}log({\\lambda})Accuracy\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni",
                    "25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni",
                    "25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n/uni25CF/uni25CF/uni25CFtrainvalidationtest/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n51 0 1 5 2 00.6 0.7 0.8 0.9 1.0Neural Net\nEpochsAccuracy/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF",
                    "/uni25CF/uni25CF/uni25CF\n416 10. Deep Learningthe consecutive co-occurrence of every distinct pair of words. {\\textquotedblleft}Bliss-fully long{\\textquotedblright} can be seen as a positive phrase in a movie review, while{\\textquotedblleft}blissfully short{\\textquotedblright} a negative. Treat the document as a sequence, taking account of all the words inthe context of those that preceded and those that follow.In the next section we explore models for sequences of data, which haveapplications in weather forecasting, speech recognition, language transla-tion, and time-series prediction, to name a few. We continue with thisIMDbexample there.10.5 Recurrent Neural NetworksMany data sources are sequential in nature, and call for special treatmentwhen building predictive models. Examples include: Documents such as book and movie reviews, newspaper articles, andtweets. The sequence and relative positions of words in a documentcapture the narrative, theme and tone, and can be exploited in taskssuch as topic classification, sentiment analysis, and language transla-tion. Time series of temperature, rainfall, wind speed, air quality, and soon. We may want to forecast the weather several days ahead, or cli-mate several decades ahead. Financialtimeseries,wherewetrackmarketindices,tradingvolumes,stock and bond prices, and exchange rates. Here prediction is oftendifficult, but as we will see, certain indices can be predicted withreasonable accuracy. Recorded speech, musical recordings, and other sound recordings. Wemay want to give a text transcription of a speech, or perhaps a lan-guage translation. We may want to assess the quality of a piece ofmusic, or assign certain attributes. Handwriting, such as doctor`s notes, and handwritten digits such aszip codes. Here we want to turn the handwriting into digital text, orread the digits (optical character recognition).In arecurrent neural network(RNN), the input objectXis asequence.recurrentneuralnetworkConsider a corpus of documents, such as the collection ofIMDbmovie re-views. Each document can be represented as a sequence ofLwords, soX={\\{}X1,X2,...,XL{\\}}, where eachX{\\ell}represents a word. The order ofthe words, and closeness of certain words in a sentence, convey semanticmeaning. RNNs",
                    " are designed to accommodate and take advantage of thesequential nature of such input objects, much like convolutional neural net-works accommodate the spatial structure of image inputs. The outputYcan also be a sequence (such as in language translation), but often is ascalar, like the binary sentiment label of a movie review document.\n10.5 Recurrent Neural Networks 417\nA1A2A3AL-1AL=A{\\ell}O{\\ell}Y\nX{\\ell}O1X1O2X2O3X3OL-1XL-1OLY\nXL...WUBWBWBWBWBWBUUUUFIGURE 10.12. Schematic of a simple recurrent neural network. The input is a\nsequence of vectors {\\{}X{\\ell}{\\}}L\n1, and here the target is a single response. The network\nprocesses the input sequence Xsequentially; each X{\\ell}feeds into the hidden layer,\nwhich also has as input the activation vector A{\\ell}{-}1from the previous element in\nthe sequence, and produces the current activation vector A{\\ell}. The same collections\nof weights W,UandBare used as each element of the sequence is processed. The\noutput layer produces a sequence of predictions O{\\ell}from the current activation\nA{\\ell}, but typically only the last of these, OL, is of relevance. To the left of the equal\nsign is a concise representation of the network, which is unrolled into a more\nexplicit version on the right.\nFigure10.12illustratesthestructureofaverybasicRNNwithasequence\nX={\\{}X1,X2,...,X L{\\}}as input, a simple output Y, and a hidden-layer\nsequence {\\{}A{\\ell}{\\}}L\n1={\\{}A1,A2,...,A L{\\}}. EachX{\\ell}is a vector; in the document\nexample X{\\ell}could represent a one-hot encoding for the {\\ell}th word based on\nthe language dictionary for the corpus (see the top panel in Figure 10.13\nfor a simple example). As the sequence is processed one vector X{\\ell}at a\ntime, the network updates the activations A{\\ell}in the hidden layer, taking\nas input the vector X{\\ell}and the activation vector A{\\ell}{-}1from the previous\n",
                    "step in the sequence. Each A{\\ell}feeds into the output layer and produces a\nprediction O{\\ell}forY.OL, the last of these, is the most relevant.\nIn detail, suppose each vector X{\\ell}of the input sequence has pcomponents\nXT\n{\\ell}=(X{\\ell}1,X{\\ell}2,...,X {\\ell}p), and the hidden layer consists of KunitsAT\n{\\ell}=\n(A{\\ell}1,A{\\ell}2,...,A {\\ell}K). As in Figure 10.4, we represent the collection of K{\\texttimes}\n(p+1) shared weights wkjfor the input layer by a matrix W, and similarly\nUis aK{\\texttimes}Kmatrix of the weights uksfor the hidden-to-hidden layers,\nandBis aK+1vector of weights {\\beta}kfor the output layer. Then\nA{\\ell}k=g(\nwk0+p{\\sum}\nj=1wkjX{\\ell}j+K{\\sum}\ns=1uksA{\\ell}{-}1,s)\n, (10.16)\nand the output O{\\ell}is computed as\nO{\\ell}={\\beta}0+K{\\sum}\nk=1{\\beta}kA{\\ell}k (10.17)\nfor a quantitative response, or with an additional sigmoid activation func-\ntion for a binary response, for example. Here g({\\textperiodcentered})is an activation function\nsuch as ReLU. Notice that the same weights W,UandBare used as we\n418 10. Deep Learning\nprocess each element in the sequence, i.e. they are not functions of {\\ell}. This\nis a form of weight sharing used by RNNs, and similar to the use of filtersweight\nsharingin convolutional neural networks (Section 10.3.1.) As we proceed from be-\nginning to end, the activations A{\\ell}accumulate a history of what has been\nseen before, so that the learned context can be used for prediction.\nFor regression problems the loss function for an observation (X,Y)is\n(Y{-}OL)2, (10.18)\nwhichonlyreferencesthefinaloutput OL={\\beta}0+{\\sum}",
                    "K\nk=1{\\beta}kALk.ThusO1,O2,\n...,O L{-}1arenotused.Whenwefitthemodel,eachelement X{\\ell}oftheinput\nsequence Xcontributes to OLvia the chain ( 10.16), and hence contributes\nindirectly to learning the shared parameters W,UandBvia the loss\n(10.18). Withninput sequence/response pairs (xi,yi),the parameters are\nfound by minimizing the sum of squares\nn{\\sum}\ni=1(yi{-}oiL)2=n{\\sum}\ni=1(\nyi{-}(\n{\\beta}0+K{\\sum}\nk=1{\\beta}kg(\nwk0+p{\\sum}\nj=1wkjxiLj+K{\\sum}\ns=1uksai,L{-}1,s)))2\n.\n(10.19)\nHere we use lowercase letters for the observed yiand vector sequences\nxi={\\{}xi1,xi2,...,x iL{\\}},15as well as the derived activations.\nSince the intermediate outputs O{\\ell}are not used, one may well ask why\nthey are there at all. First of all, they come for free, since they use the same\noutputweights Bneededtoproduce OL,andprovideanevolvingprediction\nfor the output. Furthermore, for some learning tasks the response is also a\nsequence, and so the output sequence {\\{}O1,O2,...,O L{\\}}is explicitly needed.\nWhen used at full strength, recurrent neural networks can be quite com-\nplex. We illustrate their use in two simple applications. In the first, we\ncontinue with the IMDbsentiment analysis of the previous section, where\nwe process the words in the reviews sequentially. In the second application,\nwe illustrate their use in a financial time series forecasting problem.\n10.5.1 Sequential Models for Document Classification\nHere we return to our classification task with the IMDbreviews. Our ap-\nproach in Section 10.4was to use the bag-of-words model. Here the plan\nis to use instead the sequence of words occurring in a document to make\npredictions about the label for the entire document.\nWe have, however, a dimensionality problem",
                    ": each word in our document\nis represented by a one-hot-encoded vector (dummy variable) with 10,000\nelements (one per word in the dictionary)! An approach that has become\npopular is to represent each word in a much lower-dimensional embeddingembeddingspace. This means that rather than representing each word by a binary\nvector with 9,999 zeros and a single one in some position, we will represent\nit instead by a set of mreal numbers, none of which are typically zero. Here\nmis the embedding dimension, and can be in the low 100s, or even less.\nThis means (in our case) that we need a matrix Eof dimension m{\\texttimes}10,000,\n15This is a sequence of vectors; each element xi{\\ell}is ap-vector.\n10.5 Recurrent Neural Networks 419thisisoneofthebestfilmsactuallythebestIhaveeverseenthefilmstartsonefalldayOne{-}hotEmbedFIGURE 10.13. Depiction of a sequence of 20words representing a single doc-\nument: one-hot encoded using a dictionary of 16words (top panel) and embedded\nin anm-dimensional space with m=5(bottom panel).\nwhere each column is indexed by one of the 10,000 words in our dictionary,\nand the values in that column give the mcoordinates for that word in the\nembedding space.\nFigure10.13illustrates the idea (with a dictionary of 16 rather than\n10,000, and m=5). Where does Ecome from? If we have a large corpus\nof labeled documents, we can have the neural network learnEas part\nof the optimization. In this case Eis referred to as an embedding layer,embedding\nlayerand a specialized Eis learned for the task at hand. Otherwise we can\ninsert a precomputed matrix Ein the embedding layer, a process known\nasweight freezing . Two pretrained embeddings, word2vec andGloVe, areweight\nfreezing\nword2vec\nGloVewidely used.16These are built from a very large corpus of documents by\na variant of principal components analysis (Section 12.2). The idea is that\nthe positions of words in the embedding space preserve semantic meaning;\ne.g. synonyms should appear near each other.\nSo far, so good.",
                    " Each document is now represented as a sequence of m-\nvectors that represents the sequence of words. The next step is to limit\neach document to the last Lwords. Documents that are shorter than L\nget padded with zeros upfront. So now each document is represented by a\nseries consisting of LvectorsX={\\{}X1,X2,...,X L{\\}}, and each X{\\ell}in the\nsequence has mcomponents.\nWe now use the RNN structure in Figure 10.12. The training corpus\nconsists of nseparate series (documents) of length L, each of which gets\nprocessed sequentially from left to right. In the process, a parallel series of\nhidden activation vectors A{\\ell},{\\ell}=1,...,L is created as in ( 10.16) for each\ndocument. A{\\ell}feedsintotheoutputlayertoproducetheevolvingprediction\nO{\\ell}. We use the final value OLto predict the response: the sentiment of the\nreview.\n16word2vec is described in Mikolov, Chen, Corrado, and Dean (2013), available\nathttps://code.google.com/archive/p/word2vec .GloVeis described in Pennington,\nSocher, and Manning (2014), available at https://nlp.stanford.edu/projects/glove .\n420 10. Deep Learning\nThis is a simple RNN, and has relatively few parameters. If there are K\nhidden units, the common weight matrix WhasK{\\texttimes}(m+ 1) parameters,\nthe matrix UhasK{\\texttimes}Kparameters, and Bhas2(K+ 1) for the two-class\nlogistic regression as in ( 10.15). These are used repeatedly as we process\nthe sequence X={\\{}X{\\ell}{\\}}L\n1from left to right, much like we use a single\nconvolution filter to process each patch in an image (Section 10.3.1). If the\nembedding layer Eis learned, that adds an additional m{\\texttimes}Dparameters\n(D= 10 ,000here), and is by far the biggest cost.\nWe fit the RNN as described in Figure 10.12and the accompaying text to\ntheIMDbdata. The model had an embedding matrix Ewithm= 32 (which\nwas",
                    " learned in training as opposed to precomputed), followed by a single\nrecurrent layer with K= 32 hidden units. The model was trained with\ndropout regularization on the 25,000 reviews in the designated training\nset, and achieved a disappointing 76{\\%} accuracy on the IMDbtest data. A\nnetwork using the GloVepretrained embedding matrix Eperformed slightly\nworse.\nFor ease of exposition we have presented a very simple RNN. More elab-\norate versions use long term andshort term memory (LSTM). Two tracks\nof hidden-layer activations are maintained, so that when the activation A{\\ell}\nis computed, it gets input from hidden units both further back in time,\nand closer in time {\\textemdash} a so-called LSTM RNN . With long sequences, thisLSTM RNNovercomes the problem of early signals being washed out by the time they\nget propagated through the chain to the final activation vector AL.\nWhen we refit our model using the LSTM architecture for the hidden\nlayer, the performance improved to 87{\\%} on the IMDbtest data. This is com-\nparable with the 88{\\%} achieved by the bag-of-words model in Section 10.4.\nWe give details on fitting these models in Section 10.9.6.\nDespite this added LSTM complexity, our RNN is still somewhat {\\textquotedblleft}entry\nlevel{\\textquotedblright}. We could probably achieve slightly better results by changing the\nsize of the model, changing the regularization, and including additional\nhidden layers. However, LSTM models take a long time to train, which\nmakes exploring many architectures and parameter optimization tedious.\nRNNs provide a rich framework for modeling data sequences, and they\ncontinue to evolve. There have been many advances in the development\nof RNNs {\\textemdash} in architecture, data augmentation, and in the learning algo-\nrithms. At the time of this writing (early 2020) the leading RNN configura-\ntions report accuracy above 95{\\%} on the IMDbdata. The details are beyond\nthe scope of this book.17\n10.5.2 Time Series Forecasting\nFigure10.14shows historical trading statistics from the New York Stock\nExchange. Shown are three daily time series covering the period December\n3, 1962 to December",
                    " 31, 1986:18\n17AnIMDbleaderboard can be found at https://paperswithcode.com/sota/\nsentiment-analysis-on-imdb .\n18These data were assembled by LeBaron and Weigend (1998) IEEE Transactions on\nNeural Networks , 9(1): 213{\\textendash}220.\n10.5 Recurrent Neural Networks 421Log(Trading Volume){-}1.0 0.0 0.5 1.0Dow Jones Return{-}0.04 0.00 0.04\n19651970197519801985{-}13 {-}11 {-}9 {-}8Log(Volatility)FIGURE 10.14. Historical trading statistics from the New York Stock Exchange.\nDaily values of the normalized log trading volume, DJIA return, and log volatility\nare shown for a 24-year period from 1962{\\textendash}1986. We wish to predict trading volume\non any day, given the history on all earlier days. To the left of the red bar (January\n2, 1980) is training data, and to the right test data.\nLog trading volume .Thisisthefractionofalloutstandingsharesthat\nare traded on that day, relative to a 100-day moving average of past\nturnover, on the log scale.\nDow Jones return . This is the difference between the log of the Dow\nJones Industrial Index on consecutive trading days.\nLog volatility . This is based on the absolute values of daily price\nmovements.\nPredicting stock prices is a notoriously hard problem, but it turns out that\npredicting trading volume based on recent past history is more manageable\n(and is useful for planning trading strategies).\nAn observation here consists of the measurements (vt,rt,zt)on dayt, in\nthis case the values for log{\\_}volume ,DJ{\\_}return andlog{\\_}volatility . There\nareatotalof T=6,051suchtriples,eachofwhichisplottedasatimeseries\nin Figure 10.14. One feature that strikes us immediately is that the day-\nto-day observations are not independent of each other. The series exhibit\nauto-correlation {\\textemdash} in this case values nearby in time tend to be similarauto-\ncorrelation to each other. This distinguishes time series from other data sets we have\nencount",
                    "ered, in which observations can be assumed to be independent of\n422 10. Deep Learning\n0 5 10 15 20 25 30 350.0 0.4 0.8Log( Trading Volume)\nLagAutocorrelation FunctionFIGURE 10.15. The autocorrelation function for log{\\_}volume . We see that\nnearby values are fairly strongly correlated, with correlations above 0.2as far as\n20 days apart.\neach other. To be clear, consider pairs of observations (vt,vt{-}{\\ell}),alagof{\\ell}lagdays apart. If we take all such pairs in the vtseries and compute their corre-\nlation coefficient, this gives the autocorrelation at lag {\\ell}. Figure10.15shows\nthe autocorrelation function for all lags up to 37, and we see considerable\ncorrelation.\nAnother interesting characteristic of this forecasting problem is that the\nresponse variable vt{\\textemdash}log{\\_}volume {\\textemdash} is also a predictor! In particular, we\nwill use the past values of log{\\_}volume to predict values in the future.\nRNN forecaster\nWe wish to predict a value vtfrom past values vt{-}1,vt{-}2,..., and also to\nmake use of past values of the other series rt{-}1,rt{-}2,...andzt{-}1,zt{-}2,....\nAlthough our combined data is quite a long series with 6,051 trading\ndays, the structure of the problem is different from the previous document-\nclassification example.\n We only have one series of data, not 25,000.\n We have an entire seriesof targets vt, and the inputs include past\nvalues of this series.\nHow do we represent this problem in terms of the structure displayed in\nFigure10.12? The idea is to extract many short mini-series of input se-\nquencesX={\\{}X1,X2,...,X L{\\}}with a predefined length L(called the laglagin this context), and a corresponding target Y. They have the form\nX1=\nvt{-}L\nrt{-}L\nzt{-}L\n,X2=\nvt{-}L+1\nrt{-}L+1\nzt{-}L+",
                    "1\n,{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered},XL=\nvt{-}1\nrt{-}1\nzt{-}1\n,andY=vt.\n(10.20)\nSo here the target Yis the value of log{\\_}volume vtat a single timepoint t,\nand the input sequence Xis the series of 3-vectors {\\{}X{\\ell}{\\}}L\n1each consisting\nof the three measurements log{\\_}volume ,DJ{\\_}return andlog{\\_}volatility from\ndayt{-}L,t{-}L+1, up tot{-}1. Each value of tmakes a separate (X,Y)\npair, for trunning from L+1toT. For the NYSEdata we will use the past\n10.5 Recurrent Neural Networks 423\n1980198219841986{-}1.0 0.0 0.5 1.0Test Period: Observed and Predicted\nYearlog(Trading Volume)FIGURE 10.16. RNN forecast of log{\\_}volume on theNYSEtest data. The black\nlines are the true volumes, and the superimposed orange the forecasts. The fore-\ncasted series accounts for 42{\\%} of the variance of log{\\_}volume .\nfive trading days to predict the next day`s trading volume. Hence, we use\nL=5. SinceT=6,051, we can create 6,046 such (X,Y)pairs. Clearly L\nis a parameter that should be chosen with care, perhaps using validation\ndata.\nWe fit this model with K= 12 hidden units using the 4,281 training\nsequences derived from the data before January 2, 1980 (see Figure 10.14),\nand then used it to forecast the 1,770 values of log{\\_}volume after this date.\nWe achieve an R2=0.42on the test data. Details are given in Sec-\ntion10.9.6. As astraw man ,19using yesterday`s value for log{\\_}volume as\nthe prediction for today has R2=0.18. Figure 10.16shows the forecast\nresults. We have plotted the observed values of the daily log{\\_}volume for the\ntest period 1980{\\textendash}1986 in black, and superimposed the predicted",
                    " series in\norange. The correspondence seems rather good.\nIn forecasting the value of log{\\_}volume in the test period, we have to use\nthe test data itself in forming the input sequences X. This may feel like\ncheating, but in fact it is not; we are always using past data to predict the\nfuture.\nAutoregression\nThe RNN we just fit has much in common with a traditional autoregressionauto-\nregression (AR) linear model, which we present now for comparison. We first consider\nthe response sequence vtalone, and construct a response vector yand a\nmatrixMof predictors for least squares regression as follows:\ny=\nvL+1\nvL+2\nvL+3\n...\nvT\nM=\n1 vLvL{-}1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} v1\n1vL+1 vL{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} v2\n1vL+2vL+1{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered} v3\n...............\n1vT{-}1vT{-}2{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}vT{-}L\n.(10.21)\nMandyeach have T{-}Lrows, one per observation. We see that the\npredictors for any given response vton daytare the previous Lvalues\n19A straw man here refers to a simple and sensible prediction that can be used as a\nbaseline for comparison.\n424 10. Deep Learning\nof the same series. Fitting a regression of yonMamounts to fitting the\nmodel\n{\\textasciicircum}vt={\\textasciicircum}{\\beta}0+{\\textasciicircum}{\\beta}1vt{-}1+{\\textasciicircum}{\\beta}2vt{-}2+{\\textperiodcentered}{\\textperiodcentered}{\\textperiodcentered}+{\\textasciicircum}{\\beta}Lvt{-}L, (10.22)\nand is called an order- Lautoregressive model, or simply AR( L). For the\nNYSEdata we can include lagged versions of DJ{\\_}return andlog{\\_}volatility",
                    " ,\nrtandzt, in the predictor matrix M, resulting in 3L+1columns. An AR\nmodel with L=5 achieves a test R2of0.41, slightly inferior to the 0.42\nachieved by the RNN.\nOf course the RNN and AR models are very similar. They both use\nthe same response Yand input sequences Xof length L=5 and dimen-\nsionp=3 in this case. The RNN processes this sequence from left to\nright with the same weights W(for the input layer), while the AR model\nsimply treats all Lelements of the sequence equally as a vector of L{\\texttimes}p\npredictors {\\textemdash} a process called flattening in the neural network literature.flatteningOf course the RNN also includes the hidden layer activations A{\\ell}which\ntransfer information along the sequence, and introduces additional nonlin-\nearity. From ( 10.19) withK= 12 hidden units, we see that the RNN has\n13 + 12 {\\texttimes}(1 + 3 + 12) = 205 parameters, compared to the 16 for the AR( 5)\nmodel.\nAn obvious extension of the AR model is to use the set of lagged predic-\ntors as the input vector to an ordinary feedforward neural network ( 10.1),\nand hence add more flexibility. This achieved a test R2=0.42, slightly\nbetter than the linear AR, and the same as the RNN.\nAll the models can be improved by including the variable day{\\_}of{\\_}week\ncorresponding to the day tof the target vt(which can be learned from the\ncalendar dates supplied with the data); trading volume is often higher on\nMondays and Fridays. Since there are five trading days, this one-hot en-\ncodes to five binary variables. The performance of the AR model improved\ntoR2=0.46as did the RNN, and the nonlinear AR model improved to\nR2=0.47.\nWe used the most simple version of the RNN in our examples here.\nAdditionalexperimentswiththeLSTMextensionoftheRNNyieldedsmall\nimprovements, typically of up to 1{\\%} in R2in these examples.\nWe give details of how we fit all three models in Section 10.9.6",
                    ".\n10.5.3 Summary of RNNs\nWe have illustrated RNNs through two simple use cases, and have only\nscratched the surface.\nThere are many variations and enhancements of the simple RNN we\nused for sequence modeling. One approach we did not discuss uses a one-\ndimensional convolutional neural network, treating the sequence of vectors\n(say words, as represented in the embedding space) as an image. The con-\nvolution filter slides along the sequence in a one-dimensional fashion, with\nthe potential to learn particular phrases or short subsequences relevant to\nthe learning task.\nOne can also have additional hidden layers in an RNN. For example,\nwith two hidden layers, the sequence A{\\ell}is treated as an input sequence to\nthe next hidden layer in an obvious fashion.\n10.6 When to Use Deep Learning 425\nThe RNN we used scanned the document from beginning to end; alter-\nnativebidirectional RNNs scan the sequences in both directions.bidirectionalIn language translation the target is also a sequence of words, in a\nlanguage different from that of the input sequence. Both the input se-\nquence and the target sequence are represented by a structure similar to\nFigure10.12, and they share the hidden units. In this so-called Seq2SeqSeq2Seqlearning, the hidden units are thought to capture the semantic meaning\nof the sentences. Some of the big breakthroughs in language modeling and\ntranslation resulted from the relatively recent improvements in such RNNs.\nAlgorithms used to fit RNNs can be complex and computationally costly.\nFortunately, good software protects users somewhat from these complexi-\nties,andmakesspecifyingandfittingthesemodelsrelativelypainless.Many\nof the models that we enjoy in daily life (like Google Translate ) use state-\nof-the-art architectures developed by teams of highly skilled engineers, and\nhave been trained using massive computational and data resources.\n10.6 When to Use Deep Learning\nThe performance of deep learning in this chapter has been rather impres-\nsive. It nailed the digit classification problem, and deep CNNs have really\nrevolutionized image classification. We see daily reports of new success sto-\nries for deep learning. Many of these are related to image classification\ntasks, such as machine diagnosis of mammograms or digital X-ray images,\nophthalmology eye",
                    " scans, annotations of MRI scans, and so on. Likewise\nthere are numerous successes of RNNs in speech and language translation,\nforecasting, and document modeling. The question that then begs an an-\nswer is:should we discard all our older tools, and use deep learning on every\nproblem with data? To address this question, we revisit our Hittersdataset\nfrom Chapter 6.\nThis is a regression problem, where the goal is to predict the Salaryof\na baseball player in 1987 using his performance statistics from 1986. After\nremoving players with missing responses, we are left with 263 players and\n19 variables. We randomly split the data into a training set of 176 players\n(twothirds),andatestsetof87players(onethird).Weusedthreemethods\nfor fitting a regression model to these data.\n Alinearmodelwasusedtofitthetrainingdata,andmakepredictions\non the test data. The model has 20 parameters.\n The same linear model was fit with lasso regularization. The tuning\nparameter was selected by 10-fold cross-validation on the training\ndata.Itselectedamodelwith12variableshavingnonzerocoefficients.\n A neural network with one hidden layer consisting of 64 ReLUunits\nwas fit to the data. This model has 1,345 parameters.20\n20The model was fit by stochastic gradient descent with a batch size of 32 for 1,000\nepochs, and 10{\\%} dropout regularization. The test error performance flattened out and\nstarted to slowly increase after 1,000 epochs. These fitting details are discussed in Sec-\ntion10.7.\n426 10. Deep Learning\nModel {\\#} Parameters Mean Abs. Error Test Set R2\nLinear Regression 20 254.7 0.56\nLasso 12 252.3 0.51\nNeural Network 1345 257.4 0.54\nTABLE 10.2. Prediction results on the Hitters test data for linear models fit\nby ordinary least squares and lasso, compared to a neural network fit by stochastic\ngradient descent with dropout regularization.\nCoefficient Std. error t-statistic p-value\nIntercept -226.67 86.26 -2.63 0.0103\nHits 3.06 1.02 3.00 0.0036\nWalks 0.181 2.04 0.",
                    "09 0.9294\nCRuns 0.859 0.12 7.09 {<}0.0001\nPutOuts 0.465 0.13 3.60 0.0005\nTABLE 10.3. Least squares coefficient estimates associated with the regres-\nsion ofSalaryon four variables chosen by lasso on the Hitters data set. This\nmodel achieved the best performance on the test data, with a mean absolute error\nof 224.8. The results reported here were obtained from a regression on the test\ndata, which was not used in fitting the lasso model.\nTable10.2compares the results. We see similar performance for all three\nmodels. We report the mean absolute error on the test data, as well as\nthe test R2for each method, which are all respectable (see Exercise 5).\nWe spent a fair bit of time fiddling with the configuration parameters of\nthe neural network to achieve these results. It is possible that if we were to\nspend more time, and got the form and amount of regularization just right,\nthat we might be able to match or even outperform linear regression and\nthe lasso. But with great ease we obtained linear models that work well.\nLinear models are much easier to present and understand than the neural\nnetwork, which is essentially a black box. The lasso selected 12 of the 19\nvariables in making its prediction. So in cases like this we are much better\noff following the Occam`s razor principle: when faced with several methodsOccam`s\nrazorthat give roughly equivalent performance, pick the simplest.\nAfter a bit more exploration with the lasso model, we identified an even\nsimpler model with four variables. We then refit the linear model with these\nfourvariablestothetrainingdata(theso-called relaxed lasso ),andachieved\na test mean absolute error of 224.8, the overall winner! It is tempting to\npresent the summary table from this fit, so we can see coefficients and p-\nvalues; however, since the model was selected on the training data, there\nwould be selection bias . Instead, we refit the model on the test data, which\nwas not used in the selection. Table 10.3shows the results.\nWehaveanumberofverypowerfultoolsatourdisposal,includingneural\nnetworks, random forests and boosting, support vector machines and gen-\neralized additive",
                    " models, to name a few. And then we have linear models,\nand simple variants of these. When faced with new data modeling and pre-\ndiction problems, it`s tempting to always go for the trendy new methods.\nOften they give extremely impressive results, especially when the datasets\nare very large and can support the fitting of high-dimensional nonlinear\nmodels. However, ifwe can produce models with the simpler tools that\n10.7 Fitting a Neural Network 427\nperform as well, they are likely to be easier to fit and understand, and po-\ntentially less fragile than the more complex approaches. Wherever possible,\nit makes sense to try the simpler models as well, and then make a choice\nbased on the performance/complexity tradeoff.\nTypically we expect deep learning to be an attractive choice when the\nsample size of the training set is extremely large, and when interpretability\nof the model is not a high priority.\n10.7 Fitting a Neural Network\nFitting neural networks is somewhat complex, and we give a brief overview\nhere. The ideas generalize to much more complex networks. Readers who\nfind this material challenging can safely skip it. Fortunately, as we see in\nthe lab at the end of this chapter, good software is available to fit neural\nnetwork models in a relatively automated way, without worrying about the\ntechnical details of the model-fitting procedure.\nWe start with the simple network depicted in Figure 10.1in Section 10.1.\nIn model ( 10.1) the parameters are {\\beta}=({\\beta}0,{\\beta}1,...,{\\beta}K), as well as each of\nthewk=(wk0,wk1,...,w kp),k=1,...,K . Givenobservations (xi,yi),i=\n1, . . . , n, wecouldfitthemodelbysolvinganonlinearleastsquaresproblem\nminimize\n{\\{}wk{\\}}K\n1,{\\beta}1\n2n{\\sum}\ni=1(yi{-}f(xi))2, (10.23)\nwhere\nf(xi)={\\beta}0+K{\\sum}\nk=1{\\beta}kg(\nwk0+p{\\sum}\nj=1wkjxij)\n. (10.24)\nThe objective in ( 10.23) looks simple enough, but because of the nested",
                    "\narrangement of the parameters and the symmetry of the hidden units, it is\nnot straightforward to minimize. The problem is nonconvex in the param-\neters, and hence there are multiple solutions. As an example, Figure 10.17\nshows a simple nonconvex function of a single variable {\\theta}; there are two\nsolutions: one is a local minimum and the other is a global minimum . Fur-local\nminimum\nglobal\nminimumthermore, ( 10.1) is the very simplest of neural networks; in this chapter we\nhave presented much more complex ones where these problems are com-\npounded. To overcome some of these issues and to protect from overfitting,\ntwo general strategies are employed when fitting neural networks.\nSlow Learning: the model is fit in a somewhat slow iterative fash-\nion, using gradient descent . The fitting process is then stopped whengradient\ndescentoverfitting is detected.\nRegularization: penaltiesareimposedontheparameters,usuallylasso\nor ridge as discussed in Section 6.2.\nSuppose we represent all the parameters in one long vector {\\theta}. Then we\ncan rewrite the objective in ( 10.23) as\nR({\\theta})=1\n2n{\\sum}\ni=1(yi{-}f{\\theta}(xi))2, (10.25)\n428 10. Deep Learning\nFIGURE 10.17. Illustration of gradient descent for one-dimensional {\\theta}. The\nobjective function R({\\theta})is not convex, and has two minima, one at {\\theta}={-}0.46\n(local), the other at {\\theta}=1.02(global). Starting at some value {\\theta}0(typically ran-\ndomly chosen), each step in {\\theta}moves downhill {\\textemdash} against the gradient {\\textemdash} until it\ncannot go down any further. Here gradient descent reached the global minimum\nin7steps.\nwhere we make explicit the dependence of fon the parameters. The idea\nof gradient descent is very simple.\n1. Start with a guess {\\theta}0for all the parameters in {\\theta}, and set t=0.\n2. Iterate until the objective ( 10.25) fails to decrease:\n(a) Findavector {\\delta}thatreflectsasmallchangein {\\",
                    "theta},suchthat {\\theta}t+1=\n{\\theta}t+{\\delta}reducesthe objective; i.e. such that R({\\theta}t+1){<}R({\\theta}t).\n(b) Sett{\\textleftarrow}t+1.\nOne can visualize (Figure 10.17) standing in a mountainous terrain, and\nthe goal is to get to the bottom through a series of steps. As long as each\nstep goes downhill, we must eventually get to the bottom. In this case we\nwere lucky, because with our starting guess {\\theta}0we end up at the global\nminimum. In general we can hope to end up at a (good) local minimum.\n10.7.1 Backpropagation\nHowdowefindthedirectionstomove {\\theta}soastodecreasetheobjective R({\\theta})\nin (10.25)? Thegradient ofR({\\theta}), evaluated at some current value {\\theta}={\\theta}m,gradientis the vector of partial derivatives at that point:\n{\\nabla}R({\\theta}m)={\\partial}R({\\theta})\n{\\partial}{\\theta}\n{\\theta}={\\theta}m. (10.26)\nThe subscript {\\theta}={\\theta}mmeans that after computing the vector of derivatives,\nwe evaluate it at the current guess, {\\theta}m. This gives the direction in {\\theta}-space\nin which R({\\theta})increases most rapidly. The idea of gradient descent is to\nmove{\\theta}a little in the opposite direction (since we wish to go downhill):\n{\\theta}m+1{\\textleftarrow}{\\theta}m{-}{\\rho}{\\nabla}R({\\theta}m). (10.27){-}1.0 {-}0.5 0.0 0.5 1.00123456\n{\\theta}R({\\theta})\n{\\theta}0{\\theta}1{\\theta}2{\\theta}7/uni25CF/uni25CF/uni25CF/uni25CFR({\\theta}0)R({\\theta}1)R({\\theta}2",
                    ")R({\\theta}7)\n10.7 Fitting a Neural Network 429\nFor a small enough value of the learning rate {\\rho}, this step will decrease thelearning rateobjective R({\\theta}); i.e.R({\\theta}m+1){\\leq}R({\\theta}m). If the gradient vector is zero, then\nwe may have arrived at a minimum of the objective.\nHow complicated is the calculation ( 10.26)? It turns out that it is quite\nsimple here, and remains simple even for much more complex networks,\nbecause of the chain rule of differentiation.chain ruleSinceR({\\theta})={\\sum}n\ni=1Ri({\\theta})=1\n2{\\sum}n\ni=1(yi{-}f{\\theta}(xi))2is a sum, its gradient\nis also a sum over the nobservations, so we will just examine one of these\nterms,\nRi({\\theta})=1\n2(\nyi{-}{\\beta}0{-}K{\\sum}\nk=1{\\beta}kg(\nwk0+p{\\sum}\nj=1wkjxij))2\n. (10.28)\nTo simplify the expressions to follow, we write zik=wk0+{\\sum}p\nj=1wkjxij.\nFirst we take the derivative with respect to {\\beta}k:\n{\\partial}Ri({\\theta})\n{\\partial}{\\beta}k={\\partial}Ri({\\theta})\n{\\partial}f{\\theta}(xi){\\textperiodcentered}{\\partial}f{\\theta}(xi)\n{\\partial}{\\beta}k\n={-}(yi{-}f{\\theta}(xi)){\\textperiodcentered}g(zik). (10.29)\nAnd now we take the derivative with respect to wkj:\n{\\partial}Ri({\\theta})\n{\\partial}wkj={\\partial}Ri({\\theta})\n{\\partial}f{\\theta}(xi){\\textperiodcentered}{\\partial}f{\\theta}(xi)\n{\\partial}g(zik){\\textperiodcentered}{\\partial}g(zik)\n{\\partial}zik{\\textperiodcentered}{\\partial}zik",
                    "\n{\\partial}wkj\n={-}(yi{-}f{\\theta}(xi)){\\textperiodcentered}{\\beta}k{\\textperiodcentered}g{'}(zik){\\textperiodcentered}xij. (10.30)\nNotice that both these expressions contain the residual yi{-}f{\\theta}(xi). In\n(10.29) we see that a fraction of that residual gets attributed to each of\nthe hidden units according to the value of g(zik). Then in ( 10.30) we see\na similar attribution to input jvia hidden unit k. So the act of differen-\ntiation assigns a fraction of the residual to each of the parameters via the\nchain rule {\\textemdash} a process known as backpropagation in the neural networkbackprop-\nagationliterature. Although these calculations are straightforward, it takes careful\nbookkeeping to keep track of all the pieces.\n10.7.2 Regularization and Stochastic Gradient Descent\nGradient descent usually takes many steps to reach a local minimum. In\npractice, there are a number of approaches for accelerating the process.\nAlso, when nis large, instead of summing ( 10.29){\\textendash}(10.30) over all nob-\nservations, we can sample a small fraction or minibatch of them each timeminibatchwe compute a gradient step. This process is known as stochastic gradient\ndescent(SGD) and is the state of the art for learning deep neural networks.stochastic\ngradient\ndescentFortunately, there is very good software for setting up deep learning mod-\nels, and for fitting them to data, so most of the technicalities are hidden\nfrom the user.\nWe now turn to the multilayer network (Figure 10.4) used in the digit\nrecognitionproblem.Thenetworkhasover235,000weights,whichisaround\nfour times the number of training examples. Regularization is essential here\n430 10. Deep Learning\n0 5 10 15 20 25 300.1 0.2 0.3 0.4EpochsValue of Objective FunctionTraining SetValidation Set\n0 5 10 15 20 25 300.00 0.02 0.04 0.06 0.08 0.10 0.12EpochsClassification ErrorFIGURE 10.18. Evolution of training and validation errors for the MNISTne",
                    "ural\nnetwork depicted in Figure 10.4, as a function of training epochs. The objective\nrefers to the log-likelihood ( 10.14).\nto avoid overfitting. The first row in Table 10.1uses ridge regularization on\nthe weights. This is achieved by augmenting the objective function ( 10.14)\nwith a penalty term:\nR({\\theta};{\\lambda})={-}n{\\sum}\ni=19{\\sum}\nm=0yimlog(fm(xi)) + {\\lambda}{\\sum}\nj{\\theta}2\nj. (10.31)\nThe parameter {\\lambda}is often preset at a small value, or else it is found using the\nvalidation-set approach of Section 5.3.1. We can also use different values of\n{\\lambda}for the groups of weights from different layers; in this case W1andW2\nwere penalized, while the relatively few weights Bof the output layer were\nnot penalized at all. Lasso regularization is also popular as an additional\nform of regularization, or as an alternative to ridge.\nFigure10.18shows some metrics that evolve during the training of the\nnetwork on the MNISTdata. It turns out that SGD naturally enforces its\nown form of approximately quadratic regularization.21Here the minibatch\nsizewas128observationspergradientupdate.Theterm epochslabelingtheepochshorizontal axis in Figure 10.18counts the number of times an equivalent of\nthe full training set has been processed. For this network, 20{\\%} of the 60,000\ntraining observations were used as a validation set in order to determine\nwhen training should stop. So in fact 48,000 observations were used for\ntraining, and hence there are 48,000/128{\\approx}375minibatch gradient updates\nper epoch. We see that the value of the validation objective actually starts\nto increase by 30 epochs, so early stopping can also be used as an additionalearly\nstoppingform of regularization.\n21This and other properties of SGD for deep learning are the subject of much research\nin the machine learning literature at the time of writing.\n10.7 Fitting a Neural Network 431\nFIGURE 10.19. Dropout Learning. Left: a fully connected network. Right: net-\nwork with dropout in the input and",
                    " hidden layer. The nodes in grey are selected\nat random, and ignored in an instance of training.\n10.7.3 Dropout Learning\nThe second row in Table 10.1is labeled dropout. This is a relatively newdropoutand efficient form of regularization, similar in some respects to ridge reg-\nularization. Inspired by random forests (Section 8.2), the idea is to ran-\ndomly remove a fraction {\\varphi}of the units in a layer when fitting the model.\nFigure10.19illustrates this. This is done separately each time a training\nobservation is processed. The surviving units stand in for those missing,\nand their weights are scaled up by a factor of 1/(1{-}{\\varphi})to compensate.\nThis prevents nodes from becoming over-specialized, and can be seen as\na form of regularization. In practice dropout is achieved by randomly set-\nting the activations for the {\\textquotedblleft}dropped out{\\textquotedblright} units to zero, while keeping the\narchitecture intact.\n10.7.4 Network Tuning\nThe network in Figure 10.4is considered to be relatively straightforward;\nit nevertheless requires a number of choices that all have an effect on the\nperformance:\nThe number of hidden layers, and the number of units per layer.\nModern thinking is that the number of units per hidden layer can\nbe large, and overfitting can be controlled via the various forms of\nregularization.\nRegularization tuning parameters. These include the dropout rate {\\varphi}\nand the strength {\\lambda}of lasso and ridge regularization, and are typically\nset separately at each layer.\nDetails of stochastic gradient descent. These include the batch size,\nthe number of epochs, and if used, details of data augmentation (Sec-\ntion10.3.4.)\nChoices such as these can make a difference. In preparing this MNISTexam-\nple, we achieved a respectable 1.8{\\%}misclassification error after some trial\nand error. Finer tuning and training of a similar network can get under\n1{\\%}error on these data, but the tinkering process can be tedious, and can\nresult in overfitting if done carelessly."
                ]
            }
        ]
    }
}