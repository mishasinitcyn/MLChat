146 Vector Calculus
such that the derivative of his given as
h{'}(x) =g{'}(f)f{'}(x) = (4 f3){\textperiodcentered}2(5.34)= 4(2 x+ 1)3{\textperiodcentered}2 = 8(2 x+ 1)3,(5.38)
where we used the chain rule (5.32) and substituted the definition of f
in (5.34) in g{'}(f).
5.2 Partial Differentiation and Gradients
Differentiation as discussed in Section 5.1 applies to functions fof a
scalar variable x{\in}R. In the following, we consider the general case
where the function fdepends on one or more variables x{\in}Rn, e.g.,
f(x) =f(x1, x2). The generalization of the derivative to functions of sev-
eral variables is the gradient .
We find the gradient of the function fwith respect to xbyvarying one
variable at a time and keeping the others constant. The gradient is then
the collection of these partial derivatives .
Definition 5.5 (Partial Derivative) .For a function f:Rn{\textrightarrow}R,x7{\textrightarrow}
f(x),x{\in}Rnofnvariables x1, . . . , x nwe define the partial derivatives as partial derivative
{\partial}f
{\partial}x1= lim
h{\textrightarrow}0f(x1+h, x 2, . . . , x n){-}f(x)
h
...
{\partial}f
{\partial}xn= lim
h{\textrightarrow}0f(x1, . . . , x n{-}1, xn+h){-}f(x)
h(5.39)
and collect them in the row vector
{\nabla}xf= grad f=df
dx={\partial}f(x)
{\partial}x1{\partial}f(x)
{\partial}x2{\textperiodcentered}{\textperiodcentered}{\textperiodcentered}{\partial}f(x)
{\partial}xn
{\in}R1{\texttimes}n,(5.40)
where nis the number of variables and 1is the dimension of the image/
range/codomain of f. Here, we defined the column vector x= [x1, . . . , x n]{\top}
{\in}Rn. The row vector in (5.40) is called the gradient offor the Jacobian gradient
Jacobian and is the generalization of the derivative from Section 5.1.
Remark. This definition of the Jacobian is a special case of the general
definition of the Jacobian for vector-valued functions as the collection of
partial derivatives. We will get back to this in Section 5.3. {\diamond}We can use results
from scalar
differentiation: Each
partial derivative is
a derivative with
respect to a scalar.Example 5.6 (Partial Derivatives Using the Chain Rule)
Forf(x, y) = (x+ 2y3)2, we obtain the partial derivatives
{\partial}f(x, y)
{\partial}x= 2(x+ 2y3){\partial}
{\partial}x(x+ 2y3) = 2( x+ 2y3), (5.41)
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
5.2 Partial Differentiation and Gradients 147
{\partial}f(x, y)
{\partial}y= 2(x+ 2y3){\partial}
{\partial}y(x+ 2y3) = 12( x+ 2y3)y2. (5.42)
where we used the chain rule (5.32) to compute the partial derivatives.
Remark (Gradient as a Row Vector) .It is not uncommon in the literature
to define the gradient vector as a column vector, following the conven-
tion that vectors are generally column vectors. The reason why we define
the gradient vector as a row vector is twofold: First, we can consistently
generalize the gradient to vector-valued functions f:Rn{\textrightarrow}Rm(then
the gradient becomes a matrix). Second, we can immediately apply the
multi-variate chain rule without paying attention to the dimension of the
gradient. We will discuss both points in Section 5.3. {\diamond}
Example 5.7 (Gradient)
Forf(x1, x2) =x2
1x2+x1x3
2{\in}R, the partial derivatives (i.e., the deriva-
tives of fwith respect to x1andx2) are
{\partial}f(x1, x2)
{\partial}x1= 2x1x2+x3
2 (5.43)
{\partial}f(x1, x2)
{\partial}x2=x2
1+ 3x1x2
2 (5.44)
and the gradient is then
df
dx={\partial}f(x1, x2)
{\partial}x1{\partial}f(x1, x2)
{\partial}x2
=2x1x2+x3
2x2
1+ 3x1x2
2{\in}R1{\texttimes}2.
(5.45)
5.2.1 Basic Rules of Partial Differentiation
Product rule:
(fg){'}=f{'}g+fg{'},
Sum rule:
(f+g){'}=f{'}+g{'},
Chain rule:
(g(f)){'}=g{'}(f)f{'}In the multivariate case, where x{\in}Rn, the basic differentiation rules that
we know from school (e.g., sum rule, product rule, chain rule; see also
Section 5.1.2) still apply. However, when we compute derivatives with re-
spect to vectors x{\in}Rnwe need to pay attention: Our gradients now
involve vectors and matrices, and matrix multiplication is not commuta-
tive (Section 2.2.1), i.e., the order matters.
Here are the general product rule, sum rule, and chain rule:
Product rule:{\partial}
{\partial}xf(x)g(x)={\partial}f
{\partial}xg(x) +f(x){\partial}g
{\partial}x(5.46)
Sum rule:{\partial}
{\partial}xf(x) +g(x)={\partial}f
{\partial}x+{\partial}g
{\partial}x(5.47)
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
148 Vector Calculus
Chain rule:{\partial}
{\partial}x(g{\textopenbullet}f)(x) ={\partial}
{\partial}xg(f(x))={\partial}g
{\partial}f{\partial}f
{\partial}x(5.48)
Let us have a closer look at the chain rule. The chain rule (5.48) resem- This is only an
intuition, but not
mathematically
correct since the
partial derivative is
not a fraction.bles to some degree the rules for matrix multiplication where we said that
neighboring dimensions have to match for matrix multiplication to be de-
fined; see Section 2.2.1. If we go from left to right, the chain rule exhibits
similar properties: {\partial}fshows up in the {\textquotedblleft}denominator{\textquotedblright} of the first factor
and in the {\textquotedblleft}numerator{\textquotedblright} of the second factor. If we multiply the factors to-
gether, multiplication is defined, i.e., the dimensions of {\partial}fmatch, and {\partial}f
{\textquotedblleft}cancels{\textquotedblright}, such that {\partial}g/{\partial}xremains.
5.2.2 Chain Rule
Consider a function f:R2{\textrightarrow}Rof two variables x1, x2. Furthermore,
x1(t)andx2(t)are themselves functions of t. To compute the gradient of
fwith respect to t, we need to apply the chain rule (5.48) for multivariate
functions as
df
dt=h
{\partial}f
{\partial}x1{\partial}f
{\partial}x2i{''}
{\partial}x1(t)
{\partial}t{\partial}x2(t)
{\partial}t{\#}
={\partial}f
{\partial}x1{\partial}x1
{\partial}t+{\partial}f
{\partial}x2{\partial}x2
{\partial}t, (5.49)
where ddenotes the gradient and {\partial}partial derivatives.
Example 5.8
Consider f(x1, x2) =x2
1+ 2x2, where x1= sin tandx2= cos t, then
df
dt={\partial}f
{\partial}x1{\partial}x1
{\partial}t+{\partial}f
{\partial}x2{\partial}x2
{\partial}t(5.50a)
= 2 sin t{\partial}sint
{\partial}t+ 2{\partial}cost
{\partial}t(5.50b)
= 2 sin tcost{-}2 sint= 2 sin t(cost{-}1) (5.50c)
is the corresponding derivative of fwith respect to t.
Iff(x1, x2)is a function of x1andx2, where x1(s, t)andx2(s, t)are
themselves functions of two variables sandt, the chain rule yields the
partial derivatives
{\partial}f
{\partial}s={\partial}f
{\partial}x1{\partial}x1
{\partial}s+{\partial}f
{\partial}x2{\partial}x2
{\partial}s, (5.51)
{\partial}f
{\partial}t={\partial}f
{\partial}x1{\partial}x1
{\partial}t+{\partial}f
{\partial}x2{\partial}x2
{\partial}t, (5.52)
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
5.3 Gradients of Vector-Valued Functions 149
and the gradient is obtained by the matrix multiplication
df
d(s, t)={\partial}f
{\partial}x{\partial}x
{\partial}(s, t)=h{\partial}f
{\partial}x1{\partial}f
{\partial}x2i
|{\{}z{\}}
={\partial}f
{\partial}x
{\partial}x1
{\partial}s{\partial}x1
{\partial}t
{\partial}x2
{\partial}s{\partial}x2
{\partial}t

|{\{}z {\}}
={\partial}x
{\partial}(s, t). (5.53)
This compact way of writing the chain rule as a matrix multiplication only The chain rule can
be written as a
matrix
multiplication.makes sense if the gradient is defined as a row vector. Otherwise, we will
need to start transposing gradients for the matrix dimensions to match.
This may still be straightforward as long as the gradient is a vector or a
matrix; however, when the gradient becomes a tensor (we will discuss this
in the following), the transpose is no longer a triviality.
Remark (Verifying the Correctness of a Gradient Implementation) .The
definition of the partial derivatives as the limit of the corresponding dif-
ference quotient (see (5.39)) can be exploited when numerically checking
the correctness of gradients in computer programs: When we compute Gradient checking
gradients and implement them, we can use finite differences to numer-
ically test our computation and implementation: We choose the value h
to be small (e.g., h= 10{-}4) and compare the finite-difference approxima-
tion from (5.39) with our (analytic) implementation of the gradient. If the
error is small, our gradient implementation is probably correct. {\textquotedblleft}Small{\textquotedblright}
could mean thatqP
i(dhi{-}d fi)2
P
i(dhi+d fi)2{<}10{-}6, where dhiis the finite-difference
approximation and d fiis the analytic gradient of fwith respect to the ith
variable xi. {\diamond}
5.3 Gradients of Vector-Valued Functions
Thus far, we discussed partial derivatives and gradients of functions f:
Rn{\textrightarrow}Rmapping to the real numbers. In the following, we will generalize
the concept of the gradient to vector-valued functions (vector fields) f:
Rn{\textrightarrow}Rm, where n{\geqslant}1andm {>} 1.
For a function f:Rn{\textrightarrow}Rmand a vector x= [x1, . . . , x n]{\top}{\in}Rn, the
corresponding vector of function values is given as
f(x) =
f1(x)
...
fm(x)
{\in}Rm. (5.54)
Writing the vector-valued function in this way allows us to view a vector-
valued function f:Rn{\textrightarrow}Rmas a vector of functions [f1, . . . , f m]{\top},
fi:Rn{\textrightarrow}Rthat map onto R. The differentiation rules for every fiare
exactly the ones we discussed in Section 5.2.
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).