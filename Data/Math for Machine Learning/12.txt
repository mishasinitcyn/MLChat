12
Classification with Support Vector Machines
In many situations, we want our machine learning algorithm to predict
one of a number of (discrete) outcomes. For example, an email client sorts
mail into personal mail and junk mail, which has two outcomes. Another
example is a telescope that identifies whether an object in the night sky
is a galaxy, star, or planet. There are usually a small number of outcomes,
and more importantly there is usually no additional structure on these
outcomes. In this chapter, we consider predictors that output binary val- An example of
structure is if the
outcomes were
ordered, like in the
case of small,
medium, and large
t-shirts.ues, i.e., there are only two possible outcomes. This machine learning task
is called binary classification . This is in contrast to Chapter 9, where we
binary classificationconsidered a prediction problem with continuous-valued outputs.
For binary classification, the set of possible values that the label/output
can attain is binary, and for this chapter we denote them by {\{}+1,{-}1{\}}. In
other words, we consider predictors of the form
f:RD{\textrightarrow} {\{}+1,{-}1{\}}. (12.1)
Recall from Chapter 8 that we represent each example (data point) xn
as a feature vector of Dreal numbers. The labels are often referred to as Input example xn
may also be referred
to as inputs, data
points, features, or
instances.the positive and negative classes , respectively. One should be careful not
classto infer intuitive attributes of positiveness of the +1class. For example,
in a cancer detection task, a patient with cancer is often labeled +1. In
principle, any two distinct values can be used, e.g., {\{}True,False{\}},{\{}0,1{\}}
or{\{}red,blue{\}}. The problem of binary classification is well studied, and For probabilistic
models, it is
mathematically
convenient to use
{\{}0,1{\}}as a binary
representation; see
the remark after
Example 6.12.we defer a survey of other approaches to Section 12.6.
We present an approach known as the support vector machine (SVM),
which solves the binary classification task. As in regression, we have a su-
pervised learning task, where we have a set of examples xn{\in}RDalong
with their corresponding (binary) labels yn{\in} {\{}+1,{-}1{\}}. Given a train-
ing data set consisting of example{\textendash}label pairs {\{}(x1, y1), . . . , (xN, yN){\}}, we
would like to estimate parameters of the model that will give the smallest
classification error. Similar to Chapter 9, we consider a linear model, and
hide away the nonlinearity in a transformation {\phi}of the examples (9.13).
We will revisit {\phi}in Section 12.4.
The SVM provides state-of-the-art results in many applications, with
sound theoretical guarantees (Steinwart and Christmann, 2008). There
are two main reasons why we chose to illustrate binary classification using
370
This material is published by Cambridge University Press as Mathematics for Machine Learning by
Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view
and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.
{\textcopyright}by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024. https://mml-book.com .
Classification with Support Vector Machines 371
Figure 12.1
Example 2D data,
illustrating the
intuition of data
where we can find a
linear classifier that
separates orange
crosses from blue
discs.
x(1)x(2)
SVMs. First, the SVM allows for a geometric way to think about supervised
machine learning. While in Chapter 9 we considered the machine learning
problem in terms of probabilistic models and attacked it using maximum
likelihood estimation and Bayesian inference, here we will consider an
alternative approach where we reason geometrically about the machine
learning task. It relies heavily on concepts, such as inner products and
projections, which we discussed in Chapter 3. The second reason why we
find SVMs instructive is that in contrast to Chapter 9, the optimization
problem for SVM does not admit an analytic solution so that we need to
resort to a variety of optimization tools introduced in Chapter 7.
The SVM view of machine learning is subtly different from the max-
imum likelihood view of Chapter 9. The maximum likelihood view pro-
poses a model based on a probabilistic view of the data distribution, from
which an optimization problem is derived. In contrast, the SVM view starts
by designing a particular function that is to be optimized during training,
based on geometric intuitions. We have seen something similar already
in Chapter 10, where we derived PCA from geometric principles. In the
SVM case, we start by designing a loss function that is to be minimized
on training data, following the principles of empirical risk minimization
(Section 8.2).
Let us derive the optimization problem corresponding to training an
SVM on example{\textendash}label pairs. Intuitively, we imagine binary classification
data, which can be separated by a hyperplane as illustrated in Figure 12.1.
Here, every example xn(a vector of dimension 2) is a two-dimensional
location ( x(1)
nandx(2)
n), and the corresponding binary label ynis one of
two different symbols (orange cross or blue disc). {\textquotedblleft}Hyperplane{\textquotedblright} is a word
that is commonly used in machine learning, and we encountered hyper-
planes already in Section 2.8. A hyperplane is an affine subspace of di-
mension D{-}1(if the corresponding vector space is of dimension D).
The examples consist of two classes (there are two possible labels) that
have features (the components of the vector representing the example)
arranged in such a way as to allow us to separate/classify them by draw-
ing a straight line.
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
372 Classification with Support Vector Machines
In the following, we formalize the idea of finding a linear separator
of the two classes. We introduce the idea of the margin and then extend
linear separators to allow for examples to fall on the {\textquotedblleft}wrong{\textquotedblright} side, incur-
ring a classification error. We present two equivalent ways of formalizing
the SVM: the geometric view (Section 12.2.4) and the loss function view
(Section 12.2.5). We derive the dual version of the SVM using Lagrange
multipliers (Section 7.2). The dual SVM allows us to observe a third way
of formalizing the SVM: in terms of the convex hulls of the examples of
each class (Section 12.3.2). We conclude by briefly describing kernels and
how to numerically solve the nonlinear kernel-SVM optimization problem.
12.1 Separating Hyperplanes
Given two examples represented as vectors xiandxj, one way to compute
the similarity between them is using an inner product {\langle}xi,xj{\rangle}. Recall from
Section 3.2 that inner products are closely related to the angle between
two vectors. The value of the inner product between two vectors depends
on the length (norm) of each vector. Furthermore, inner products allow
us to rigorously define geometric concepts such as orthogonality and pro-
jections.
The main idea behind many classification algorithms is to represent
data in RDand then partition this space, ideally in a way that examples
with the same label (and no other examples) are in the same partition.
In the case of binary classification, the space would be divided into two
parts corresponding to the positive and negative classes, respectively. We
consider a particularly convenient partition, which is to (linearly) split
the space into two halves using a hyperplane. Let example x{\in}RDbe an
element of the data space. Consider a function
f:RD{\textrightarrow}R (12.2a)
x7{\textrightarrow}f(x) :={\langle}w,x{\rangle}+b , (12.2b)
parametrized by w{\in}RDandb{\in}R. Recall from Section 2.8 that hy-
perplanes are affine subspaces. Therefore, we define the hyperplane that
separates the two classes in our binary classification problem as
x{\in}RD:f(x) = 0	. (12.3)
An illustration of the hyperplane is shown in Figure 12.2, where the
vector wis a vector normal to the hyperplane and bthe intercept. We can
derive that wis a normal vector to the hyperplane in (12.3) by choosing
any two examples xaandxbon the hyperplane and showing that the
vector between them is orthogonal to w. In the form of an equation,
f(xa){-}f(xb) ={\langle}w,xa{\rangle}+b{-}({\langle}w,xb{\rangle}+b) (12.4a)
={\langle}w,xa{-}xb{\rangle}, (12.4b)
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
12.1 Separating Hyperplanes 373
Figure 12.2
Equation of a
separating
hyperplane (12.3).
(a) The standard
way of representing
the equation in 3D.
(b) For ease of
drawing, we look at
the hyperplane edge
on.w
(a) Separating hyperplane in 3Dw
.
0.Positive
.
Negativeb
(b) Projection of the setting in (a) onto
a plane
where the second line is obtained by the linearity of the inner product
(Section 3.2). Since we have chosen xaandxbto be on the hyperplane,
this implies that f(xa) = 0 andf(xb) = 0 and hence {\langle}w,xa{-}xb{\rangle}= 0.
Recall that two vectors are orthogonal when their inner product is zero. wis orthogonal to
any vector on the
hyperplane.Therefore, we obtain that wis orthogonal to any vector on the hyperplane.
Remark. Recall from Chapter 2 that we can think of vectors in different
ways. In this chapter, we think of the parameter vector was an arrow
indicating a direction, i.e., we consider wto be a geometric vector. In
contrast, we think of the example vector xas a data point (as indicated
by its coordinates), i.e., we consider xto be the coordinates of a vector
with respect to the standard basis. {\diamond}
When presented with a test example, we classify the example as pos-
itive or negative depending on the side of the hyperplane on which it
occurs. Note that (12.3) not only defines a hyperplane; it additionally de-
fines a direction. In other words, it defines the positive and negative side
of the hyperplane. Therefore, to classify a test example xtest, we calcu-
late the value of the function f(xtest)and classify the example as +1if
f(xtest){\geqslant}0and{-}1otherwise. Thinking geometrically, the positive ex-
amples lie {\textquotedblleft}above{\textquotedblright} the hyperplane and the negative examples {\textquotedblleft}below{\textquotedblright} the
hyperplane.
When training the classifier, we want to ensure that the examples with
positive labels are on the positive side of the hyperplane, i.e.,
{\langle}w,xn{\rangle}+b{\geqslant}0 when yn= +1 (12.5)
and the examples with negative labels are on the negative side, i.e.,
{\langle}w,xn{\rangle}+b {<}0 when yn={-}1. (12.6)
Refer to Figure 12.2 for a geometric intuition of positive and negative
examples. These two conditions are often presented in a single equation
yn({\langle}w,xn{\rangle}+b){\geqslant}0. (12.7)
Equation (12.7) is equivalent to (12.5) and (12.6) when we multiply both
sides of (12.5) and (12.6) with yn= 1andyn={-}1, respectively.
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
374 Classification with Support Vector Machines
Figure 12.3
Possible separating
hyperplanes. There
are many linear
classifiers (green
lines) that separate
orange crosses from
blue discs.
x(1)x(2)
12.2 Primal Support Vector Machine
Based on the concept of distances from points to a hyperplane, we now
are in a position to discuss the support vector machine. For a dataset
{\{}(x1, y1), . . . , (xN, yN){\}}that is linearly separable, we have infinitely many
candidate hyperplanes (refer to Figure 12.3), and therefore classifiers,
that solve our classification problem without any (training) errors. To find
a unique solution, one idea is to choose the separating hyperplane that
maximizes the margin between the positive and negative examples. In
other words, we want the positive and negative examples to be separated
by a large margin (Section 12.2.1). In the following, we compute the dis- A classifier with
large margin turns
out to generalize
well (Steinwart and
Christmann, 2008).tance between an example and a hyperplane to derive the margin. Recall
that the closest point on the hyperplane to a given point (example xn) is
obtained by the orthogonal projection (Section 3.8).
12.2.1 Concept of the Margin
The concept of the margin is intuitively simple: It is the distance of the margin
separating hyperplane to the closest examples in the dataset, assuming There could be two
or more closest
examples to a
hyperplane.that the dataset is linearly separable. However, when trying to formalize
this distance, there is a technical wrinkle that may be confusing. The tech-
nical wrinkle is that we need to define a scale at which to measure the
distance. A potential scale is to consider the scale of the data, i.e., the raw
values of xn. There are problems with this, as we could change the units
of measurement of xnand change the values in xn, and, hence, change
the distance to the hyperplane. As we will see shortly, we define the scale
based on the equation of the hyperplane (12.3) itself.
Consider a hyperplane {\langle}w,x{\rangle}+b, and an example xaas illustrated in
Figure 12.4. Without loss of generality, we can consider the example xa
to be on the positive side of the hyperplane, i.e., {\langle}w,xa{\rangle}+b {>}0. We
would like to compute the distance r {>}0ofxafrom the hyperplane. We
do so by considering the orthogonal projection (Section 3.8) of xaonto
the hyperplane, which we denote by x{'}
a. Since wis orthogonal to the
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
12.2 Primal Support Vector Machine 375
Figure 12.4 Vector
addition to express
distance to
hyperplane:
xa=x{'}
a+rw
{\parallel}w{\parallel}.
.0.xa
w.x{'}
ar
hyperplane, we know that the distance ris just a scaling of this vector w.
If the length of wis known, then we can use this scaling factor rfactor
to work out the absolute distance between xaandx{'}
a. For convenience,
we choose to use a vector of unit length (its norm is 1) and obtain this
by dividing wby its norm,w
{\parallel}w{\parallel}. Using vector addition (Section 2.4), we
obtain
xa=x{'}
a+rw
{\parallel}w{\parallel}. (12.8)
Another way of thinking about ris that it is the coordinate of xain the
subspace spanned by w/{\parallel}w{\parallel}. We have now expressed the distance of xa
from the hyperplane as r, and if we choose xato be the point closest to
the hyperplane, this distance ris the margin.
Recall that we would like the positive examples to be further than r
from the hyperplane, and the negative examples to be further than dis-
tance r(in the negative direction) from the hyperplane. Analogously to
the combination of (12.5) and (12.6) into (12.7), we formulate this ob-
jective as
yn({\langle}w,xn{\rangle}+b){\geqslant}r . (12.9)
In other words, we combine the requirements that examples are at least
raway from the hyperplane (in the positive and negative direction) into
one single inequality.
Since we are interested only in the direction, we add an assumption to
our model that the parameter vector wis of unit length, i.e., {\parallel}w{\parallel}= 1,
where we use the Euclidean norm {\parallel}w{\parallel}={\sqrt{}}
w{\top}w(Section 3.1). This We will see other
choices of inner
products
(Section 3.2) in
Section 12.4.assumption also allows a more intuitive interpretation of the distance r
(12.8) since it is the scaling factor of a vector of length 1.
Remark. A reader familiar with other presentations of the margin would
notice that our definition of {\parallel}w{\parallel}= 1 is different from the standard
presentation if the SVM was the one provided by Sch {\textasciidieresis}olkopf and Smola
(2002), for example. In Section 12.2.3, we will show the equivalence of
both approaches. {\diamond}
Collecting the three requirements into a single constrained optimization
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
376 Classification with Support Vector Machines
Figure 12.5
Derivation of the
margin: r=1
{\parallel}w{\parallel}..xa
w
{\langle}w,x{\rangle}+
b= 0{\langle}w,x{\rangle}+
b= 1.x{'}
ar
problem, we obtain the objective
max
w,b,rr|{\{}z{\}}
margin
subject to yn({\langle}w,xn{\rangle}+b){\geqslant}r| {\{}z {\}}
data fitting,{\parallel}w{\parallel}= 1|{\{}z{\}}
normalization, r {>} 0,(12.10)
which says that we want to maximize the margin rwhile ensuring that
the data lies on the correct side of the hyperplane.
Remark. The concept of the margin turns out to be highly pervasive in ma-
chine learning. It was used by Vladimir Vapnik and Alexey Chervonenkis
to show that when the margin is large, the {\textquotedblleft}complexity{\textquotedblright} of the function
class is low, and hence learning is possible (Vapnik, 2000). It turns out
that the concept is useful for various different approaches for theoret-
ically analyzing generalization error (Steinwart and Christmann, 2008;
Shalev-Shwartz and Ben-David, 2014). {\diamond}
12.2.2 Traditional Derivation of the Margin
In the previous section, we derived (12.10) by making the observation that
we are only interested in the direction of wand not its length, leading to
the assumption that {\parallel}w{\parallel}= 1. In this section, we derive the margin max-
imization problem by making a different assumption. Instead of choosing
that the parameter vector is normalized, we choose a scale for the data.
We choose this scale such that the value of the predictor {\langle}w,x{\rangle}+bis1at
the closest example. Let us also denote the example in the dataset that is Recall that we
currently consider
linearly separable
data.closest to the hyperplane by xa.
Figure 12.5 is identical to Figure 12.4, except that now we rescaled the
axes, such that the example xalies exactly on the margin, i.e., {\langle}w,xa{\rangle}+
b= 1. Since x{'}
ais the orthogonal projection of xaonto the hyperplane, it
must by definition lie on the hyperplane, i.e.,
{\langle}w,x{'}
a{\rangle}+b= 0. (12.11)
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
12.2 Primal Support Vector Machine 377
By substituting (12.8) into (12.11), we obtain

w,xa{-}rw
{\parallel}w{\parallel}
+b= 0. (12.12)
Exploiting the bilinearity of the inner product (see Section 3.2), we get
{\langle}w,xa{\rangle}+b{-}r{\langle}w,w{\rangle}
{\parallel}w{\parallel}= 0. (12.13)
Observe that the first term is 1by our assumption of scale, i.e., {\langle}w,xa{\rangle}+
b= 1. From (3.16) in Section 3.1, we know that {\langle}w,w{\rangle}={\parallel}w{\parallel}2. Hence,
the second term reduces to r{\parallel}w{\parallel}. Using these simplifications, we obtain
r=1
{\parallel}w{\parallel}. (12.14)
This means we derived the distance rin terms of the normal vector w
of the hyperplane. At first glance, this equation is counterintuitive as we We can also think of
the distance as the
projection error that
incurs when
projecting xaonto
the hyperplane.seem to have derived the distance from the hyperplane in terms of the
length of the vector w, but we do not yet know this vector. One way to
think about it is to consider the distance rto be a temporary variable
that we only use for this derivation. Therefore, for the rest of this section
we will denote the distance to the hyperplane by1
{\parallel}w{\parallel}. In Section 12.2.3,
we will see that the choice that the margin equals 1is equivalent to our
previous assumption of {\parallel}w{\parallel}= 1in Section 12.2.1.
Similar to the argument to obtain (12.9), we want the positive and
negative examples to be at least 1away from the hyperplane, which yields
the condition
yn({\langle}w, xn{\rangle}+b){\geqslant}1. (12.15)
Combining the margin maximization with the fact that examples need to
be on the correct side of the hyperplane (based on their labels) gives us
max
w,b1
{\parallel}w{\parallel}(12.16)
subject to yn({\langle}w,xn{\rangle}+b){\geqslant}1 for all n= 1, . . . , N. (12.17)
Instead of maximizing the reciprocal of the norm as in (12.16), we often
minimize the squared norm. We also often include a constant1
2that does The squared norm
results in a convex
quadratic
programming
problem for the
SVM (Section 12.5).not affect the optimal w, bbut yields a tidier form when we compute the
gradient. Then, our objective becomes
min
w,b1
2{\parallel}w{\parallel}2(12.18)
subject to yn({\langle}w,xn{\rangle}+b){\geqslant}1 for all n= 1, . . . , N . (12.19)
Equation (12.18) is known as the hard margin SVM . The reason for the hard margin SVM
expression {\textquotedblleft}hard{\textquotedblright} is because the formulation does not allow for any vi-
olations of the margin condition. We will see in Section 12.2.4 that this
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
378 Classification with Support Vector Machines
{\textquotedblleft}hard{\textquotedblright} condition can be relaxed to accommodate violations if the data is
not linearly separable.
12.2.3 Why We Can Set the Margin to 1
In Section 12.2.1, we argued that we would like to maximize some value
r, which represents the distance of the closest example to the hyperplane.
In Section 12.2.2, we scaled the data such that the closest example is of
distance 1to the hyperplane. In this section, we relate the two derivations,
and show that they are equivalent.
Theorem 12.1. Maximizing the margin r, where we consider normalized
weights as in (12.10) ,
max
w,b,rr|{\{}z{\}}
margin
subject to yn({\langle}w,xn{\rangle}+b){\geqslant}r| {\{}z {\}}
data fitting,{\parallel}w{\parallel}= 1|{\{}z{\}}
normalization, r {>} 0,(12.20)
is equivalent to scaling the data, such that the margin is unity:
min
w,b1
2{\parallel}w{\parallel}2
|{\{}z{\}}
margin
subject to yn({\langle}w,xn{\rangle}+b){\geqslant}1| {\{}z {\}}
data fitting.(12.21)
Proof Consider (12.20). Since the square is a strictly monotonic trans-
formation for non-negative arguments, the maximum stays the same if we
consider r2in the objective. Since {\parallel}w{\parallel}= 1 we can reparametrize the
equation with a new weight vector w{'}that is not normalized by explicitly
usingw{'}
{\parallel}w{'}{\parallel}. We obtain
max
w{'},b,rr2
subject to ynw{'}
{\parallel}w{'}{\parallel},xn
+b
{\geqslant}r, r {>} 0.(12.22)
Equation (12.22) explicitly states that the distance ris positive. Therefore,
we can divide the first constraint by r, which yields Note that r {>}0
because we
assumed linear
separability, and
hence there is no
issue to divide by r.max
w{'},b,rr2
subject to yn
*
w{'}
{\parallel}w{'}{\parallel}r|{\{}z{\}}
w{'}{'},xn+
+b
r|{\{}z{\}}
b{'}{'}
{\geqslant}1, r {>} 0(12.23)
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
12.2 Primal Support Vector Machine 379
Figure 12.6
(a) Linearly
separable and
(b) non-linearly
separable data.
x(1)x(2)
(a) Linearly separable data, with a large
margin
x(1)x(2)(b) Non-linearly separable data
renaming the parameters to w{'}{'}andb{'}{'}. Since w{'}{'}=w{'}
{\parallel}w{'}{\parallel}r, rearranging for
rgives
{\parallel}w{'}{'}{\parallel}=w{'}
{\parallel}w{'}{\parallel}r=1
r{\textperiodcentered}w{'}
{\parallel}w{'}{\parallel}=1
r. (12.24)
By substituting this result into (12.23), we obtain
max
w{'}{'},b{'}{'}1
{\parallel}w{'}{'}{\parallel}2
subject to yn({\langle}w{'}{'},xn{\rangle}+b{'}{'}){\geqslant}1.(12.25)
The final step is to observe that maximizing1
{\parallel}w{'}{'}{\parallel}2yields the same solution
as minimizing1
2{\parallel}w{'}{'}{\parallel}2, which concludes the proof of Theorem 12.1.
12.2.4 Soft Margin SVM: Geometric View
In the case where data is not linearly separable, we may wish to allow
some examples to fall within the margin region, or even to be on the
wrong side of the hyperplane as illustrated in Figure 12.6.
The model that allows for some classification errors is called the soft soft margin SVM
margin SVM . In this section, we derive the resulting optimization problem
using geometric arguments. In Section 12.2.5, we will derive an equiv-
alent optimization problem using the idea of a loss function. Using La-
grange multipliers (Section 7.2), we will derive the dual optimization
problem of the SVM in Section 12.3. This dual optimization problem al-
lows us to observe a third interpretation of the SVM: as a hyperplane that
bisects the line between convex hulls corresponding to the positive and
negative data examples (Section 12.3.2).
The key geometric idea is to introduce a slack variable {\xi}ncorresponding slack variable
to each example{\textendash}label pair (xn, yn)that allows a particular example to be
within the margin or even on the wrong side of the hyperplane (refer to
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
380 Classification with Support Vector Machines
Figure 12.7 Soft
margin SVM allows
examples to be
within the margin or
on the wrong side of
the hyperplane. The
slack variable {\xi}
measures the
distance of a
positive example
x+to the positive
margin hyperplane
{\langle}w,x{\rangle}+b= 1
whenx+is on the
wrong side..x+w
{\langle}w,x{\rangle}+
b= 0{\langle}w,x{\rangle}+
b= 1.
{\xi}
Figure 12.7). We subtract the value of {\xi}nfrom the margin, constraining
{\xi}nto be non-negative. To encourage correct classification of the samples,
we add {\xi}nto the objective
min
w,b,{\xi}1
2{\parallel}w{\parallel}2+CNX
n=1{\xi}n (12.26a)
subject to yn({\langle}w,xn{\rangle}+b){\geqslant}1{-}{\xi}n (12.26b)
{\xi}n{\geqslant}0 (12.26c)
forn= 1, . . . , N . In contrast to the optimization problem (12.18) for the
hard margin SVM, this one is called the soft margin SVM . The parameter soft margin SVM
C {>}0trades off the size of the margin and the total amount of slack that
we have. This parameter is called the regularization parameter since, as regularization
parameter we will see in the following section, the margin term in the objective func-
tion (12.26a) is a regularization term. The margin term {\parallel}w{\parallel}2is called
theregularizer , and in many books on numerical optimization, the reg- regularizer
ularization parameter is multiplied with this term (Section 8.2.3). This
is in contrast to our formulation in this section. Here a large value of C
implies low regularization, as we give the slack variables larger weight,
hence giving more priority to examples that do not lie on the correct side
of the margin. There are
alternative
parametrizations of
this regularization,
which is
why (12.26a) is also
often referred to as
theC-SVM.Remark. In the formulation of the soft margin SVM (12.26a) wis reg-
ularized, but bis not regularized. We can see this by observing that the
regularization term does not contain b. The unregularized term bcom-
plicates theoretical analysis (Steinwart and Christmann, 2008, chapter 1)
and decreases computational efficiency (Fan et al., 2008). {\diamond}
12.2.5 Soft Margin SVM: Loss Function View
Let us consider a different approach for deriving the SVM, following the
principle of empirical risk minimization (Section 8.2). For the SVM, we
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
12.2 Primal Support Vector Machine 381
choose hyperplanes as the hypothesis class, that is
f(x) ={\langle}w,x{\rangle}+b. (12.27)
We will see in this section that the margin corresponds to the regulariza-
tion term. The remaining question is, what is the loss function ? In con- loss function
trast to Chapter 9, where we consider regression problems (the output
of the predictor is a real number), in this chapter, we consider binary
classification problems (the output of the predictor is one of two labels
{\{}+1,{-}1{\}}). Therefore, the error/loss function for each single example{\textendash}
label pair needs to be appropriate for binary classification. For example,
the squared loss that is used for regression (9.10b) is not suitable for bi-
nary classification.
Remark. The ideal loss function between binary labels is to count the num-
ber of mismatches between the prediction and the label. This means that
for a predictor fapplied to an example xn, we compare the output f(xn)
with the label yn. We define the loss to be zero if they match, and one if
they do not match. This is denoted by 1(f(xn)=yn)and is called the
zero-one loss . Unfortunately, the zero-one loss results in a combinatorial zero-one loss
optimization problem for finding the best parameters w, b. Combinatorial
optimization problems (in contrast to continuous optimization problems
discussed in Chapter 7) are in general more challenging to solve. {\diamond}
What is the loss function corresponding to the SVM? Consider the error
between the output of a predictor f(xn)and the label yn. The loss de-
scribes the error that is made on the training data. An equivalent way to
derive (12.26a) is to use the hinge loss hinge loss
{\ell}(t) = max {\{}0,1{-}t{\}}where t=yf(x) =y({\langle}w,x{\rangle}+b).(12.28)
Iff(x)is on the correct side (based on the corresponding label y) of the
hyperplane, and further than distance 1, this means that t{\geqslant}1and the
hinge loss returns a value of zero. If f(x)is on the correct side but too
close to the hyperplane ( 0{<} t {<} 1), the example xis within the margin,
and the hinge loss returns a positive value. When the example is on the
wrong side of the hyperplane ( t {<}0), the hinge loss returns an even larger
value, which increases linearly. In other words, we pay a penalty once we
are closer than the margin to the hyperplane, even if the prediction is
correct, and the penalty increases linearly. An alternative way to express
the hinge loss is by considering it as two linear pieces
{\ell}(t) =(
0 if t{\geqslant}1
1{-}tift {<}1, (12.29)
as illustrated in Figure 12.8. The loss corresponding to the hard margin
SVM 12.18 is defined as
{\ell}(t) =(
0 if t{\geqslant}1
{\infty}ift {<}1. (12.30)
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
382 Classification with Support Vector Machines
Figure 12.8 The
hinge loss is a
convex upper bound
of zero-one loss.
{-}2 0 2
t024max{\{}0,1{-}t{\}}Zero-one loss
Hinge loss
This loss can be interpreted as never allowing any examples inside the
margin.
For a given training set {\{}(x1, y1), . . . , (xN, yN){\}}, we seek to minimize
the total loss, while regularizing the objective with {\ell}2-regularization (see
Section 8.2.3). Using the hinge loss (12.28) gives us the unconstrained
optimization problem
min
w,b1
2{\parallel}w{\parallel}2
|{\{}z{\}}
regularizer+CNX
n=1max{\{}0,1{-}yn({\langle}w,xn{\rangle}+b){\}}
| {\{}z {\}}
error term. (12.31)
The first term in (12.31) is called the regularization term or the regularizer regularizer
(see Section 8.2.3), and the second term is called the loss term or the error loss term
error termterm. Recall from Section 12.2.4 that the term1
2{\parallel}w{\parallel}2arises directly from
the margin. In other words, margin maximization can be interpreted as
regularization . regularization
In principle, the unconstrained optimization problem in (12.31) can
be directly solved with (sub-)gradient descent methods as described in
Section 7.1. To see that (12.31) and (12.26a) are equivalent, observe that
the hinge loss (12.28) essentially consists of two linear parts, as expressed
in (12.29). Consider the hinge loss for a single example-label pair (12.28).
We can equivalently replace minimization of the hinge loss over twith a
minimization of a slack variable {\xi}with two constraints. In equation form,
min
tmax{\{}0,1{-}t{\}} (12.32)
is equivalent to
min
{\xi},t{\xi}
subject to {\xi}{\geqslant}0, {\xi}{\geqslant}1{-}t .(12.33)
By substituting this expression into (12.31) and rearranging one of the
constraints, we obtain exactly the soft margin SVM (12.26a).
Remark. Let us contrast our choice of the loss function in this section to the
loss function for linear regression in Chapter 9. Recall from Section 9.2.1
that for finding maximum likelihood estimators, we usually minimize the
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
12.3 Dual Support Vector Machine 383
negative log-likelihood. Furthermore, since the likelihood term for linear
regression with Gaussian noise is Gaussian, the negative log-likelihood for
each example is a squared error function. The squared error function is the
loss function that is minimized when looking for the maximum likelihood
solution. {\diamond}
12.3 Dual Support Vector Machine
The description of the SVM in the previous sections, in terms of the vari-
ableswandb, is known as the primal SVM. Recall that we consider inputs
x{\in}RDwith Dfeatures. Since wis of the same dimension as x, this
means that the number of parameters (the dimension of w) of the opti-
mization problem grows linearly with the number of features.
In the following, we consider an equivalent optimization problem (the
so-called dual view), which is independent of the number of features. In-
stead, the number of parameters increases with the number of examples
in the training set. We saw a similar idea appear in Chapter 10, where we
expressed the learning problem in a way that does not scale with the num-
ber of features. This is useful for problems where we have more features
than the number of examples in the training dataset. The dual SVM also
has the additional advantage that it easily allows kernels to be applied,
as we shall see at the end of this chapter. The word {\textquotedblleft}dual{\textquotedblright} appears often
in mathematical literature, and in this particular case it refers to convex
duality. The following subsections are essentially an application of convex
duality, which we discussed in Section 7.2.
12.3.1 Convex Duality via Lagrange Multipliers
Recall the primal soft margin SVM (12.26a). We call the variables w,b,
and{\xi}corresponding to the primal SVM the primal variables. We use {\alpha}n{\geqslant} In Chapter 7, we
used{\lambda}as Lagrange
multipliers. In this
section, we follow
the notation
commonly chosen in
SVM literature, and
use{\alpha}and{\gamma}.0as the Lagrange multiplier corresponding to the constraint (12.26b) that
the examples are classified correctly and {\gamma}n{\geqslant}0as the Lagrange multi-
plier corresponding to the non-negativity constraint of the slack variable;
see (12.26c). The Lagrangian is then given by
L(w, b, {\xi}, {\alpha}, {\gamma} ) =1
2{\parallel}w{\parallel}2+CNX
n=1{\xi}n (12.34)
{-}NX
n=1{\alpha}n(yn({\langle}w,xn{\rangle}+b){-}1 +{\xi}n)
| {\{}z {\}}
constraint (12.26b){-}NX
n=1{\gamma}n{\xi}n
|{\{}z{\}}
constraint (12.26c).
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
384 Classification with Support Vector Machines
By differentiating the Lagrangian (12.34) with respect to the three primal
variables w,b, and {\xi}respectively, we obtain
{\partial}L
{\partial}w=w{\top}{-}NX
n=1{\alpha}nynxn{\top}, (12.35)
{\partial}L
{\partial}b={-}NX
n=1{\alpha}nyn, (12.36)
{\partial}L
{\partial}{\xi}n=C{-}{\alpha}n{-}{\gamma}n. (12.37)
We now find the maximum of the Lagrangian by setting each of these
partial derivatives to zero. By setting (12.35) to zero, we find
w=NX
n=1{\alpha}nynxn, (12.38)
which is a particular instance of the representer theorem (Kimeldorf and representer theorem
Wahba, 1970). Equation (12.38) states that the optimal weight vector in The representer
theorem is actually
a collection of
theorems saying
that the solution of
minimizing
empirical risk lies in
the subspace
(Section 2.4.3)
defined by the
examples.the primal is a linear combination of the examples xn. Recall from Sec-
tion 2.6.1 that this means that the solution of the optimization problem
lies in the span of training data. Additionally, the constraint obtained by
setting (12.36) to zero implies that the optimal weight vector is an affine
combination of the examples. The representer theorem turns out to hold
for very general settings of regularized empirical risk minimization (Hof-
mann et al., 2008; Argyriou and Dinuzzo, 2014). The theorem has more
general versions (Sch {\textasciidieresis}olkopf et al., 2001), and necessary and sufficient
conditions on its existence can be found in Yu et al. (2013).
Remark. The representer theorem (12.38) also provides an explanation
of the name {\textquotedblleft}support vector machine.{\textquotedblright} The examples xn, for which the
corresponding parameters {\alpha}n= 0, do not contribute to the solution wat
all. The other examples, where {\alpha}n{>}0, are called support vectors since support vector
they {\textquotedblleft}support{\textquotedblright} the hyperplane. {\diamond}
By substituting the expression for winto the Lagrangian (12.34), we
obtain the dual
D({\xi}, {\alpha}, {\gamma} ) =1
2NX
i=1NX
j=1yiyj{\alpha}i{\alpha}j{\langle}xi,xj{\rangle} {-}NX
i=1yi{\alpha}i*NX
j=1yj{\alpha}jxj,xi+
+CNX
i=1{\xi}i{-}bNX
i=1yi{\alpha}i+NX
i=1{\alpha}i{-}NX
i=1{\alpha}i{\xi}i{-}NX
i=1{\gamma}i{\xi}i.
(12.39)
Note that there are no longer any terms involving the primal variable w.
By setting (12.36) to zero, we obtainPN
n=1yn{\alpha}n= 0. Therefore, the term
involving balso vanishes. Recall that inner products are symmetric and
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
12.3 Dual Support Vector Machine 385
bilinear (see Section 3.2). Therefore, the first two terms in (12.39) are
over the same objects. These terms (colored blue) can be simplified, and
we obtain the Lagrangian
D({\xi}, {\alpha}, {\gamma} ) ={-}1
2NX
i=1NX
j=1yiyj{\alpha}i{\alpha}j{\langle}xi,xj{\rangle}+NX
i=1{\alpha}i+NX
i=1(C{-}{\alpha}i{-}{\gamma}i){\xi}i.
(12.40)
The last term in this equation is a collection of all terms that contain slack
variables {\xi}i. By setting (12.37) to zero, we see that the last term in (12.40)
is also zero. Furthermore, by using the same equation and recalling that
the Lagrange multiplers {\gamma}iare non-negative, we conclude that {\alpha}i{\leqslant}C.
We now obtain the dual optimization problem of the SVM, which is ex-
pressed exclusively in terms of the Lagrange multipliers {\alpha}i. Recall from
Lagrangian duality (Definition 7.1) that we maximize the dual problem.
This is equivalent to minimizing the negative dual problem, such that we
end up with the dual SVM dual SVM
min
{\alpha}1
2NX
i=1NX
j=1yiyj{\alpha}i{\alpha}j{\langle}xi,xj{\rangle} {-}NX
i=1{\alpha}i
subject toNX
i=1yi{\alpha}i= 0
0{\leqslant}{\alpha}i{\leqslant}Cfor all i= 1, . . . , N .(12.41)
The equality constraint in (12.41) is obtained from setting (12.36) to
zero. The inequality constraint {\alpha}i{\geqslant}0is the condition imposed on La-
grange multipliers of inequality constraints (Section 7.2). The inequality
constraint {\alpha}i{\leqslant}Cis discussed in the previous paragraph.
The set of inequality constraints in the SVM are called {\textquotedblleft}box constraints{\textquotedblright}
because they limit the vector {\alpha}= [{\alpha}1,{\textperiodcentered}{\textperiodcentered}{\textperiodcentered}, {\alpha}N]{\top}{\in}RNof Lagrange mul-
tipliers to be inside the box defined by 0andCon each axis. These
axis-aligned boxes are particularly efficient to implement in numerical
solvers (Dost {\textasciiacute}al, 2009, chapter 5). It turns out that
examples that lie
exactly on the
margin are
examples whose
dual parameters lie
strictly inside the
box constraints,
0{<} {\alpha}i{<} C. This is
derived using the
Karush Kuhn Tucker
conditions, for
example in
Sch{\textasciidieresis}olkopf and
Smola (2002).Once we obtain the dual parameters {\alpha}, we can recover the primal pa-
rameters wby using the representer theorem (12.38). Let us call the op-
timal primal parameter w{*}. However, there remains the question on how
to obtain the parameter b{*}. Consider an example xnthat lies exactly on
the margin`s boundary, i.e., {\langle}w{*},xn{\rangle}+b=yn. Recall that ynis either +1
or{-}1. Therefore, the only unknown is b, which can be computed by
b{*}=yn{-} {\langle}w{*},xn{\rangle}. (12.42)
Remark. In principle, there may be no examples that lie exactly on the
margin. In this case, we should compute |yn{-} {\langle}w{*},xn{\rangle}|for all support
vectors and take the median value of this absolute value difference to be
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
386 Classification with Support Vector Machines
Figure 12.9 Convex
hulls. (a) Convex
hull of points, some
of which lie within
the boundary;
(b) convex hulls
around positive and
negative examples.
(a) Convex hull.
c
d (b) Convex hulls around positive (blue) and
negative (orange) examples. The distance be-
tween the two convex sets is the length of the
difference vector c{-}d.
the value of b{*}. A derivation of this can be found in http://fouryears.
eu/2012/06/07/the-svm-bias-term-conspiracy/ . {\diamond}
12.3.2 Dual SVM: Convex Hull View
Another approach to obtain the dual SVM is to consider an alternative
geometric argument. Consider the set of examples xnwith the same label.
We would like to build a convex set that contains all the examples such
that it is the smallest possible set. This is called the convex hull and is
illustrated in Figure 12.9.
Let us first build some intuition about a convex combination of points.
Consider two points x1andx2and corresponding non-negative weights
{\alpha}1, {\alpha}2{\geqslant}0such that {\alpha}1+{\alpha}2= 1. The equation {\alpha}1x1+{\alpha}2x2describes each
point on a line between x1andx2. Consider what happens when we add
a third point x3along with a weight {\alpha}3{\geqslant}0such thatP3
n=1{\alpha}n= 1.
The convex combination of these three points x1,x2,x3spans a two-
dimensional area. The convex hull of this area is the triangle formed by convex hull
the edges corresponding to each pair of of points. As we add more points,
and the number of points becomes greater than the number of dimen-
sions, some of the points will be inside the convex hull, as we can see in
Figure 12.9(a).
In general, building a convex convex hull can be done by introducing
non-negative weights {\alpha}n{\geqslant}0corresponding to each example xn. Then
the convex hull can be described as the set
conv ( X) =(NX
n=1{\alpha}nxn)
withNX
n=1{\alpha}n= 1 and {\alpha}n{\geqslant}0,(12.43)
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
12.3 Dual Support Vector Machine 387
for all n= 1, . . . , N . If the two clouds of points corresponding to the
positive and negative classes are separated, then the convex hulls do not
overlap. Given the training data (x1, y1), . . . , (xN, yN), we form two con-
vex hulls, corresponding to the positive and negative classes respectively.
We pick a point c, which is in the convex hull of the set of positive exam-
ples, and is closest to the negative class distribution. Similarly, we pick a
pointdin the convex hull of the set of negative examples and is closest to
the positive class distribution; see Figure 12.9(b). We define a difference
vector between dandcas
w:=c{-}d. (12.44)
Picking the points canddas in the preceding cases, and requiring them
to be closest to each other is equivalent to minimizing the length/norm of
w, so that we end up with the corresponding optimization problem
arg min
w{\parallel}w{\parallel}= arg min
w1
2{\parallel}w{\parallel}2. (12.45)
Sincecmust be in the positive convex hull, it can be expressed as a convex
combination of the positive examples, i.e., for non-negative coefficients
{\alpha}+
n
c=X
n:yn=+1{\alpha}+
nxn. (12.46)
In (12.46), we use the notation n:yn= +1 to indicate the set of indices
nfor which yn= +1 . Similarly, for the examples with negative labels, we
obtain
d=X
n:yn={-}1{\alpha}{-}
nxn. (12.47)
By substituting (12.44), (12.46), and (12.47) into (12.45), we obtain the
objective
min
{\alpha}1
2X
n:yn=+1{\alpha}+
nxn{-}X
n:yn={-}1{\alpha}{-}
nxn2
. (12.48)
Let{\alpha}be the set of all coefficients, i.e., the concatenation of {\alpha}+and{\alpha}{-}.
Recall that we require that for each convex hull that their coefficients sum
to one,
X
n:yn=+1{\alpha}+
n= 1 andX
n:yn={-}1{\alpha}{-}
n= 1. (12.49)
This implies the constraint
NX
n=1yn{\alpha}n= 0. (12.50)
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
388 Classification with Support Vector Machines
This result can be seen by multiplying out the individual classes
NX
n=1yn{\alpha}n=X
n:yn=+1(+1){\alpha}+
n+X
n:yn={-}1({-}1){\alpha}{-}
n (12.51a)
=X
n:yn=+1{\alpha}+
n{-}X
n:yn={-}1{\alpha}{-}
n= 1{-}1 = 0 . (12.51b)
The objective function (12.48) and the constraint (12.50), along with the
assumption that {\alpha}{\geqslant}0, give us a constrained (convex) optimization prob-
lem. This optimization problem can be shown to be the same as that of
the dual hard margin SVM (Bennett and Bredensteiner, 2000a).
Remark. To obtain the soft margin dual, we consider the reduced hull. The
reduced hull is similar to the convex hull but has an upper bound to the reduced hull
size of the coefficients {\alpha}. The maximum possible value of the elements
of{\alpha}restricts the size that the convex hull can take. In other words, the
bound on {\alpha}shrinks the convex hull to a smaller volume (Bennett and
Bredensteiner, 2000b). {\diamond}
12.4 Kernels
Consider the formulation of the dual SVM (12.41). Notice that the in-
ner product in the objective occurs only between examples xiandxj.
There are no inner products between the examples and the parameters.
Therefore, if we consider a set of features {\phi}(xi)to represent xi, the only
change in the dual SVM will be to replace the inner product. This mod-
ularity, where the choice of the classification method (the SVM) and the
choice of the feature representation {\phi}(x)can be considered separately,
provides flexibility for us to explore the two problems independently. In
this section, we discuss the representation {\phi}(x)and briefly introduce the
idea of kernels, but do not go into the technical details.
Since{\phi}(x)could be a non-linear function, we can use the SVM (which
assumes a linear classifier) to construct classifiers that are nonlinear in
the examples xn. This provides a second avenue, in addition to the soft
margin, for users to deal with a dataset that is not linearly separable. It
turns out that there are many algorithms and statistical methods that have
this property that we observed in the dual SVM: the only inner products
are those that occur between examples. Instead of explicitly defining a
non-linear feature map {\phi}({\textperiodcentered})and computing the resulting inner product
between examples xiandxj, we define a similarity function k(xi,xj)be-
tween xiandxj. For a certain class of similarity functions, called kernels , kernel
the similarity function implicitly defines a non-linear feature map {\phi}({\textperiodcentered}).
Kernels are by definition functions k:X {\texttimes} X {\textrightarrow} Rfor which there exists The inputs Xof the
kernel function can
be very general and
are not necessarily
restricted to RD.a Hilbert space Hand{\phi}:X {\textrightarrow} H a feature map such that
k(xi,xj) ={\langle}{\phi}(xi),{\phi}(xj){\rangle}H. (12.52)
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
12.4 Kernels 389
Figure 12.10 SVM
with different
kernels. Note that
while the decision
boundary is
nonlinear, the
underlying problem
being solved is for a
linear separating
hyperplane (albeit
with a nonlinear
kernel).
First featureSecond feature
(a) SVM with linear kernel
First featureSecond feature (b) SVM with RBF kernel
First featureSecond feature
(c) SVM with polynomial (degree 2) kernel
First featureSecond feature (d) SVM with polynomial (degree 3) kernel
There is a unique reproducing kernel Hilbert space associated with every
kernel k(Aronszajn, 1950; Berlinet and Thomas-Agnan, 2004). In this
unique association, {\phi}(x) =k({\textperiodcentered},x)is called the canonical feature map .canonical feature
map The generalization from an inner product to a kernel function (12.52) is
known as the kernel trick (Sch{\textasciidieresis}olkopf and Smola, 2002; Shawe-Taylor and kernel trick
Cristianini, 2004), as it hides away the explicit non-linear feature map.
The matrix K{\in}RN{\texttimes}N, resulting from the inner products or the appli-
cation of k({\textperiodcentered},{\textperiodcentered})to a dataset, is called the Gram matrix , and is often just Gram matrix
referred to as the kernel matrix . Kernels must be symmetric and positive kernel matrix
semidefinite functions so that every kernel matrix Kis symmetric and
positive semidefinite (Section 3.2.3):
{\forall}z{\in}RN:z{\top}Kz{\geqslant}0. (12.53)
Some popular examples of kernels for multivariate real-valued data xi{\in}
RDare the polynomial kernel, the Gaussian radial basis function kernel,
and the rational quadratic kernel (Sch {\textasciidieresis}olkopf and Smola, 2002; Rasmussen
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
390 Classification with Support Vector Machines
and Williams, 2006). Figure 12.10 illustrates the effect of different kernels
on separating hyperplanes on an example dataset. Note that we are still
solving for hyperplanes, that is, the hypothesis class of functions are still
linear. The non-linear surfaces are due to the kernel function.
Remark. Unfortunately for the fledgling machine learner, there are mul-
tiple meanings of the word {\textquotedblleft}kernel.{\textquotedblright} In this chapter, the word {\textquotedblleft}kernel{\textquotedblright}
comes from the idea of the reproducing kernel Hilbert space (RKHS) (Aron-
szajn, 1950; Saitoh, 1988). We have discussed the idea of the kernel in lin-
ear algebra (Section 2.7.3), where the kernel is another word for the null
space. The third common use of the word {\textquotedblleft}kernel{\textquotedblright} in machine learning is
the smoothing kernel in kernel density estimation (Section 11.5). {\diamond}
Since the explicit representation {\phi}(x)is mathematically equivalent to
the kernel representation k(xi,xj), a practitioner will often design the
kernel function such that it can be computed more efficiently than the
inner product between explicit feature maps. For example, consider the
polynomial kernel (Sch {\textasciidieresis}olkopf and Smola, 2002), where the number of
terms in the explicit expansion grows very quickly (even for polynomials
of low degree) when the input dimension is large. The kernel function
only requires one multiplication per input dimension, which can provide
significant computational savings. Another example is the Gaussian ra-
dial basis function kernel (Sch {\textasciidieresis}olkopf and Smola, 2002; Rasmussen and
Williams, 2006), where the corresponding feature space is infinite dimen-
sional. In this case, we cannot explicitly represent the feature space but
can still compute similarities between a pair of examples using the kernel. The choice of
kernel, as well as
the parameters of
the kernel, is often
chosen using nested
cross-validation
(Section 8.6.1).Another useful aspect of the kernel trick is that there is no need for
the original data to be already represented as multivariate real-valued
data. Note that the inner product is defined on the output of the function
{\phi}({\textperiodcentered}), but does not restrict the input to real numbers. Hence, the function
{\phi}({\textperiodcentered})and the kernel function k({\textperiodcentered},{\textperiodcentered})can be defined on any object, e.g.,
sets, sequences, strings, graphs, and distributions (Ben-Hur et al., 2008;
G{\textasciidieresis}artner, 2008; Shi et al., 2009; Sriperumbudur et al., 2010; Vishwanathan
et al., 2010).
12.5 Numerical Solution
We conclude our discussion of SVMs by looking at how to express the
problems derived in this chapter in terms of the concepts presented in
Chapter 7. We consider two different approaches for finding the optimal
solution for the SVM. First we consider the loss view of SVM 8.2.2 and ex-
press this as an unconstrained optimization problem. Then we express the
constrained versions of the primal and dual SVMs as quadratic programs
in standard form 7.3.2.
Consider the loss function view of the SVM (12.31). This is a convex
unconstrained optimization problem, but the hinge loss (12.28) is not dif-
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .
12.5 Numerical Solution 391
ferentiable. Therefore, we apply a subgradient approach for solving it.
However, the hinge loss is differentiable almost everywhere, except for
one single point at the hinge t= 1. At this point, the gradient is a set of
possible values that lie between 0and{-}1. Therefore, the subgradient gof
the hinge loss is given by
g(t) =

{-}1 t {<}1
[{-}1,0]t= 1
0 t {>}1. (12.54)
Using this subgradient, we can apply the optimization methods presented
in Section 7.1.
Both the primal and the dual SVM result in a convex quadratic pro-
gramming problem (constrained optimization). Note that the primal SVM
in (12.26a) has optimization variables that have the size of the dimen-
sionDof the input examples. The dual SVM in (12.41) has optimization
variables that have the size of the number Nof examples.
To express the primal SVM in the standard form (7.45) for quadratic
programming, let us assume that we use the dot product (3.5) as the
inner product. We rearrange the equation for the primal SVM (12.26a), Recall from
Section 3.2 that we
use the phrase dot
product to mean the
inner product on
Euclidean vector
space.such that the optimization variables are all on the right and the inequality
of the constraint matches the standard form. This yields the optimization
min
w,b,{\xi}1
2{\parallel}w{\parallel}2+CNX
n=1{\xi}n
subject to{-}ynx{\top}
nw{-}ynb{-}{\xi}n{\leqslant}{-}1
{-}{\xi}n{\leqslant}0(12.55)
n= 1, . . . , N . By concatenating the variables w, b,xninto a single vector,
and carefully collecting the terms, we obtain the following matrix form of
the soft margin SVM:
min
w,b,{\xi}1
2
w
b
{\xi}
{\top}ID 0D,N+1
0N+1,D0N+1,N+1
w
b
{\xi}
+0D+1,1C1N,1{\top}
w
b
{\xi}

subject to{-}Y X {-}y{-}IN
0N,D+1 {-}IN
w
b
{\xi}
{\leqslant}{-}1N,1
0N,1
.
(12.56)
In the preceding optimization problem, the minimization is over the pa-
rameters [w{\top}, b,{\xi}{\top}]{\top}{\in}RD+1+N, and we use the notation: Imto rep-
resent the identity matrix of size m{\texttimes}m,0m,nto represent the matrix
of zeros of size m{\texttimes}n, and 1m,nto represent the matrix of ones of size
m{\texttimes}n. In addition, yis the vector of labels [y1,{\textperiodcentered}{\textperiodcentered}{\textperiodcentered}, yN]{\top},Y= diag( y)
{\textcopyright}2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
392 Classification with Support Vector Machines
is an NbyNmatrix where the elements of the diagonal are from y, and
X{\in}RN{\texttimes}Dis the matrix obtained by concatenating all the examples.
We can similarly perform a collection of terms for the dual version of the
SVM (12.41). To express the dual SVM in standard form, we first have to
express the kernel matrix Ksuch that each entry is Kij=k(xi,xj). If we
have an explicit feature representation xithen we define Kij={\langle}xi,xj{\rangle}.
For convenience of notation we introduce a matrix with zeros everywhere
except on the diagonal, where we store the labels, that is, Y= diag( y).
The dual SVM can be written as
min
{\alpha}1
2{\alpha}{\top}Y KY {\alpha} {-}1{\top}
N,1{\alpha}
subject to
y{\top}
{-}y{\top}
{-}IN
IN
{\alpha}{\leqslant}0N+2,1
C1N,1
.(12.57)
Remark. In Sections 7.3.1 and 7.3.2, we introduced the standard forms
of the constraints to be inequality constraints. We will express the dual
SVM`s equality constraint as two inequality constraints, i.e.,
Ax=bis replaced by Ax{\leqslant}band Ax{\geqslant}b. (12.58)
Particular software implementations of convex optimization methods may
provide the ability to express equality constraints. {\diamond}
Since there are many different possible views of the SVM, there are
many approaches for solving the resulting optimization problem. The ap-
proach presented here, expressing the SVM problem in standard convex
optimization form, is not often used in practice. The two main implemen-
tations of SVM solvers are Chang and Lin (2011) (which is open source)
and Joachims (1999). Since SVMs have a clear and well-defined optimiza-
tion problem, many approaches based on numerical optimization tech-
niques (Nocedal and Wright, 2006) can be applied (Shawe-Taylor and
Sun, 2011).
12.6 Further Reading
The SVM is one of many approaches for studying binary classification.
Other approaches include the perceptron, logistic regression, Fisher dis-
criminant, nearest neighbor, naive Bayes, and random forest (Bishop, 2006;
Murphy, 2012). A short tutorial on SVMs and kernels on discrete se-
quences can be found in Ben-Hur et al. (2008). The development of SVMs
is closely linked to empirical risk minimization, discussed in Section 8.2.
Hence, the SVM has strong theoretical properties (Vapnik, 2000; Stein-
wart and Christmann, 2008). The book about kernel methods (Sch {\textasciidieresis}olkopf
and Smola, 2002) includes many details of support vector machines and
Draft (2023-12-19) of {\textquotedblleft}Mathematics for Machine Learning{\textquotedblright}. Feedback: https://mml-book.com .